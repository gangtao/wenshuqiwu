[{
  "tag": "P",
  "text": "Recall the short answer to the question ‘what are Markov chain Monte Carlo methods?’ Here it is again as a TL;DR:",
  "translation": "回想一下“什么是马尔可夫链蒙特卡罗方法？”这个问题的简短答案，这里又是TL; DR："
}, {
  "tag": "P",
  "text": "MCMC methods are used to approximate the posterior distribution of a parameter of interest by random sampling in a probabilistic space.",
  "translation": "MCMC方法用于通过概率空间中的随机采样来估计目标参数的后验分布。"
}, {
  "tag": "P",
  "text": "I hope I’ve explained that short answer, why you would use MCMC methods, and how they work. The inspiration for this post was a talk I gave as part of General Assembly’s Data Science Immersive course in Washington, DC. The goals of that talk were to explain Markov chain Monte Carlo methods to a non-technical audience, and I’ve tried to do the same here. Leave a comment if you think this explanation is off the mark in some way, or if it could be made more intuitive.",
  "translation": "希望我已经解释了这个简短答案，为什么要使用MCMC方法以及它们如何工作。 这篇文章的灵感来自我在华盛顿特区举行的大会“数据科学沉浸式”课程中的一次演讲。 演讲的目的是向非技术人员介绍马尔可夫链蒙特卡洛方法，我在这里也尝试这样做。 如果您认为此解释在某种程度上不合时宜，或者可以使其更加直观，请发表评论。"
}, {
  "tag": "P",
  "text": "With some knowledge of Monte Carlo simulations and Markov chains, I hope the math-free explanation of how MCMC methods work is pretty intuitive.",
  "translation": "有了蒙特卡洛仿真和马尔可夫链的一些知识，我希望MCMC方法如何工作的无数学解释非常直观。"
}, {
  "tag": "P",
  "text": "Recall that we are trying to estimate the posterior distribution for the parameter we’re interested in, average human height:",
  "translation": "回想一下，我们正在尝试估算我们感兴趣的参数的平均人身高度的后验分布："
}, {
  "tag": "FIGURE",
  "image": "https://miro.medium.com/max/30/0*XeUN0u_-EPh6MMCV.?q=20",
  "caption": "I am not a visualization expert, nor apparently am I any good at keeping my example within the bounds of common sense: my example of the posterior distribution seriously overestimates average human height.",
  "type": "image"
}, {
  "tag": "P",
  "text": "We know that the posterior distribution is somewhere in the range of our prior distribution and our likelihood distribution, but for whatever reason, we can’t compute it directly. Using MCMC methods, we’ll effectively draw samples from the posterior distribution, and then compute statistics like the average on the samples drawn.",
  "translation": "我们知道后验分布在我们先验分布和似然分布的范围内，但是由于某种原因，我们无法直接对其进行计算。 使用MCMC方法，我们将有效地从后验分布中抽取样本，然后计算统计数据，例如抽取样本的平均值。"
}, {
  "tag": "P",
  "text": "To begin, MCMC methods pick a random parameter value to consider. The simulation will continue to generate random values (this is the Monte Carlo part), but subject to some rule for determining what makes a good parameter value. The trick is that, for a pair of parameter values, it is possible to compute which is a better parameter value, by computing how likely each value is to explain the data, given our prior beliefs. If a randomly generated parameter value is better than the last one, it is added to the chain of parameter values with a certain probability determined by how much better it is (this is the Markov chain part).",
  "translation": "首先，MCMC方法选择一个随机参数值进行考虑。 模拟将继续生成随机值（这是“蒙特卡洛”部分），但要遵循确定确定良好参数值的规则。 诀窍在于，根据我们先前的信念，对于一对参数值，可以通过计算每个值解释数据的可能性来计算哪个参数值更好。 如果随机生成的参数值好于最后一个参数值，则会以一定的概率将其添加到参数值链中，概率由其好多少决定（这是马尔可夫链部分）。"
}, {
  "tag": "P",
  "text": "To explain this visually, lets recall that the height of a distribution at a certain value represents the probability of observing that value. Therefore, we can think of our parameter values (the x-axis) exhibiting areas of high and low probability, shown on the y-axis. For a single parameter, MCMC methods begin by randomly sampling along the x-axis:",
  "translation": "为了直观地说明这一点，让我们回想一下，某个值处的分布高度表示观察该值的概率。 因此，我们可以想到我们的参数值（x轴）在y轴上显示高概率和低概率区域。 对于单个参数，MCMC方法从沿x轴随机采样开始："
}, {
  "tag": "FIGURE",
  "image": "https://miro.medium.com/max/30/0*fqwsnMwAXnAdWxDH.?q=20",
  "caption": "Red points are random parameter samples",
  "type": "image"
}, {
  "tag": "P",
  "text": "Since the random samples are subject to fixed probabilities, they tend to converge after a period of time in the region of highest probability for the parameter we’re interested in:",
  "translation": "由于随机样本具有固定的概率，因此在一段时间后，对于我们感兴趣的参数，它们趋于收敛在最高概率范围内："
}, {
  "tag": "FIGURE",
  "image": "https://miro.medium.com/max/30/0*aYE_6eWzDWfVCOsQ.?q=20",
  "caption": "Blue points just represent random samples after an arbitrary point in time, when convergence is expected to have occurred. Note: I’m stacking point vertically purely for illustrative purposes.",
  "type": "image"
}, {
  "tag": "P",
  "text": "After convergence has occurred, MCMC sampling yields a set of points which are samples from the posterior distribution. Draw a histogram around those points, and compute whatever statistics you like:",
  "translation": "收敛发生后，MCMC采样会得出一组点，这些点是来自后验分布的样本。 围绕这些点绘制直方图，并计算所需的任何统计量："
}, {
  "tag": "FIGURE",
  "image": "https://miro.medium.com/max/30/0*fMEvHsN-xlVzLw1H.?q=20",
  "type": "image"
}, {
  "tag": "P",
  "text": "Any statistic calculated on the set of samples generated by MCMC simulations is our best guess of that statistic on the true posterior distribution.",
  "translation": "根据MCMC模拟生成的样本集计算出的任何统计数据，都是我们对真实后验分布的统计数据的最佳猜测。"
}, {
  "tag": "P",
  "text": "MCMC methods can also be used to estimate the posterior distribution of more than one parameter (human height and weight, say). For n parameters, there exist regions of high probability in n-dimensional space where certain sets of parameter values better explain observed data. Therefore, I think of MCMC methods as randomly sampling inside a probabilistic space to approximate the posterior distribution.",
  "translation": "MCMC方法也可用于估计多个参数（例如，人的身高和体重）的后验分布。 对于n个参数，在n维空间中存在高概率区域，其中某些参数值集可以更好地解释观察到的数据。 因此，我认为MCMC方法是在概率空间内随机采样以近似后验分布。"
}, {
  "tag": "P",
  "text": "The second element to understanding MCMC methods are Markov chains. These are simply sequences of events that are probabilistically related to one another. Each event comes from a set of outcomes, and each outcome determines which outcome occurs next, according to a fixed set of probabilities.",
  "translation": "理解MCMC方法的第二个要素是马尔可夫链。 这些只是事件序列，它们在概率上相互关联。 每个事件都来自一组结果，每个结果根据一组固定的概率确定下一个结果。"
}, {
  "tag": "P",
  "text": "An important feature of Markov chains is that they are memoryless: everything that you would possibly need to predict the next event is available in the current state, and no new information comes from knowing the history of events. A game like Chutes and Ladders exhibits this memorylessness, or Markov Property, but few things in the real world actually work this way. Nevertheless, Markov chains are powerful ways of understanding the world.",
  "translation": "马尔可夫链的一个重要特征是它们没有记忆力：在当前状态下，您可能需要用来预测下一个事件的所有信息都是可用的，并且不知道事件的历史就可以得到新的信息。 诸如《溜槽》和《阶梯》之类的游戏就表现出这种无记忆性或“马尔可夫特性”，但现实世界中很少有东西能以这种方式起作用。 然而，马尔可夫链是理解世界的有力方法。"
}, {
  "tag": "P",
  "text": "In the 19th century, the bell curve was observed as a common pattern in nature. (We’ve noted, for example, that human heights follow a bell curve.) Galton Boards, which simulate the average values of repeated random events by dropping marbles through a board fitted with pegs, reproduce the normal curve in their distribution of marbles:",
  "translation": "在19世纪，钟形曲线被视为自然界中的常见图案。 （例如，我们注意到人类的身高遵循钟形曲线。）高尔顿板通过将大理石从装有钉子的板上掉下来来模拟重复随机事件的平均值，从而再现了大理石分布的正态曲线："
}, {
  "tag": "FIGURE",
  "image": "https://miro.medium.com/max/30/0*HDeFoQPFGs9ueUI0.?q=20",
  "type": "image"
}, {
  "tag": "P",
  "text": "Pavel Nekrasov, a Russian mathematician and theologian, argued that the bell curve and, more generally, the law of large numbers, were simply artifacts of children’s games and trivial puzzles, where every event was completely independent. He thought that interdependent events in the real world, such as human actions, did not conform to nice mathematical patterns or distributions.",
  "translation": "俄国数学家和神学家帕维尔·涅克拉索夫（Pavel Nekrasov）认为，钟形曲线以及更广泛的说是大数定律，只是儿童游戏和琐碎难题的产物，而每个事件都是完全独立的。 他认为现实世界中相互依存的事件（例如人类行为）不符合良好的数学模式或分布。"
}, {
  "tag": "P",
  "text": "Andrey Markov, for whom Markov chains are named, sought to prove that non-independent events may also conform to patterns. One of his best known examples required counting thousands of two-character pairs from a work of Russian poetry. Using those pairs, he computed the conditional probability of each character. That is, given a certain preceding letter or white space, there was a certain chance that the next letter would be an A, or a T, or a whitespace. Using those probabilities, Markov was ability to simulate an arbitrarily long sequence of characters. This was a Markov chain. Although the first few characters are largely determined by the choice of starting character, Markov showed that in the long run, the distribution of characters settled into a pattern. Thus, even interdependent events, if they are subject to fixed probabilities, conform to an average.",
  "translation": "以马尔可夫链命名的安德烈·马尔科夫（Andrey Markov）试图证明非独立事件也可能符合模式。 他最著名的例子之一是从俄罗斯诗歌作品中算出成千上万的两个字符对。 利用这些对，他计算了每个角色的条件概率。 也就是说，给定特定的前一个字母或空白，则很有可能下一个字母将是A或T或空白。 使用这些概率，Markov能够模拟任意长的字符序列。 这是一个马尔可夫链。 尽管前几个字符在很大程度上取决于起始字符的选择，但马可夫（Markov）表明，从长远来看，字符的分布会稳定在一个模式中。 因此，即使相互依存的事件具有固定的概率，也要符合平均值。"
}, {
  "tag": "P",
  "text": "For a more useful example, imagine you live in a house with five rooms. You have a bedroom, bathroom, living room, dining room, and kitchen. Lets collect some data, assuming that what room you are in at any given point in time is all we need to say what room you are likely to enter next. For instance, if you are in the kitchen, you have a 30% chance to stay in the kitchen, a 30% chance to go into the dining room, a 20% chance to go into the living room, a 10% chance to go into the bathroom, and a 10% chance to go into the bedroom. Using a set of probabilities for each room, we can construct a chain of predictions of which rooms you are likely to occupy next.",
  "translation": "举一个更有用的例子，假设您住在一间有五个房间的房子里。 您有一间卧室，浴室，客厅，饭厅和厨房。 让我们收集一些数据，假设在任何给定时间点您所处的房间就是我们接下来要说的您可能进入的房间。 例如，如果您在厨房里，则有30％的机会留在厨房，有30％的机会进入饭厅，有20％的机会进入客厅，有10％的机会去 进入浴室，有10％的机会进入卧室。 使用每个房间的一组概率，我们可以构建一系列预测，以预测您接下来可能会占用哪些房间。"
}, {
  "tag": "P",
  "text": "Making predictions a few states out might be useful, if we want to predict where someone in the house will be a little while after being in the kitchen. But since our predictions are just based on one observation of where a person is in the house, its reasonable to think they won’t be very good. If someone went from the bedroom to the bathroom, for example, its more likely they’ll go right back to the bedroom than if they had come from the kitchen. So the Markov Property doesn’t usually apply to the real world.",
  "translation": "如果我们要预测房子里某个人在厨房里待一会儿的位置，那么做出一些陈述可能会很有用。 但是，由于我们的预测只是基于对某人在房屋中的位置的一种观察，因此有理由认为他们的状况不会很好。 例如，如果有人从卧室去洗手间，那么比起从厨房来的人，他们更有可能马上回到卧室。 因此，马尔可夫财产通常不适用于现实世界。"
}, {
  "tag": "P",
  "text": "Running the Markov chain for thousands of iterations, however, does give the long-run prediction of what room you’re likely to be in. More importantly, this prediction isn’t affected at all by which room the person began in! Intuitively, this makes sense: it doesn’t matter where someone is in the house at one point in time in order to simulate and describe where they are likely to be in the long-term, or in general. So Markov chains, which seem like an unreasonable way to model a random variable over a few periods, can be used to compute the long-run tendency of that variable if we understand the probabilities that govern its behavior.",
  "translation": "但是，运行Markov链进行数千次迭代，确实可以对您可能位于的房间进行长期预测。更重要的是，此预测完全不受此人开始进入哪个房间的影响！ 从直觉上讲，这是有道理的：在某个时间点某人在房屋中的哪个位置，以模拟并描述他们长期或总体上可能位于的位置，都没有关系。 因此，如果我们了解控制变量行为的概率，则马尔可夫链似乎是在几个周期内对随机变量建模的一种不合理的方法，可以用来计算该变量的长期趋势。"
}, {
  "tag": "P",
  "text": "Monte Carlo simulations are just a way of estimating a fixed parameter by repeatedly generating random numbers. By taking the random numbers generated and doing some computation on them, Monte Carlo simulations provide an approximation of a parameter where calculating it directly is impossible or prohibitively expensive.",
  "translation": "蒙特卡洛模拟只是通过重复生成随机数来估算固定参数的一种方法。 通过获取所生成的随机数并对其进行一些计算，蒙特卡洛模拟提供了一个参数的近似值，其中直接计算它是不可能的，也可能是非常昂贵的。"
}, {
  "tag": "P",
  "text": "Suppose that we’d like to estimate the area of the follow circle:",
  "translation": "假设我们要估算跟随圈的面积："
}, {
  "tag": "FIGURE",
  "image": "https://miro.medium.com/max/30/0*OVzsV_Mw2OGi1mQ4.?q=20",
  "type": "image"
}, {
  "tag": "P",
  "text": "Since the circle is inside a square with 10 inch sides, the area can be easily calculated as 78.5 square inches. Instead, however, we can drop 20 points randomly inside the square. Then we count the proportion of points that fell within the circle, and multiply that by the area of the square. That number is a pretty good approximation of the area of the circle.",
  "translation": "由于该圆在边长为10英寸的正方形内，因此可以轻松地将面积计算为78.5平方英寸。 但是，我们可以在正方形内随机放置20个点。 然后，我们计算落入圆内的点的比例，然后乘以正方形的面积。 该数字非常近似于圆的面积。"
}, {
  "tag": "FIGURE",
  "image": "https://miro.medium.com/max/30/0*OuLZBu6HPdhignVB.?q=20",
  "type": "image"
}, {
  "tag": "P",
  "text": "Since 15 of the 20 points lay inside the circle, it looks like the circle is approximately 75 square inches. Not too bad for a Monte Carlo simulation with only 20 random points.",
  "translation": "由于20个点中的15个位于圆内，因此该圆大约为75平方英寸。 对于只有20个随机点的Monte Carlo模拟来说还不错。"
}, {
  "tag": "P",
  "text": "Now, imagine we’d like to calculate the area of the shape plotted by the Batman Equation:",
  "translation": "现在，假设我们要计算蝙蝠侠方程式绘制的形状的面积："
}, {
  "tag": "FIGURE",
  "image": "https://miro.medium.com/max/30/0*5ahw9HtOjbMK0tmO.?q=20",
  "type": "image"
}, {
  "tag": "P",
  "text": "Here’s a shape we never learned an equation for! Therefore, finding the area of the bat signal is very hard. Nevertheless, by dropping points randomly inside a rectangle containing the shape, Monte Carlo simulations can provide an approximation of the area quite easily!",
  "translation": "这是我们从未学过方程式的形状！ 因此，很难找到蝙蝠信号的区域。 但是，通过将点随机地拖放到包含该形状的矩形内，蒙特卡洛模拟可以非常轻松地提供该区域的近似值！"
}, {
  "tag": "P",
  "text": "Monte Carlo simulations aren’t only used for estimating the area of difficult shapes. By generating a lot of random numbers, they can be used to model very complicated processes. In practice, they’re used to forecast the weather, or estimate the probability of winning an election.",
  "translation": "蒙特卡洛模拟不仅用于估算困难形状的面积。 通过生成许多随机数，它们可以用于对非常复杂的过程进行建模。 实际上，它们是用来预测天气或估算赢得选举的可能性。"
}, {
  "tag": "P",
  "text": "So, what are Markov chain Monte Carlo (MCMC) methods? The short answer is:",
  "translation": "那么，什么是马尔可夫链蒙特卡罗（MCMC）方法？ 简短的答案是："
}, {
  "tag": "P",
  "text": "MCMC methods are used to approximate the posterior distribution of a parameter of interest by random sampling in a probabilistic space.",
  "translation": "MCMC方法用于通过概率空间中的随机采样来估计目标参数的后验分布。"
}, {
  "tag": "P",
  "text": "In this article, I will explain that short answer, without any math.",
  "translation": "在本文中，我将不带任何数学解释简短的答案。"
}, {
  "tag": "P",
  "text": "First, some terminology. A parameter of interest is just some number that summarizes a phenomenon we’re interested in. In general we use statistics to estimate parameters. For example, if we want to learn about the height of human adults, our parameter of interest might be average height in in inches. A distribution is a mathematical representation of every possible value of our parameter and how likely we are to observe each one. The most famous example is a bell curve:",
  "translation": "首先，一些术语。 感兴趣的参数只是一些数字，它概括了我们感兴趣的现象。通常，我们使用统计信息来估计参数。 例如，如果我们想了解成年人的身高，我们感兴趣的参数可能是以英寸为单位的平均身高。 分布是参数的每个可能值以及我们观察每个参数的可能性的数学表示。 最著名的例子是钟形曲线："
}, {
  "tag": "FIGURE",
  "image": "https://miro.medium.com/max/30/0*ZnhfX8oUe0biQRR6.?q=20",
  "caption": "Courtesy M. W. Toews",
  "type": "image"
}, {
  "tag": "P",
  "text": "In the Bayesian way of doing statistics, distributions have an additional interpretation. Instead of just representing the values of a parameter and how likely each one is to be the true value, a Bayesian thinks of a distribution as describing our beliefs about a parameter. Therefore, the bell curve above shows we’re pretty sure the value of the parameter is quite near zero, but we think there’s an equal likelihood of the true value being above or below that value, up to a point.",
  "translation": "在贝叶斯统计方法中，分布有其他解释。 贝叶斯不仅仅表示一个参数的值以及每个参数成为真实值的可能性，还认为分布描述了我们对参数的信念。 因此，上面的钟形曲线表明我们很确定该参数的值非常接近零，但是我们认为真实值在该点之上或之下的可能性均相等。"
}, {
  "tag": "P",
  "text": "As it happens, human heights do follow a normal curve, so let’s say we believe the true value of average human height follows a bell curve like this:",
  "translation": "碰巧的是，人类的身高确实遵循一条正常曲线，所以可以说，我们认为人类平均身高的真实值遵循如下的钟形曲线："
}, {
  "tag": "FIGURE",
  "image": "https://miro.medium.com/max/30/0*tlqbgInNAYyi3dgx.?q=20",
  "type": "image"
}, {
  "tag": "P",
  "text": "Clearly, the person with beliefs represented by this graph has been living among giants for years, because as far as they know, the most likely average adult height is 6'2\" (but they’re not super confident one way or another).",
  "translation": "显然，此图表示的具有信念的人已经在巨人中生活了多年，因为据他们所知，最有可能的成年人平均身高是6'2“（但他们对某一方向并不超级自信）。"
}, {
  "tag": "P",
  "text": "Lets imagine this person went and collected some data, and they observed a range of people between 5' and 6'. We can represent that data below, along with another normal curve that shows which values of average human height best explain the data:",
  "translation": "让我们想象这个人去收集了一些数据，他们观察到范围在5'和6'之间的人。 我们可以在下面表示该数据，以及另一条正常曲线，该曲线显示哪些平均人类身高值最能解释该数据："
}, {
  "tag": "FIGURE",
  "image": "https://miro.medium.com/max/30/0*kkaO7QpZeGOg9DRf.?q=20",
  "type": "image"
}, {
  "tag": "P",
  "text": "In Bayesian statistics, the distribution representing our beliefs about a parameter is called the prior distribution, because it captures our beliefs prior to seeing any data. The likelihood distribution summarizes what the observed data are telling us, by representing a range of parameter values accompanied by the likelihood that each each parameter explains the data we are observing. Estimating the parameter value that maximizes the likelihood distribution is just answering the question: what parameter value would make it most likely to observe the data we have observed? In the absence of prior beliefs, we might stop there.",
  "translation": "在贝叶斯统计中，代表我们对参数的信念的分布称为先验分布，因为它可以在查看任何数据之前捕获我们的信念。 可能性分布通过代表一系列参数值以及每个参数解释我们正在观察的数据的可能性来总结观察到的数据告诉我们的内容。 估计最大化似然分布的参数值只是在回答这个问题：什么参数值最有可能观察到我们所观察到的数据？ 如果没有先前的信念，我们可能会停在那里。"
}, {
  "tag": "P",
  "text": "The key to Bayesian analysis, however, is to combine the prior and the likelihood distributions to determine the posterior distribution. This tells us which parameter values maximize the chance of observing the particular data that we did, taking into account our prior beliefs. In our case, the posterior distribution looks like this:",
  "translation": "但是，贝叶斯分析的关键是结合先验分布和似然分布来确定后验分布。 这告诉我们，考虑到我们先前的信念，哪些参数值使观察我们所做的特定数据的机会最大化。 在我们的例子中，后验分布看起来像这样："
}, {
  "tag": "FIGURE",
  "image": "https://miro.medium.com/max/30/0*3M3HCM8goJjlS-KX.?q=20",
  "type": "image"
}, {
  "tag": "P",
  "text": "Above, the red line represents the posterior distribution. You can think of it as a kind of average of the prior and the likelihood distributions. Since the prior distribution is shorter and more spread out, it represents a set of belief that is ‘less sure’ about the true value of average human height. Meanwhile, the likelihood summarizes the data within a relatively narrow range, so it represents a ‘more sure’ guess about the true parameter value.",
  "translation": "上方的红线表示后验分布。 您可以将其视为先验分布和似然分布的平均值。 由于先前的分布更短且分布更广，因此它代表了一组“无法确定”平均人类身高的真实价值的信念。 同时，可能性在相对狭窄的范围内汇总了数据，因此它表示对真实参数值的“更有把握”的猜测。"
}, {
  "tag": "P",
  "text": "When the prior the likelihood are combined, the data (represented by the likelihood) dominate the weak prior beliefs of the hypothetical individual who had grown up among giants. Although that individual still believes the average human height is slightly higher than just what the data is telling him, he is mostly convinced by the data.",
  "translation": "当先验可能性组合时，数据（由可能性表示）支配了在巨人中成长的假设个体的弱先验信念。 尽管该人仍然认为人的平均身高略高于数据告诉他的高度，但他还是对数据深信不疑。"
}, {
  "tag": "P",
  "text": "In the case of two bell curves, solving for the posterior distribution is very easy. There is a simple equation for combining the two. But what if our prior and likelihood distributions weren’t so well-behaved? Sometimes it is most accurate to model our data or our prior beliefs using distributions which don’t have convenient shapes. What if our likelihood were best represented by a distribution with two peaks, and for some reason we wanted to account for some really wacky prior distribution? I’ve visualized that scenario below, by hand drawing an ugly prior distribution:",
  "translation": "在两个钟形曲线的情况下，求解后验分布非常容易。 有一个简单的方程式将两者结合起来。 但是，如果我们的先验分布和似然分布不是那么良好怎么办？ 有时，使用没有方便形状的分布来对我们的数据或我们先前的信念建模是最准确的。 如果我们的可能性最好用两个峰值的分布来表示，又由于某种原因，我们想解释一些确实古怪的先前分布，该怎么办？ 我通过手工绘制一个丑陋的先前分布图来可视化以下情况："
}, {
  "tag": "FIGURE",
  "image": "https://miro.medium.com/max/30/0*PcWai087HhpJtbkm.?q=20",
  "caption": "Visualizations rendered in Matplotlib, enhanced using MS Paint",
  "type": "image"
}, {
  "tag": "P",
  "text": "As before, there exists some posterior distribution that gives the likelihood for each parameter value. But its a little hard to see what it might look like, and it is impossible to solve for analytically. Enter MCMC methods.",
  "translation": "和以前一样，存在一些后验分布，这些后验分布给出了每个参数值的可能性。 但是很难看到它的外观，并且无法通过解析来解决。 输入MCMC方法。"
}, {
  "tag": "P",
  "text": "MCMC methods allow us to estimate the shape of a posterior distribution in case we can’t compute it directly. Recall that MCMC stands for Markov chain Monte Carlo methods. To understand how they work, I’m going to introduce Monte Carlo simulations first, then discuss Markov chains.",
  "translation": "MCMC方法可让我们估算后验分布的形状，以防无法直接计算后验分布。 回想一下，MCMC代表马尔可夫链蒙特卡罗方法。 为了了解它们的工作原理，我将首先介绍蒙特卡洛模拟，然后讨论马尔可夫链。"
}, {
  "tag": "H1",
  "text": "A Zero-Math Introduction to Markov Chain Monte Carlo Methods",
  "translation": "马尔可夫链蒙特卡罗方法的零数学介绍"
}, {
  "tag": "P",
  "text": "For many of us, Bayesian statistics is voodoo magic at best, or completely subjective nonsense at worst. Among the trademarks of the Bayesian approach, Markov chain Monte Carlo methods are especially mysterious. They’re math-heavy and computationally expensive procedures for sure, but the basic reasoning behind them, like so much else in data science, can be made intuitive. That is my goal here.",
  "translation": "对于我们中的许多人而言，贝叶斯统计数据充其量是巫术魔术，或者最坏的情况是完全主观的废话。 在贝叶斯方法的商标中，马尔可夫链蒙特卡洛方法尤其神秘。 它们肯定是数学运算繁重且计算量很大的过程，但是它们背后的基本推理（与数据科学中的其他许多事物一样）可以变得直观。 这是我的目标。"
}, {
  "tag": "PRE",
  "text": "(本文翻译自Ben Shaver的文章《A Zero-Math Introduction to Markov Chain Monte Carlo Methods》，参考：https://towardsdatascience.com/a-zero-math-introduction-to-markov-chain-monte-carlo-methods-dcba889e0c50)",
  "translation": "（本文翻译自Ben Shaver的文章《马尔可夫链蒙特卡洛方法的零数学介绍》，参考：https：//towardsdatascience.com/a-zero-math-introduction-to-markov-chain-monte-carlo- 方法-dcba889e0c50）"
}]