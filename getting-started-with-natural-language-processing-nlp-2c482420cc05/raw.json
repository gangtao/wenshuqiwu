[{
  "tag": "P",
  "text": "We’ve touched upon most of the basics but of course, there’s a lot more to NLP. However, this article is a good starting point and hopefully helpful for beginners because these were the first things I learned when I started off!",
  "translation": "我们已经介绍了大多数基础知识，但是NLP当然还有很多。 但是，本文是一个不错的起点，希望对初学者有所帮助，因为这是我刚开始学习的第一件事！"
}, {
  "tag": "H2",
  "text": "Toxic comments classifier",
  "translation": "有毒评论分类器"
}, {
  "tag": "H2",
  "text": "Toxic Comment Classification Challenge",
  "translation": "有毒评论分类挑战"
}, {
  "tag": "H3",
  "text": "Identify and classify toxic online comments",
  "translation": "识别和分类有毒的在线评论"
}, {
  "tag": "H1",
  "text": "Getting Started with Natural Language Processing (NLP) — preprocessing, word embeddings, text classification, and more!",
  "translation": "自然语言处理（NLP）入门-预处理，单词嵌入，文本分类等！"
}, {
  "tag": "H2",
  "text": "using simple Python libraries",
  "translation": "使用简单的Python库"
}, {
  "tag": "FIGURE",
  "image": "https://miro.medium.com/max/30/1*HpKh-MGZIq-GyRAiqEUnSw.png?q=20",
  "caption": "source",
  "type": "image",
  "file": "1*HpKh-MGZIq-GyRAiqEUnSw.png"
}, {
  "tag": "P",
  "text": "There’s so much going on in natural language processing these days (GRUs, LSTMs, XLNet, BERT and so on!). It can be confusing figuring out where to begin. This article talks about the basics of natural language processing including data cleaning, normalization, encoding, sentiment analysis and a simple text classifier using basic yet powerful Python libraries. This is often the first step before diving into complicated deep learning models.",
  "translation": "如今，自然语言处理领域发生了很多事情（GRU，LSTM，XLNet，BERT等！）。 弄清楚从哪里开始可能会造成混淆。 本文讨论了自然语言处理的基础，包括使用基本但功能强大的Python库进行的数据清理，规范化，编码，情感分析和简单的文本分类器。 这通常是进入复杂的深度学习模型之前的第一步。"
}, {
  "tag": "H1",
  "text": "Data Cleaning and normalization",
  "translation": "数据清理和规范化"
}, {
  "tag": "P",
  "text": "Depending on the nature of the problem, this step may or may not be required. If our model is trying to learn the language to the largest extent, it may be best to use the data in its raw format, in fact, modern deep learning techniques recommend not to remove stop words, emojis or lowercase letters, because they provide additional context. However, if you’re trying to do a trend analysis or classification based on certain word occurrences (like in a bag-of-words model), it helps to perform this step. There are a few common preprocessing steps I’d like to highlight here:",
  "translation": "根据问题的性质，可能需要也可能不需要此步骤。 如果我们的模型正在尝试最大程度地学习语言，则最好以原始格式使用数据，实际上，现代深度学习技术建议不要删除停用词，表情符号或小写字母，因为它们会提供附加信息 上下文。 但是，如果您尝试根据某些单词的出现情况进行趋势分析或分类（例如在词袋模型中），则有助于执行此步骤。 我想在此处重点介绍一些常见的预处理步骤："
}, {
  "tag": "OL",
  "texts": ["Removing punctuation: When trying to train a machine learning model, it helps to reduce overfitting by removing punctuation (like !,* etc.). However, be careful to not remove something important, for example, question marks (?) help to recognize questions.", "Removing emojis: Sometimes people attach emojis to words without spaces (for example: you❤ ) making it difficult to interpret such words. Removing emojis can help with such cases. Again, be careful while removing these as emojis might actually be really useful for tasks like sentiment analysis and topic classification.", "Removing stop words: For tasks like data exploration and trend analysis, it might not be very useful to see common words like ‘the’, ‘and’ , ‘of’ etc. The sklearn package actually has a collection of commonly used English stop words that we can use to remove these.", "Making all text lowercase: This is the simplest way to normalize text. (after all, BeTTer and better do have the same semantic implication)", "Stemming words: Another way of normalizing is by replacing derived words with their root form (eg: ‘posting’, ‘posted’, ‘posts’ are all replaced by ‘post’). To stem words we use the PorterStemmer util provided by nltk.", "Extracting/Removing hashtags and mentions: Hashtags and mentions can be very useful in identifying trends in your data. It helps to extract them out of your text and analyze them separately."],
  "translations": ["删除标点符号：尝试训练机器学习模型时，它可以通过删除标点符号（如！，*等）来减少过度拟合。 但是，请注意不要删除重要的内容，例如，问号（？）有助于识别问题。", "删除表情符号：有时人们将表情符号附加到没有空格的单词（例如：you❤）上，从而难以解释此类单词。 删除表情符号可以在这种情况下提供帮助。 同样，删除这些表情符号时要小心，因为表情符号实际上可能对情感分析和主题分类等任务非常有用。", "删除停用词：对于诸如数据探索和趋势分析之类的任务，查看诸如“ the”，“ and”，“ of”等常用词可能不是很有用。sklearn软件包实际上包含了一组常用的英语停用词 我们可以用来删除这些。", "使所有文本变为小写：这是标准化文本的最简单方法。 （毕竟，BeTTer和更好的语言确实具有相同的语义含义）", "词干：规范化的另一种方法是将派生词替换为其词根形式（例如，“发布”，“已发布”，“帖子”全部替换为“帖子”）。 为了阻止单词，我们使用nltk提供的PorterStemmer util。", "提取/删除主题标签和注释：主题标签和注释在识别数据趋势方面非常有用。 它有助于将它们从文本中提取出来并分别进行分析。"]
}, {
  "tag": "P",
  "text": "Here’s a simple function to perform the above-mentioned tasks:",
  "translation": "这是执行上述任务的简单功能："
}, {
  "tag": "FIGURE",
  "type": "code",
  "raw": "https://gist.github.com/aakanksha-ns/b5fc64fbb040b6a9bb665dd4b838fb50/raw/5200f0bba8a2671e2fab7b9b570103469844651f/preprocess_text.py",
  "code": "import re\nfrom sklearn.feature_extraction.text import ENGLISH_STOP_WORDS\nfrom nltk.stem.porter import PorterStemmer\nimport emoji\nimport string\n\ndef preprocess_text(text, remove_stop = True, stem_words = False, remove_mentions_hashtags = True):\n  \"\"\"\n  eg:\n  input: preprocess_text(\"@water #dream hi hello where are you going be there tomorrow happening happen happens\",  \n  stem_words = True) \n  output: ['tomorrow', 'happen', 'go', 'hello']\n  \"\"\"\n    \n    # Remove emojis\n    emoji_pattern = re.compile(\"[\" \"\\U0001F1E0-\\U0001F6FF\" \"]+\", flags=re.UNICODE)\n    text = emoji_pattern.sub(r\"\", text)\n    text = \"\".join([x for x in text if x not in emoji.UNICODE_EMOJI])\n\n    if remove_mentions_hashtags:\n        text = re.sub(r\"@(\\w+)\", \" \", text)\n        text = re.sub(r\"#(\\w+)\", \" \", text)\n        \n    text = re.sub(r\"[^\\x00-\\x7F]+\", \" \", text)\n    regex = re.compile('[' + re.escape(string.punctuation) + '0-9\\\\r\\\\t\\\\n]') # remove punctuation and numbers\n    nopunct = regex.sub(\" \", text.lower())\n    words = (''.join(nopunct)).split()\n    \n    if(remove_stop):\n        words = [w for w in words if w not in ENGLISH_STOP_WORDS]\n        words = [w for w in words if len(w) > 2]  # remove a,an,of etc.\n        \n    if(stem_words):\n        stemmer = PorterStemmer()\n        words = [stemmer.stem(w) for w in words]\n        \n    return list(words) "
}, {
  "tag": "H1",
  "text": "Word vectors — What are they?",
  "translation": "字向量-它们是什么？"
}, {
  "tag": "P",
  "text": "Machine learning algorithms are only capable of processing fixed-length numerical inputs i.e, they cannot take string inputs to process textual data! This is where word vectors come in, where we represent each word using vectors of fixed length. Individual word vectors are then used to encode sentences.",
  "translation": "机器学习算法只能处理固定长度的数字输入，即它们不能采用字符串输入来处理文本数据！ 这是单词向量进入的地方，我们使用固定长度的向量表示每个单词。 然后将各个单词向量用于编码句子。"
}, {
  "tag": "H2",
  "text": "One-hot encoding:",
  "translation": "一键编码："
}, {
  "tag": "P",
  "text": "This is the simplest way of encoding words. It assumes a bag-of-words representation where each word is considered as an independent entity and word relationships are ignored (for example, job and occupation are considered as completely independent words even though they practically have the same meaning). This method involves creating a vocabulary of distinct words from the entire corpus, the length of this vocabulary being the length of each word vector. Each vector has a designated index for itself in the word vector and that index is marked 1 while others are marked 0 to represent the particular word.",
  "translation": "这是编码单词的最简单方法。 它假定使用词袋表示法，其中每个单词都被视为一个独立的实体，并且单词关系被忽略（例如，尽管工作和职业实际上具有相同的含义，但它们被视为完全独立的单词）。 该方法涉及从整个语料库创建不同单词的词汇表，该词汇表的长度是每个单词向量的长度。 每个向量在单词向量中都有自己的指定索引，该索引标记为1，而其他索引标记为0以表示特定单词。"
}, {
  "tag": "P",
  "text": "Example:",
  "translation": "例："
}, {
  "tag": "FIGURE",
  "image": "https://miro.medium.com/max/30/1*J9-5WuqxuibdBOnxrwjr_w.png?q=20",
  "type": "image",
  "file": "1*J9-5WuqxuibdBOnxrwjr_w.png"
}, {
  "tag": "P",
  "text": "The vocabulary here consists of 9 distinct words and these words can be one hot encoded into vectors of length 9. The word vector representations aregoing : [1,0,0,0,0,0,0,0,0] good : [0,1,0,0,0,0,0,0,0] and so on..",
  "translation": "这里的词汇表由9个不同的词组成，这些词可以被一个热编码为长度为9的向量。这些词向量的表示形式如下：[1,0,0,0,0,0,0,0,0,0]好：[ 0,1,0,0,0,0,0,0,0]，依此类推。"
}, {
  "tag": "P",
  "text": "Using this representation the text Tomorrow will be a good day can be encoded into: [0,1,1,0,1,1,0,1,0]. Notice how the word will is ignored because it doesn’t exist in the vocabulary at all. Having a good and extensive vocabulary is necessary for making this model work well. Also note how the word relationships (the order of occurrence, semantic relationships) are completely ignored in this representation.",
  "translation": "使用此表示形式，明天将是美好的一天的文本可以编码为：[0,1,1,0,1,1,0,1,0]。 注意单词将被忽略，因为它根本不存在于词汇表中。 要使此模型正常工作，必须具有良好而广泛的词汇表。 还要注意在此表示形式中如何完全忽略单词关系（出现的顺序，语义关系）。"
}, {
  "tag": "H2",
  "text": "Word2Vec word embeddings:",
  "translation": "Word2Vec词嵌入："
}, {
  "tag": "P",
  "text": "This method of word encoding (commonly known as word embeddings) takes context into consideration. For example, we can expect the words king and royal to have a smaller spatial distance than parrot and honey . Word2vec uses a shallow two-layer neural network to perform a specific task (based on the method used) and learn weights for the hidden layer for every word. These learned hidden layer weights are used as our final word vectors. You can read the original paper to get an in-depth understanding of how these word vectors are obtained. But at a high level, these are the two common methods of obtaining context-based word vectors using Word2Vec:",
  "translation": "单词编码的这种方法（通常称为单词嵌入）考虑了上下文。 例如，我们可以预期国王和王室一词的空间距离要比鹦鹉和蜂蜜小。 Word2vec使用浅层两层神经网络执行特定任务（基于所使用的方法），并为每个单词学习隐藏层的权重。 这些学习的隐藏层权重用作我们的最终单词向量。 您可以阅读原始论文，以深入了解如何获得这些单词向量。 但总的来说，这是使用Word2Vec获取基于上下文的单词向量的两种常用方法："
}, {
  "tag": "P",
  "text": "CBOW (Continuous Bag of words):",
  "translation": "CBOW（连续词袋）："
}, {
  "tag": "FIGURE",
  "image": "https://miro.medium.com/max/23/1*tdm-R9q0kNP0BnSEvruD-Q.png?q=20",
  "caption": "source : https://arxiv.org/pdf/1301.3781.pdf",
  "type": "image",
  "file": "1*tdm-R9q0kNP0BnSEvruD-Q.png"
}, {
  "tag": "P",
  "text": "The CBOW model architecture tries to predict the current target word (the center word) based on the source context words (surrounding words).",
  "translation": "CBOW模型体系结构尝试根据源上下文单词（环绕单词）预测当前的目标单词（中心单词）。"
}, {
  "tag": "P",
  "text": "Skip — Gram model:",
  "translation": "跳过—语法模型："
}, {
  "tag": "FIGURE",
  "image": "https://miro.medium.com/max/24/1*9h65DVJOmReTO9d2x26t-Q.png?q=20",
  "caption": "source: https://arxiv.org/pdf/1301.3781.pdf",
  "type": "image",
  "file": "1*9h65DVJOmReTO9d2x26t-Q.png"
}, {
  "tag": "P",
  "text": "The Skip-gram model architecture tries to achieve the reverse of what the CBOW model does. It tries to predict the source context words (surrounding words) given a target word (the center word).",
  "translation": "Skip-gram模型体系结构试图实现与CBOW模型相反的功能。 它尝试在给定目标单词（中心单词）的情况下预测源上下文单词（环绕单词）。"
}, {
  "tag": "P",
  "text": "The window size (number of surrounding words considered on each side) is a hyperparameter in both cases.",
  "translation": "在两种情况下，窗口大小（每侧考虑的周围单词数）都是超参数。"
}, {
  "tag": "H2",
  "text": "Glove:",
  "translation": "手套："
}, {
  "tag": "P",
  "text": "Glove is quite similar to Word2vec but unlike Word2vec, Glove takes advantage of the global co-occurrences of words rather than just the local context which makes it more powerful in some ways. Again, you can get a better understanding by going through the original paper.",
  "translation": "Glove与Word2vec非常相似，但与Word2vec不同，Glove利用了单词的全局共现，而不仅仅是局部上下文，这使它在某些方面更加强大。 同样，通过阅读原始论文，您可以更好地理解。"
}, {
  "tag": "H1",
  "text": "Word Embeddings — How do I use them?",
  "translation": "词嵌入-如何使用它们？"
}, {
  "tag": "P",
  "text": "Now that we have a rough idea about what word embeddings are and why they are useful, let’s talk about how we can use them to our advantage.",
  "translation": "现在我们对什么是词嵌入及其为何有用有了一个大概的想法，让我们来谈谈如何利用它们来发挥我们的优势。"
}, {
  "tag": "H2",
  "text": "Using pre-trained word vectors:",
  "translation": "使用预训练的单词向量："
}, {
  "tag": "P",
  "text": "There are many publicly available pre-trained word vectors of different vector lengths like Glove, fasttext, etc. These have been trained on massive corpora ( Wikipedia, twitter and common crawl datasets) and can be downloaded and used to encode words in our corpus.",
  "translation": "有许多公开的，经过预先训练的，具有不同向量长度的预训练词向量，例如Glove，fasttext等。这些已经在大规模语料库（Wikipedia，twitter和常见的抓取数据集）上进行了训练，可以下载并用于在我们的语料库中对词进行编码。"
}, {
  "tag": "P",
  "text": "Example: Finding the most similar document to a given document using word vector similarity",
  "translation": "示例：使用词向量相似度查找与给定文档最相似的文档"
}, {
  "tag": "P",
  "text": "Problem Statement:",
  "translation": "问题陈述："
}, {
  "tag": "P",
  "text": "Given a set of documents belonging to different topics (training set), when given a new document can we find the most similar document to it from the original set?",
  "translation": "给定一组属于不同主题的文档（培训集），当给定一个新文档时，我们能否从原始文档集中找到与其最相似的文档？"
}, {
  "tag": "P",
  "text": "Approach:",
  "translation": "方法："
}, {
  "tag": "OL",
  "texts": ["Load the pre-trained word vector file into a dictionary with the word as key and its vector representation as the value.", "Find the centroid vector for each document in the training set by averaging the word vectors of words that exist in the particular document (ignore words that are not part of the vocabulary)", "Find the centroid of the new document, pick the document from the training set whose centroid is closest to the new document’s centroid (using a suitable measure of similarity, like Euclidean distance, cosine similarity, etc.)"],
  "translations": ["将经过预训练的单词向量文件加载到字典中，以单词为键，其向量表示为值。", "通过对特定文档中存在的单词的单词矢量求平均值，找到训练集中每个文档的质心矢量（忽略不属于词汇表的单词）", "找到新文档的质心，从训练集中选择其质心最接近新文档质心的文档（使用合适的相似度度量，例如欧氏距离，余弦相似度等）"]
}, {
  "tag": "P",
  "text": "Code:",
  "translation": "码："
}, {
  "tag": "P",
  "text": "Here’s some helper functions load the glove dictionary, find the centroid and find the distance between centroids:",
  "translation": "这是一些辅助函数，用于加载手套字典，查找质心以及查找质心之间的距离："
}, {
  "tag": "FIGURE",
  "type": "code",
  "raw": "https://gist.github.com/aakanksha-ns/b7a8595927abdcaed479498b103cc011/raw/409a876a0d5a0dc0df288de48918d8244197e7c0/find_centroid.py",
  "code": "import numpy as np\n\n#loading the glove file into a dictionary of words\ndef load_glove(filename):\n    glove_dict = {}\n    with open(filename) as f:\n        file_content = f.readlines()\n    for line in file_content:\n        line_content = line.split()\n        glove_dict[line_content[0]] = np.array(line_content[1:], dtype=float)\n    return glove_dict\n\n#get centroid of a particular document\ndef get_centroid(text, gloves):\n    words_list = preprocess_text(text)\n    word_vec_sum = 0\n    words_count = 0\n    for w in words_list:\n        if w in gloves:\n            word_vec_sum += gloves[w]\n            words_count += 1\n    if words_count:\n        return word_vec_sum/words_count\n    else:\n        return 0\n\n#get distance between two centroids\ndef get_distance (a,b):\n    return (np.linalg.norm(a - b))"
}, {
  "tag": "H2",
  "text": "Training to generate word vectors from scratch:",
  "translation": "训练从头开始生成单词向量："
}, {
  "tag": "P",
  "text": "If you want to find word vectors for your particular corpus, you can use the gensim package for training.",
  "translation": "如果要查找特定语料库的单词向量，可以使用gensim包进行训练。"
}, {
  "tag": "P",
  "text": "Example:",
  "translation": "例："
}, {
  "tag": "P",
  "text": "Code:",
  "translation": "码："
}, {
  "tag": "FIGURE",
  "type": "code",
  "raw": "https://gist.github.com/aakanksha-ns/eef77e308acbd76c88c3a7a9e26cfa58/raw/73ae1f5a08c31e311aae51ddf299d5a9684a474a/gensim_medium_demo.py",
  "code": "import gensim\nwith open('./potato.txt') as f:\n    text = f.read()\nwords_list = [preprocess_text(text)]\nmodel = gensim.models.Word2Vec(\n        words_list,\n        size=150,\n        window=2,\n        min_count=1,\n        workers=10,\n        iter=10)\nprint('vocabulary: ', model.wv.vocab.keys(),'\\n')\nprint(model.wv.most_similar('starchy'),'\\n')\nprint(model.wv.word_vec('potato'),'\\n')"
}, {
  "tag": "P",
  "text": "Output:",
  "translation": "输出："
}, {
  "tag": "FIGURE",
  "image": "https://miro.medium.com/max/30/1*zt9axCd5lF8oXjs0XuE1DQ.png?q=20",
  "type": "image",
  "file": "1*zt9axCd5lF8oXjs0XuE1DQ.png"
}, {
  "tag": "P",
  "text": "In the example above, I’ve just used two lines from this Wikipedia page. The training is very fast and simple, all you need to input is the list of words, size of the word vectors you need, the window size (number of surrounding words to be considered), and the minimum number of occurrences of the word for it to be considered as part of the vocabulary. It’s easy to inspect the vocabulary, obtain the vector and also look at the most common words from the corpus. Of course, training from scratch might not always yield as good results as the pre-trained ones but it is good for problems that involves data that looks very different from the datasets used in pretraining.",
  "translation": "在上面的示例中，我仅使用了Wikipedia页面上的两行内容。 训练是非常快速和简单的，您只需要输入单词列表，所需单词矢量的大小，窗口大小（要考虑的周围单词的数量）以及出现该单词的最小次数 它被视为词汇的一部分。 检查词汇表，获取向量以及查看语料库中最常用的词都很容易。 当然，从头开始的训练可能并不总是能获得与预训练的结果一样好的结果，但是对于涉及数据看起来与预训练中使用的数据集截然不同的问题的好处。"
}, {
  "tag": "H1",
  "text": "Data Exploration",
  "translation": "数据探索"
}, {
  "tag": "P",
  "text": "EDA with text data is not as straightforward as tabular or numerical data. However, there are some libraries that can make these tasks easier. For the rest of this article I’ve used the following dataset from Kaggle:",
  "translation": "带有文本数据的EDA不如表格或数字数据那么简单。 但是，有些库可以使这些任务更容易。 对于本文的其余部分，我使用了Kaggle的以下数据集："
}, {
  "tag": "H2",
  "text": "Toxic Comment Classification Challenge",
  "translation": "有毒评论分类挑战"
}, {
  "tag": "H3",
  "text": "Identify and classify toxic online comments",
  "translation": "识别和分类有毒的在线评论"
}, {
  "tag": "H2",
  "text": "Exploration using spacy:",
  "translation": "使用spacy探索："
}, {
  "tag": "P",
  "text": "Spacy is a very powerful NLP library that has a variety of uses. It can be used for named entity recognition, identifying the part of speech a word belongs to and even give the word vector and sentiment of the word.",
  "translation": "Spacy是一个非常强大的NLP库，具有多种用途。 它可用于命名实体识别，识别单词所属的语音部分，甚至给出单词的矢量和情感。"
}, {
  "tag": "P",
  "text": "Code:",
  "translation": "码："
}, {
  "tag": "FIGURE",
  "type": "code",
  "raw": "https://gist.github.com/aakanksha-ns/6e6e15c75a6ec0fb79b11dbd9a3ab0c0/raw/0e5fe6c86bab164aefc20efd43d7c130812d7cfb/spacy_demo.py",
  "code": "import spacy\nnlp = spacy.load(\"en_core_web_sm\")\ntext = ' '.join(toxic_data[:1000]['comment_text'])\ndoc = nlp(' '.join(preprocess_text(text)))\n\nx = doc[0]\nprint('word:', x, '\\n')\nprint('part of speech:', x.pos_, '\\n')\nprint('sentiment:', x.sentiment,'\\n')\nprint('sentiment:', x.sentiment,'\\n')\nprint('word vector:', x.vector)"
}, {
  "tag": "P",
  "text": "Output:",
  "translation": "输出："
}, {
  "tag": "FIGURE",
  "image": "https://miro.medium.com/max/30/1*xfBtTqeUzatGcGIv-56w-A.png?q=20",
  "type": "image",
  "file": "1*xfBtTqeUzatGcGIv-56w-A.png"
}, {
  "tag": "P",
  "text": "The nlp function from spacy converts each word into a token having various attributes like the ones mentioned in the above example.",
  "translation": "来自spacy的nlp函数将每个单词转换为具有各种属性的令牌，例如上面示例中提到的那些属性。"
}, {
  "tag": "H2",
  "text": "Wordcloud:",
  "translation": "Wordcloud："
}, {
  "tag": "P",
  "text": "Wordclouds are a simple yet interesting way to visualize how frequently various words appear in our corpus. Let’s take the most frequently occurring nouns in our comments’ data for example:",
  "translation": "Wordclouds是一种简单而有趣的方法，用于可视化各种单词在我们的语料库中出现的频率。 让我们以评论数据中最常出现的名词为例："
}, {
  "tag": "P",
  "text": "Code:",
  "translation": "码："
}, {
  "tag": "FIGURE",
  "type": "code",
  "raw": "https://gist.github.com/aakanksha-ns/bc20cdc1302a21eebfd39c405b58751c/raw/9eefc00df878dadce2a8ffe3d6ca192965055ee4/wordcloud_medium_demo.py",
  "code": "from wordcloud import WordCloud\nimport matplotlib.pyplot as plt\n\ndef make_wc(word_list):\n    wordcloud = WordCloud()\n    wordcloud.fit_words(dict(Counter(word_list).most_common(40)))\n\n    fig=plt.figure(figsize=(10, 10))\n    plt.imshow(wordcloud)\n    plt.axis(\"off\")\n    plt.show()\n    \nmake_wc([token.text for token in doc if token.pos_ in ['NOUN']])"
}, {
  "tag": "P",
  "text": "Output:",
  "translation": "输出："
}, {
  "tag": "FIGURE",
  "image": "https://miro.medium.com/max/30/1*Vv8YNv2eOTFQbGuYX6r-pg.png?q=20",
  "caption": "article, talk, and page are the most frequently occurring nouns",
  "type": "image",
  "file": "1*Vv8YNv2eOTFQbGuYX6r-pg.png"
}, {
  "tag": "H1",
  "text": "Sentiment Analysis",
  "translation": "情绪分析"
}, {
  "tag": "P",
  "text": "A very common task in NLP is identifying how positive or negative a particular comment or piece of text is. The vaderSentiment package provides a quick and easy way to do this:",
  "translation": "在NLP中，一项非常常见的任务是确定特定评论或文本的正面或负面。 vaderSentiment软件包提供了一种快速简便的方法："
}, {
  "tag": "P",
  "text": "Code:",
  "translation": "码："
}, {
  "tag": "PRE",
  "text": "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzersentiment_analyzer = SentimentIntensityAnalyzer()def get_sentiment_score(text):    return sentiment_analyzer.polarity_scores(text)['compound']",
  "translation": "从vaderSentiment.vaderSentiment导入SentimentIntensityAnalyzersentiment_analyzer = SentimentIntensityAnalyzer（）def get_sentiment_score（text）：返回sentiment_analyzer.polarity_scores（text）['compound']"
}, {
  "tag": "P",
  "text": "Output:",
  "translation": "输出："
}, {
  "tag": "FIGURE",
  "image": "https://miro.medium.com/max/30/1*106P5WiWxV15suIfXScRPw.png?q=20",
  "caption": "The sentiment score for a toxic comment seems to be close to -1 whereas a non-toxic one has a score greater than 0 as expected",
  "type": "image",
  "file": "1*106P5WiWxV15suIfXScRPw.png"
}, {
  "tag": "H1",
  "text": "Text Classification",
  "translation": "文字分类"
}, {
  "tag": "P",
  "text": "For classification, one of the easiest libraries I’ve used is fasttext . It was released by Facebook in 2016 and uses a linear technique both for combining the word vectors into the vector representing the text and for computing the classification criterion. It takes very little time to train and gives decent results for most common text-classification problems. It can be used to come up with a baseline model. You can read the original paper to get a better understanding of the mechanics behind the fasttext classifier.",
  "translation": "对于分类，我使用过的最简单的库之一是fasttext。 它于2016年由Facebook发布，并使用线性技术将单词向量组合到代表文本的向量中，并用于计算分类标准。 它花费很少的时间进行培训，并为大多数常见的文本分类问题提供了不错的结果。 它可以用来提出基线模型。 您可以阅读原始文章，以更好地了解fasttext分类器背后的机制。"
}, {
  "tag": "P",
  "text": "This is how I used fasttext to classify toxic vs non-toxic comments:",
  "translation": "这就是我使用fasttext对有毒评论与无毒评论进行分类的方式："
}, {
  "tag": "H2",
  "text": "Toxic comments classifier",
  "translation": "有毒评论分类器"
}, {
  "tag": "PRE",
  "text": "(本文翻译自Aakanksha NS的文章《Getting Started with Natural Language Processing (NLP) — preprocessing, word embeddings, text classification, and more!》，参考：https://towardsdatascience.com/getting-started-with-natural-language-processing-nlp-2c482420cc05)",
  "translation": "（本文翻译自Aakanksha NS的文章《自然语言处理入门-预处理，词嵌入，文本分类等等！》，参考：https：//towardsdatascience.com/getting-started-with-natural- language-processing-nlp-2c482420cc05）"
}]