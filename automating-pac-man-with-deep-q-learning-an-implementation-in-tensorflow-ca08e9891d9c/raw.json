[{
  "tag": "H1",
  "text": "Automating Pac-man with Deep Q-learning: An Implementation in Tensorflow.",
  "translation": "通过深度Q学习使吃豆人自动化：Tensorflow中的一种实现。"
}, {
  "tag": "H2",
  "text": "Fundamentals of Reinforcement Learning",
  "translation": "强化学习基础"
}, {
  "tag": "H1",
  "text": "Introduction",
  "translation": "介绍"
}, {
  "tag": "P",
  "text": "Over the course of our articles covering the fundamentals of reinforcement learning at GradientCrescent, we’ve studied both model-based and sample-based approaches to reinforcement learning. Briefly, the former class is characterized by requiring knowledge of the complete probability distributions of all possible state transitions, and can be exemplified by Markovian Decision Processes. In contrast, sample-based learning methods allow for the determination of state values simply through repeated observations, without the need for transition dynamics. Within this domain, we’ve covered both Monte Carlo and Temporal Difference learning. Briefly, the two can be separated by the frequency of state-value updates: while a Monte Carlo approach requires that an episode be finished for a round of updates to take place, Temporal Difference approaches update intra-episode incrementally, using old estimations of state-values together with discounted rewards to generate new updates.",
  "translation": "在涉及GradientCrescent强化学习基础的文章中，我们研究了基于模型和基于样本的强化学习方法。 简而言之，前一类的特征是需要了解所有可能状态转换的完整概率分布，并且可以通过马尔可夫决策过程来举例说明。 相反，基于样本的学习方法只需要通过反复观察即可确定状态值，而无需进行转换动力学。 在这一领域中，我们涵盖了蒙特卡洛和时差学习。 简而言之，可以用状态值更新的频率将两者分开：虽然蒙特卡洛方法要求完成一集才能进行一轮更新，但时差方法使用状态的旧估计来增量更新内集 值与折扣奖励一起生成新的更新。"
}, {
  "tag": "P",
  "text": "The rapid reactivity of TD or “online” learning approaches makes them suitable for highly dynamic environments, as the values of states and actions is continuously updated throughout time through sets of estimates. Perhaps most notably, TD is the foundation of Q-learning, a more advanced algorithm used to train agents tackling game environments such as those observed in the OpenAI Atari gyms, and the focus of this article.",
  "translation": "TD或“在线”学习方法的快速反应性使其适合于高度动态的环境，因为状态和动作的值会通过估算集在整个时间内不断更新。 也许最值得注意的是，TD是Q学习的基础，Q学习是一种更高级的算法，用于训练特工应对诸如OpenAI Atari体育馆中观察到的游戏环境之类的游戏环境，也是本文的重点。"
}, {
  "tag": "FIGURE",
  "type": "code"
}, {
  "tag": "H1",
  "text": "Going Beyond TD: SARSA & Q-learning",
  "translation": "超越TD：SARSA和Q学习"
}, {
  "tag": "P",
  "text": "Recall that in Temporal Difference learning, we observed that an agent behaves cyclically in an environment, through sequence of States (S), Actions (A), and (Rewards).",
  "translation": "回想一下在时间差异学习中，我们观察到一个主体通过状态（S），动作（A）和（奖励）的顺序在环境中循环地行为。"
}, {
  "tag": "FIGURE",
  "image": "https://miro.medium.com/max/30/0*Hs2L2mUcseEtUMcR?q=20",
  "type": "image",
  "file": "0*Hs2L2mUcseEtUMcR"
}, {
  "tag": "FIGURE",
  "image": "https://miro.medium.com/max/30/0*lVucLC7_lkZ205za.png?q=20",
  "type": "image",
  "file": "0*lVucLC7_lkZ205za.png"
}, {
  "tag": "P",
  "text": "Due to this cyclic behavior, we can update the value of the previous state as soon as we reach the next state. However, we can expand the scope of our training to include state-action values, just as we did with Markov Decision Processes prior. This is generally known as SARSA. Let’s compare the state-action and state-value TD update equations:",
  "translation": "由于这种周期性行为，我们可以在到达下一个状态后立即更新前一个状态的值。 但是，就像我们之前对马尔可夫决策过程所做的那样，我们可以将培训的范围扩展到包括国家行动值。 这通常称为SARSA。 让我们比较状态动作和状态值TD更新方程："
}, {
  "tag": "FIGURE",
  "image": "https://miro.medium.com/max/30/1*PyhofFmoKZn0bu1Ud2sLuw.png?q=20",
  "type": "image",
  "file": "1*PyhofFmoKZn0bu1Ud2sLuw.png"
}, {
  "tag": "P",
  "text": "Q-learning takes this a step further by forcing a selection of the action with the highest action value during an update, in a similar way to what’s observed with Bellman Optimality Equations. We can inspect SARSA and Q-learning next to the Bellman and Bellman Optimality Equations, below:",
  "translation": "Q学习通过在更新过程中强制选择具有最高动作值的动作来更进一步，这类似于使用Bellman最优性方程式观察到的方式。 我们可以在下面的Bellman和Bellman最优性方程式旁边检查SARSA和Q学习："
}, {
  "tag": "FIGURE",
  "image": "https://miro.medium.com/max/30/1*qGtUpZKJ8FhXx-UD6q3ulg.png?q=20",
  "type": "image",
  "file": "1*qGtUpZKJ8FhXx-UD6q3ulg.png"
}, {
  "tag": "P",
  "text": "You may be wondering about how ensure complete exploration of our state-action space, given the need to constantly select actions for a state with the highest existing action-value. In theory, we could be avoiding the optimal action simply by failing to evaluate it in the first place. To encourage exploration, we can use a decaying e-greedy policy, essentially forcing the agent to select an apparent sub-optimal action in order to learn more about its value, at a certain percentage of the time. By introducing a decaying process, we can limit exploration once all of the states have been evaluated, after which we’ll permanently select the optimal actions for each state.",
  "translation": "考虑到需要不断地为现有动作值最高的状态选择动作，您可能想知道如何确保对状态动作空间的完整探索。 从理论上讲，我们可能仅通过不首先评估就可以避免最佳行动。 为了鼓励探索，我们可以使用递减的电子贪婪策略，从本质上讲，它迫使代理程序选择一个明显的次优操作，以便在一定百分比的时间内了解其价值。 通过引入衰减过程，我们可以在评估完所有状态后限制探索，然后我们将为每个状态永久选择最佳操作。"
}, {
  "tag": "P",
  "text": "As we’ve tackled Pong before with a MDP-based model, let’s take what we’ve learned about Q-learning and apply it to a game of Atari’s Ms. Pac-man.",
  "translation": "在我们之前使用基于MDP的模型处理Pong时，让我们了解一下有关Q学习的知识，并将其应用于Atari的《吃豆人》游戏。"
}, {
  "tag": "H1",
  "text": "Implementation",
  "translation": "实作"
}, {
  "tag": "P",
  "text": "Our Google Colaboratory implementation is written in Python utilizing Tensorflow Core, and can be found on the GradientCrescent Github. It’s based on that by Ravichandiran et. al, but upgraded to be compatible with Tensorflow 2.0, and significantly expanded to facilitate improved visualization and explanations. As the implementation for this approach is quite convoluted, let’s summarize the order of actions required:",
  "translation": "我们的Google合作实验室实现是使用Tensorflow Core用Python编写的，可以在GradientCrescent Github上找到。 它基于Ravichandiran等人的观点。 ，但已升级为与Tensorflow 2.0兼容，并进行了显着扩展以促进更好的可视化和说明。 由于此方法的实现非常复杂，因此，我们总结一下所需的操作顺序："
}, {
  "tag": "OL",
  "texts": ["We define our Deep Q-learning neural network. This is a CNN that takes in-game screen images and outputs the probabilities of each of the actions, or Q-values, in the Ms-Pacman gamespace. To acquire a tensor of probabilitieses, we do not include any activation function in our final layer.", "As Q-learning require us to have knowledge of both the current and next states, we need to start with data generation. We feed preprocessed input images of the game space, representing initial states s, into the network, and acquire the initial probability distribution of actions, or Q-values. Before training, these values will appear random and sub-optimal.", "With our tensor of probabilities, we then select the action with the current highest probability using the argmax() function, and use it to build an epsilon greedy policy.", "Using our policy, we’ll then select the action a, and evaluate our decision in the gym environment to receive information on the new state s’, the reward r, and whether the episode has been finished.", "We store this combination of information in a buffer in the list form <s,a,r,s’,d>, and repeat steps 2–4 for a preset number of times to build up a large enough buffer dataset.", "Once step 5 has finished,we move to generate our target y-values, R’ and A’, that are required for the loss calculation. While the former is simply discounted from R, we obtain the A’ by feeding S’ into our network.", "With all of our components in place, we can then calculate the loss to train our network.", "Once training has finished, we’ll evaluate the performance of our agent graphically and through a demonstration."],
  "translations": ["我们定义了我们的深度Q学习神经网络。 这是一个CNN，可获取游戏中的屏幕图像并输出Ms-Pacman游戏空间中每个动作或Q值的概率。 为了获得概率张量，我们在最后一层不包括任何激活函数。", "由于Q学习需要我们了解当前状态和下一状态，因此我们需要从数据生成开始。 我们将代表初始状态s的游戏空间的预处理输入图像输入网络，并获取动作或Q值的初始概率分布。 在训练之前，这些值将显示为随机且次优。", "利用概率张量，然后使用argmax（）函数选择当前概率最高的操作，并使用它来构建epsilon贪婪策略。", "然后，根据我们的政策，我们选择动作a，并在体育馆环境中评估我们的决定，以接收有关新状态s，奖励r和情节是否结束的信息。", "我们将此信息组合以列表形式<s，a，r，s’，d>存储在缓冲区中，并重复步骤2-4进行预设次数，以建立足够大的缓冲区数据集。", "第5步完成后，我们开始生成损耗计算所需的目标y值R'和A'。 尽管前者只是从R打折而来，但我们通过将S'输入到我们的网络中来获得A'。", "有了所有组件之后，我们就可以计算损失来训练我们的网络。", "培训结束后，我们将以图形方式并通过演示评估代理的性能。"]
}, {
  "tag": "P",
  "text": "Let’s get started. With Tensorflow 2 on the horizon for Colaboratory environments, we’ve converted our code to be TF2 compliant, using the new compat package. Note that this code is not TF2 native.",
  "translation": "让我们开始吧。 随着Tensorflow 2在协作环境中的出现，我们已经使用新的compat软件包将代码转换为TF2兼容。 请注意，此代码不是TF2本机代码。"
}, {
  "tag": "P",
  "text": "Let’s by importing all of the necessary packages, including the OpenAI gym environments and Tensorflow core.",
  "translation": "让我们导入所有必需的程序包，包括OpenAI体育馆环境和Tensorflow核心。"
}, {
  "tag": "PRE",
  "text": "import numpy as npimport gymimport tensorflow as tffrom tensorflow.contrib.layers import flatten, conv2d, fully_connectedfrom collections import deque, Counterimport randomfrom datetime import datetime",
  "translation": "将numpy导入为np导入健身房导入tensorflow导入为tf from tensorflow.contrib.layers导入flatten，conv2d，full_connectedfrom集合导入deque，Counterimport randomfrom datetime导入datetime"
}, {
  "tag": "P",
  "text": "Next, we define a preprocessing function to crop the images from our gym environment and convert them into one-dimensional tensors. We’ve seen this before in our Pong automation implementation.",
  "translation": "接下来，我们定义一个预处理功能，以从健身房环境中裁剪图像并将其转换为一维张量。 我们在Pong自动化实施中已经看到了这一点。"
}, {
  "tag": "PRE",
  "text": "def preprocess_observation(obs):  # Crop and resize the image  img = obs[1:176:2, ::2]  # Convert the image to greyscale  img = img.mean(axis=2)  # Improve image contrast  img[img==color] = 0  # Next we normalize the image from -1 to +1  img = (img — 128) / 128–1  return img.reshape(88,80,1)",
  "translation": "def preprocess_observation（obs）：＃裁剪并调整图像大小img = obs [1：176：2，:: 2]＃将图像转换为灰度img = img.mean（axis = 2）＃改善图像对比度img [img = = [color] = 0＃接下来，我们将图像从-1标准化为+1 img =（img — 128）/ 128–1返回img.reshape（88,80,1）"
}, {
  "tag": "P",
  "text": "Next, let’s initialize the gym environment, and inspect a few screens of gameplay, and also understand the 9 actions available within the gamespace. Naturally, this information is not available to our agent.",
  "translation": "接下来，让我们初始化健身环境，检查几个游戏画面，并了解游戏空间中可用的9种动作。 当然，我们的代理商无法获得此信息。"
}, {
  "tag": "PRE",
  "text": "env = gym.make(“MsPacman-v0”)n_outputs = env.action_space.nprint(n_outputs)print(env.env.get_action_meanings())observation = env.reset()import tensorflow as tfimport matplotlib.pyplot as pltfor i in range(22):  if i > 20:    plt.imshow(observation)    plt.show()observation, _, _, _ = env.step(1)",
  "translation": "env = Gym.make（“ MsPacman-v0”）n_outputs = env.action_space.nprint（n_outputs）print（env.env.get_action_意义（（））observation = env.reset（）将tensorflow作为tfimport matplotlib.pyplot作为pltfor i in 范围（22）：如果i> 20：plt.imshow（observation）plt.show（）observation，_，_，_ = env.step（1）"
}, {
  "tag": "P",
  "text": "You should observe the following:",
  "translation": "您应注意以下几点："
}, {
  "tag": "FIGURE",
  "image": "https://miro.medium.com/max/30/1*lH0KxZh2D7yVww5piOdOew.png?q=20",
  "type": "image",
  "file": "1*lH0KxZh2D7yVww5piOdOew.png"
}, {
  "tag": "P",
  "text": "We can take this chance to compare our original and preprocessed input images:",
  "translation": "我们可以借此机会比较原始和预处理的输入图像："
}, {
  "tag": "FIGURE",
  "image": "https://miro.medium.com/max/30/1*TeUoR8Iou_6Z4XXLjB-uYw.png?q=20",
  "type": "image",
  "file": "1*TeUoR8Iou_6Z4XXLjB-uYw.png"
}, {
  "tag": "P",
  "text": "Next, let’s define our model, a deep Q-network. This is essentially a three layer convolutional network that takes preprocessed input images, flattens and feeds them to a fully-connected layer, and outputs the probabilities of taking each action in the game space. As previously mentioned, there’s no activation layer here, as the presence of one would result in a binary output distribution.",
  "translation": "接下来，让我们定义我们的模型，即深度Q网络。 本质上，这是一个三层卷积网络，它获取预处理的输入图像，展平并将其馈送到完全连接的层，并输出在游戏空间中采取每个动作的概率。 如前所述，这里没有激活层，因为一个激活层会导致二进制输出分布。"
}, {
  "tag": "PRE",
  "text": "def q_network(X, name_scope):# Initialize layers  initializer = tf.compat.v1.keras.initializers.VarianceScaling(scale=2.0)  with tf.compat.v1.variable_scope(name_scope) as scope:    # initialize the convolutional layers    layer_1 = conv2d(X, num_outputs=32, kernel_size=(8,8), stride=4, padding=’SAME’, weights_initializer=initializer)    tf.compat.v1.summary.histogram(‘layer_1’,layer_1)    layer_2 = conv2d(layer_1, num_outputs=64, kernel_size=(4,4),    stride=2, padding=’SAME’, weights_initializer=initializer)    tf.compat.v1.summary.histogram(‘layer_2’,layer_2)    layer_3 = conv2d(layer_2, num_outputs=64, kernel_size=(3,3), stride=1, padding=’SAME’, weights_initializer=initializer)    tf.compat.v1.summary.histogram(‘layer_3’,layer_3)    flat = flatten(layer_3)    fc = fully_connected(flat, num_outputs=128, weights_initializer=initializer)    tf.compat.v1.summary.histogram(‘fc’,fc)    #Add final output layer    output = fully_connected(fc, num_outputs=n_outputs, activation_fn=None, weights_initializer=initializer)    tf.compat.v1.summary.histogram(‘output’,output)    vars = {v.name[len(scope.name):]: v for v in tf.compat.v1.get_collection(key=tf.compat.v1.GraphKeys.TRAINABLE_VARIABLES, scope=scope.name)}    #Return both variables and outputs togetherreturn vars, output",
  "translation": "def q_network（X，name_scope）：＃使用tf.compat.v1.variable_scope（name_scope）作为范围初始化层初始化程序= tf.compat.v1.keras.initializers.VarianceScaling（scale = 2.0）：＃初始化卷积层layer_1 = conv2d（X，num_outputs = 32，kernel_size =（8,8），stride = 4，padding ='SAME'，weights_initializer = initializer）tf.compat.v1.summary.histogram（'layer_1'，layer_1）layer_2 = conv2d（ layer_1，num_outputs = 64，kernel_size =（4,4），stride = 2，padding ='SAME'，weights_initializer = initializer）tf.compat.v1.summary.histogram（'layer_2'，layer_2）layer_3 = conv2d（layer_2， num_outputs = 64，kernel_size =（3,3），stride = 1，padding ='SAME'，weights_initializer = initializer）tf.compat.v1.summary.histogram（'layer_3'，layer_3）flat = flatten（layer_3）fc = fully_connected（flat，num_outputs = 128，weights_initializer = initializer）tf.compat.v1.summary.histogram（'fc'，fc）＃添加最终输出层输出= fully_connected（fc，num_outputs = n_outp uts，activation_fn = None，weights_initializer = initializer）tf.compat.v1.summary.histogram（'output'，output）vars = {v.name [len（scope.name）：]：v for tf.compat中的v。 v1.get_collection（key = tf.compat.v1.GraphKeys.TRAINABLE_VARIABLES，scope = scope.name）}＃一起返回变量和输出，返回变量，输出"
}, {
  "tag": "P",
  "text": "Let’s also take this chance to define our hyperparameters for our model and training process",
  "translation": "让我们也借此机会为模型和训练过程定义我们的超参数"
}, {
  "tag": "PRE",
  "text": "num_episodes = 800batch_size = 48input_shape = (None, 88, 80, 1)#Recall shape is img.reshape(88,80,1)learning_rate = 0.001X_shape = (None, 88, 80, 1)discount_factor = 0.97global_step = 0copy_steps = 100steps_train = 4start_steps = 2000",
  "translation": "num_episodes = 800batch_size = 48input_shape =（None，88，80，1）＃调用形状为img.reshape（88,80,1）learning_rate = 0.001X_shape =（None，88，80，1）discount_factor = 0.97global_step = 0copy_steps = 100steps_train = 4start_steps = 2000"
}, {
  "tag": "P",
  "text": "Recall, that Q-learning requires us to select actions with the highest action values. To ensure that we still visit every single possible state-action combination, we’ll have our agent follow an epsilon-greedy policy, with an exploration rate of 5%. We’ll set a this exploration rate to decay with time, as we eventually assume all combinations have already been explored — any exploration after that point would simply result in the forced selection of sub-optimal actions.",
  "translation": "回想一下，Q学习要求我们选择具有最高动作值的动作。 为了确保我们仍然可以访问每个可能的国家行为组合，我们将让代理商遵循epsilon-greedy政策，探索率为5％。 由于最终假设所有组合都已被探索，因此我们将设定此探索速率随时间衰减-在此之后的任何探索只会导致强制选择次优动作。"
}, {
  "tag": "PRE",
  "text": "epsilon = 0.5eps_min = 0.05eps_max = 1.0eps_decay_steps = 500000#def epsilon_greedy(action, step):  p = np.random.random(1).squeeze() #1D entries returned using squeeze  epsilon = max(eps_min, eps_max — (eps_max-eps_min) * step/eps_decay_steps) #Decaying policy with more steps  if np.random.rand() < epsilon:    return np.random.randint(n_outputs)  else:    return action",
  "translation": "epsilon = 0.5eps_min = 0.05eps_max = 1.0eps_decay_steps = 500000＃def epsilon_greedy（action，step）：p = np.random.random（1）.squeeze（）＃1D使用压缩epsilon = max（eps_min，eps_max-返回的条目 eps_max-eps_min）* step / eps_decay_steps）＃如果np.random.rand（）<epsilon：返回np.random.randint（n_outputs），则衰减策略具有更多步骤，否则：返回操作"
}, {
  "tag": "P",
  "text": "Recall from the equations above, that the update function for Q-learning requires the following:",
  "translation": "回想一下上面的公式，Q学习的更新功能需要满足以下条件："
}, {
  "tag": "UL",
  "texts": ["The current state s", "The current action a", "The reward following the current action r", "The next state s’", "The next action a’"],
  "translations": ["当前状态", "当前动作", "当前动作后的奖励", "下一个州", "下一个动作是"]
}, {
  "tag": "P",
  "text": "To supply these parameters in meaningful quantities, we need to evaluate our current policy following a set of parameters and store all of the variables in a buffer, from which we’ll draw data in minibatches during training.. This is unlike in our previous implementation in Pong, where we used an incremental approach. Let’s go ahead and create our buffer and a simple sampling function:",
  "translation": "为了以有意义的数量提供这些参数，我们需要根据一组参数评估当前策略，并将所有变量存储在缓冲区中，在训练期间，我们将从这些缓冲区中提取数据。 在Pong，我们使用了增量方法。 让我们继续创建缓冲区和一个简单的采样函数："
}, {
  "tag": "PRE",
  "text": "buffer_len = 20000#Buffer is made from a deque — double ended queueexp_buffer = deque(maxlen=buffer_len)def sample_memories(batch_size):  perm_batch = np.random.permutation(len(exp_buffer))[:batch_size]  mem = np.array(exp_buffer)[perm_batch]  return mem[:,0], mem[:,1], mem[:,2], mem[:,3], mem[:,4]",
  "translation": "buffer_len = 20000＃缓冲区由双端队列制成-双端队列exp_buffer =双端队列（maxlen = buffer_len）def sample_memories（batch_size）：perm_batch = np.random.permutation（len（exp_buffer））[：batch_size] mem = np.array（ exp_buffer）[perm_batch]返回mem [：，0]，mem [：，1]，mem [：，2]，mem [：，3]，mem [：，4]"
}, {
  "tag": "P",
  "text": "Next, let’s copy the weight parameters of our original network into a target network. This dual-network approach allows us to generate data during the training process using an existing policy while still optimizing our parameters for the next policy iteration.",
  "translation": "接下来，让我们将原始网络的权重参数复制到目标网络中。 这种双网络方法使我们可以在训练过程中使用现有策略生成数据，同时仍为下一次策略迭代优化参数。"
}, {
  "tag": "PRE",
  "text": "# we build our Q network, which takes the input X and generates Q values for all the actions in the statemainQ, mainQ_outputs = q_network(X, ‘mainQ’)# similarly we build our target Q network, for policy evaluationtargetQ, targetQ_outputs = q_network(X, ‘targetQ’)copy_op = [tf.compat.v1.assign(main_name, targetQ[var_name]) for var_name, main_name in mainQ.items()]copy_target_to_main = tf.group(*copy_op)",
  "translation": "＃我们建立我们的Q网络，它接受输入X并为stateQ中的所有动作生成Q值mainQ，mainQ_outputs = q_network（X，'mainQ'）＃类似地，我们建立目标Q网络，以进行策略评估targetQ，targetQ_outputs = q_network （X，'targetQ'）copy_op = [tf.compat.v1.assign（main_name，targetQ [var_name]）表示mainQ.items（）中的var_name，main_name] copy_target_to_main = tf.group（* copy_op）"
}, {
  "tag": "P",
  "text": "Finally, we’ll also define our loss. This is simply the squared difference of our target action (with the highest action value) and our predicted action. We’ll use an ADAM optimizer to minimize our loss during training.",
  "translation": "最后，我们还要定义损失。 这只是我们的目标操作（具有最高操作值）和我们的预测操作的平方差。 我们将使用ADAM优化器来最大程度地减少训练过程中的损失。"
}, {
  "tag": "PRE",
  "text": "# define a placeholder for our output i.e actiony = tf.compat.v1.placeholder(tf.float32, shape=(None,1))# now we calculate the loss which is the difference between actual value and predicted valueloss = tf.reduce_mean(input_tensor=tf.square(y — Q_action))# we use adam optimizer for minimizing the lossoptimizer = tf.compat.v1.train.AdamOptimizer(learning_rate)training_op = optimizer.minimize(loss)init = tf.compat.v1.global_variables_initializer()loss_summary = tf.compat.v1.summary.scalar(‘LOSS’, loss)merge_summary = tf.compat.v1.summary.merge_all()file_writer = tf.compat.v1.summary.FileWriter(logdir, tf.compat.v1.get_default_graph())",
  "translation": "＃为输出定义一个占位符，即actiony = tf.compat.v1.placeholder（tf.float32，shape =（None，1））＃现在，我们计算损失，即实际值与预测值之差= tf.reduce_mean （input_tensor = tf.square（y — Q_action））＃我们使用adam优化器来最小化lossoptimizer = tf.compat.v1.train.AdamOptimizer（learning_rate）training_op = Optimizer.minimize（loss）init = tf.compat.v1。 global_variables_initializer（）loss_summary = tf.compat.v1.summary.scalar（'LOSS'，loss）merge_summary = tf.compat.v1.summary.merge_all（）file_writer = tf.compat.v1.summary.FileWriter（logdir，tf。 compat.v1.get_default_graph（））"
}, {
  "tag": "P",
  "text": "With all of our code defined, let’s run our network and go over the training process. We’ve defined most of this in the initial summary, but let’s recall for posterity.",
  "translation": "定义好所有代码后，我们就可以运行我们的网络并完成培训过程。 我们在初始摘要中已定义了大部分内容，但让我们回顾一下后代。"
}, {
  "tag": "UL",
  "texts": ["For each epoch, we feed an input image into our network to generate a probability distribution of the available actions, before using an epsilon-greedy policy to select the next action", "We then input this into the gym environment, and obtain information on the next state and accompanying rewards, and store this into our buffer.", "After our buffer is large enough, we sample the next states into our network in order to obtain the next action. We also calculate the next reward by discounting the current one", "We generate our target y-values through the Q-learning update function, and train our network.", "By minimizing the training loss, we update the network weight parameters to output improved state-action values for the next policy."],
  "translations": ["对于每个时期，在使用epsilon-greedy策略选择下一个动作之前，我们将输入图像馈入网络以生成可用动作的概率分布。", "然后，我们将其输入到体育馆环境中，并获取有关下一个状态和伴随奖励的信息，并将其存储在我们的缓冲区中。", "在缓冲区足够大之后，我们将下一个状态采样到我们的网络中以获得下一个动作。 我们还通过折现当前奖励来计算下一个奖励", "我们通过Q学习更新功能生成目标y值，并训练我们的网络。", "通过最大程度地减少训练损失，我们更新了网络权重参数，以便为下一个策略输出改进的状态操作值。"]
}, {
  "tag": "PRE",
  "text": "with tf.compat.v1.Session() as sess:  init.run()  # for each episode  history = []  for i in range(num_episodes):    done = False    obs = env.reset()    epoch = 0    episodic_reward = 0    actions_counter = Counter()    episodic_loss = []    # while the state is not the terminal state    while not done:    # get the preprocessed game screen   obs = preprocess_observation(obs)    # feed the game screen and get the Q values for each action,     actions = mainQ_outputs.eval(feed_dict={X:[obs],     in_training_mode:False})    # get the action    action = np.argmax(actions, axis=-1)    actions_counter[str(action)] += 1    # select the action using epsilon greedy policy    action = epsilon_greedy(action, global_step)    # now perform the action and move to the next state, next_obs, receive reward    next_obs, reward, done, _ = env.step(action)    # Store this transition as an experience in the replay buffer! Quite important    exp_buffer.append([obs, action, preprocess_observation(next_obs), reward, done])    # After certain steps we move on to generating y-values for Q network with samples from the experience replay buffer    if global_step % steps_train == 0 and global_step > start_steps:        o_obs, o_act, o_next_obs, o_rew, o_done = sample_memories(batch_size)        # states        o_obs = [x for x in o_obs]        # next states        o_next_obs = [x for x in o_next_obs]        # next actions        next_act = mainQ_outputs.eval(feed_dict={X:o_next_obs, in_training_mode:False})        #discounted reward for action: these are our Y-values        y_batch = o_rew + discount_factor * np.max(next_act, axis=-1) * (1-o_done)        # merge all summaries and write to the file        mrg_summary = merge_summary.eval(feed_dict={X:o_obs, y:np.expand_dims(y_batch, axis=-1), X_action:o_act, in_training_mode:False})        file_writer.add_summary(mrg_summary, global_step)        # To calculate the loss, we run the previously defined functions mentioned while feeding inputs        train_loss, _ = sess.run([loss, training_op], feed_dict={X:o_obs, y:np.expand_dims(y_batch, axis=-1), X_action:o_act, in_training_mode:True})        episodic_loss.append(train_loss)  # after some interval we copy our main Q network weights to target Q network  if (global_step+1) % copy_steps == 0 and global_step > start_steps:    copy_target_to_main.run()  obs = next_obs  epoch += 1  global_step += 1  episodic_reward += reward  history.append(episodic_reward)print(‘Epochs per episode:’, epoch, ‘Episode Reward:’, episodic_reward,”Episode number:”, len(history))",
  "translation": "使用tf.compat.v1.Session（）作为会话：init.run（）＃每个情节的历史= []范围内的i（num_episodes）：完成=错误obs = env.reset（）时代= 0 episodic_reward = 0 actions_counter = Counter（）episodic_loss = []＃当状态不是终端状态但未完成时：＃获取预处理的游戏屏幕obs = preprocess_observation（obs）＃进入游戏屏幕并获取每个动作的Q值，actions = mainQ_outputs.eval（feed_dict = {X：[obs]，in_training_mode：False}）＃获取动作action = np.argmax（动作，轴= -1）actions_counter [str（action）] + = 1＃使用epsilon greedy policy action = epsilon_greedy（action，global_step）＃现在执行该动作并移至下一个状态next_obs，获得奖励next_obs，奖励完成，_ = env.step（action）＃将这种过渡存储为经验重播缓冲区！非常重要的exp_buffer.append（[obs，action，preprocess_observation（next_obs），奖励，完成]）＃在某些步骤之后，如果global_step％steps_train == 0，我们将使用经验重播缓冲区中的样本为Q网络生成y值和global_step> start_steps：o_obs，o_act，o_next_obs，o_rew，o_done = sample_memories（batch_size）＃状态o_obs = [x表示x in o_obs]＃下一个状态o_next_obs = [x表示o_next_obs中的x]＃下一个动作next_act = mainQ。 （feed_dict = {X：o_next_obs，in_training_mode：False}）＃行动折扣：这些是我们的Y值y_batch = o_rew + Discount_factor * np.max（next_act，axis = -1）*（1-o_done）＃合并所有摘要并写入文件mrg_summary = merge_summary.eval（feed_dict = {X：o_obs，y：np.expand_dims（y_batch，axis = -1），X_action：o_act，in_training_mode：False}）file_writer.add_summary（mrg_summary，global_step ）＃计算t亏了，我们在输入输入train_loss，_ = sess.run（[loss，training_op]，feed_dict = {X：o_obs，y：np.expand_dims（y_batch，axis = -1），X_action： o_act，in_training_mode：True}）episodic_loss.append（train_loss）＃经过一段时间后，如果（global_step + 1）％copy_steps == 0并且global_step> start_steps：copy_target_to_main.run（）obs，我们将主要Q网络权重复制到目标Q网络= next_obs纪元+ = 1 global_step + = 1情节奖励+ =奖励历史记录。append（episodic_reward）print（'epochs per episode：'，epoch，'Episode Reward：'，​​episodic_reward，“ Episode number：”，len（history））"
}, {
  "tag": "P",
  "text": "Once training is complete, we can plot the reward distribution against incremental episodes. The first 550 episodes (roughly 2 hours) looks something like this:",
  "translation": "训练完成后，我们可以针对增量情节绘制奖励分布。 前550集（大约2小时）如下所示："
}, {
  "tag": "FIGURE",
  "image": "https://miro.medium.com/max/30/1*JCZ0uv_qgNJ7J676xcauhw.png?q=20",
  "type": "image",
  "file": "1*JCZ0uv_qgNJ7J676xcauhw.png"
}, {
  "tag": "P",
  "text": "After an additional 800 episodes, this converges into the following:",
  "translation": "再过800集后，这会收敛到以下内容："
}, {
  "tag": "FIGURE",
  "image": "https://miro.medium.com/max/30/1*SeZhhGjQpntVaaeqG9tXzA.png?q=20",
  "type": "image",
  "file": "1*SeZhhGjQpntVaaeqG9tXzA.png"
}, {
  "tag": "P",
  "text": "To evaluate our results within the confinement of the Colaboratory environment, we can record an entire episode and display it within a virtual display using a wrapped based on the IPython library:",
  "translation": "为了在协作环境的限制内评估结果，我们可以记录整个情节并使用基于IPython库的包装在虚拟显示器中显示它："
}, {
  "tag": "PRE",
  "text": "“””Utility functions to enable video recording of gym environment and displaying it. To enable video, just do “env = wrap_env(env)””“”def show_video():  mp4list = glob.glob(‘video/*.mp4’)  if len(mp4list) > 0:    mp4 = mp4list[0]    video = io.open(mp4, ‘r+b’).read()    encoded = base64.b64encode(video)    ipythondisplay.display(HTML(data=’’’<video alt=”test” autoplay          loop controls style=”height: 400px;”>          <source src=”data:video/mp4;base64,{0}” type=”video/mp4\" />          </video>’’’.format(encoded.decode(‘ascii’))))  else:    print(“Could not find video”)    def wrap_env(env):  env = Monitor(env, ‘./video’, force=True)  return env",
  "translation": "“”“实用程序功能可启用并记录健身房环境的视频。 要启用视频，只需执行“ env = wrap_env（env）”““” def show_video（）：如果len（mp4list）> 0，则mp4list = glob.glob（'video / *。mp4'）：mp4 = mp4list [0] video = io.open（mp4，'r + b'）。read（）编码= base64.b64encode（video）ipythondisplay.display（HTML（data ='''<video alt =“ test”自动播放循环控件style =” 高度：400像素；”> <source src =” data：video / mp4; base64，{0}” type =” video / mp4“ /> </ video>'''。format（encoded.decode（'ascii'） ）））else：print（“找不到视频”）def wrap_env（env）：env = Monitor（env，'./video'，force = True）return env"
}, {
  "tag": "P",
  "text": "We then run a new session of our environment using our model, and record it.",
  "translation": "然后，我们使用模型运行环境的新会话并进行记录。"
}, {
  "tag": "PRE",
  "text": "#Evaluate model on openAi GYMobservation = env.reset()new_observation = observationprev_input = Nonedone = Falsewith tf.compat.v1.Session() as sess:  init.run()  while True:    if True:    #set input to network to be difference image    obs = preprocess_observation(observation)    # feed the game screen and get the Q values for each action    actions = mainQ_outputs.eval(feed_dict={X:[obs], in_training_mode:False})    # get the action    action = np.argmax(actions, axis=-1)    actions_counter[str(action)] += 1    # select the action using epsilon greedy policy    action = epsilon_greedy(action, global_step)    env.render()    observation = new_observation    # now perform the action and move to the next state, next_obs, receive reward    new_observation, reward, done, _ = env.step(action)    if done:      #observation = env.reset()      break  env.close()  show_video()",
  "translation": "＃在openAi上评估模型GYMobservation = env.reset（）new_observation =观测值prev_input = Nonedone = False，将tf.compat.v1.Session（）设置为sess：init.run（），而True：如果为True：＃将网络输入设置为差异 image obs = preprocess_observation（observation）＃进入游戏屏幕并获取每个动作的Q值= mainQ_outputs.eval（feed_dict = {X：[obs]，in_training_mode：False}）＃获取动作action = np.argmax（ 动作，轴= -1）actions_counter [str（action）] + = 1＃使用epsilon贪婪策略选择动作action = epsilon_greedy（action，global_step）env.render（）观察= new_observation＃现在执行该动作并移至 下一个状态，next_obs，获得奖励new_observation，奖励，完成，_ = env.step（action），如果完成：#observation = env.reset（）break env.close（）show_video（）"
}, {
  "tag": "P",
  "text": "You should observe something a few rounds of the game! Here’s a couple of episodes we recorded.",
  "translation": "您应该观察游戏的一些回合！ 这是我们录制的几集。"
}, {
  "tag": "FIGURE",
  "type": "code"
}, {
  "tag": "FIGURE",
  "type": "code"
}, {
  "tag": "P",
  "text": "Not bad for a model trained in a few hours, scoring well above 400. In particular, it seems our agent performs quite well when directly chased by a ghost, but is still poor at anticipating incoming ones, probably as it hasn’t had enough experience observing their movements yet.",
  "translation": "对于经过数小时训练的模型来说，还不错，得分超过400。特别是，当我们的探员直接被幽灵追赶时，它的表现似乎不错，但在预测来袭者方面仍然表现欠佳，可能是因为它还不够用 体验观察他们的动作呢。"
}, {
  "tag": "P",
  "text": "That wraps up this introduction to Q-learning. In our next article, we’ll move on from the world of Atari to tackling one of the most well known FPS games in the world. Stay tuned!",
  "translation": "到此结束了对Q学习的介绍。 在我们的下一篇文章中，我们将从Atari的世界转向解决世界上最著名的FPS游戏之一。 敬请关注！"
}, {
  "tag": "P",
  "text": "We hope you enjoyed this article, and hope you check out the many other articles on GradientCrescent, covering applied and theoretical aspects of AI. To stay up to date with the latest updates on GradientCrescent, please consider following the publication and following our Github repository.",
  "translation": "我们希望您喜欢本文，也希望您阅读有关GradientCrescent的其他许多文章，涵盖AI的应用和理论方面。 要了解GradientCrescent的最新更新，请考虑遵循出版物并遵循我们的Github存储库。"
}, {
  "tag": "P",
  "text": "References",
  "translation": "参考文献"
}, {
  "tag": "P",
  "text": "Sutton et. al, Reinforcement Learning",
  "translation": "萨顿等 al，强化学习"
}, {
  "tag": "P",
  "text": "White et. al, Fundamentals of Reinforcement Learning, University of Alberta",
  "translation": "白等。 al，艾伯塔大学强化学习基础"
}, {
  "tag": "P",
  "text": "Silva et. al, Reinforcement Learning, UCL",
  "translation": "席尔瓦（Silva）等。 al，强化学习，UCL"
}, {
  "tag": "P",
  "text": "Ravichandiran et. al, Hands-On Reinforcement Learning with Python",
  "translation": "Ravichandiran等。 al，使用Python进行动手强化学习"
}, {
  "tag": "PRE",
  "text": "(本文翻译自Adrian Yijie Xu的文章《Automating Pac-man with Deep Q-learning: An Implementation in Tensorflow.》，参考：https://towardsdatascience.com/automating-pac-man-with-deep-q-learning-an-implementation-in-tensorflow-ca08e9891d9c)",
  "translation": "（本文翻译自：Adrian Yiyi Xu的文章《通过深度Q学习实现吃豆人自动化：Tensorflow的实现。》，参考：https：//towardsdatascience.com/automating-pac-man-with-deep-q-learning 张量流中的实现ca08e9891d9c）"
}]