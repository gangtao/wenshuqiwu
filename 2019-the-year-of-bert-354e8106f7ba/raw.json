[{
  "tag": "H1",
  "text": "2019: The Year of BERT",
  "translation": "2019：BERT年"
}, {
  "tag": "P",
  "text": "As we wrap up 2019, it’s interesting to reflect on the major recent trends in the field of machine learning for language. 2019 has been a landmark year for NLP, with new records across a variety of important tasks, from reading comprehension to sentiment analysis. The key research trend that stands out is the rise of transfer learning in NLP, which refers to using massive pre-trained models and fine-tuning them to your specific language-related task. Transfer learning allows you to reuse knowledge from previously built models, which can give you a boost in performance and generalisation, while demanding much less labelled training data.",
  "translation": "在2019年总结之际，有趣的是，我们反思了语言机器学习领域的近期主要趋势。 对于NLP来说，2019年是具有里程碑意义的一年，其新记录涵盖了从阅读理解到情感分析等一系列重要任务。 突出的主要研究趋势是NLP中迁移学习的兴起，这是指使用大规模的预训练模型并将其微调为与您特定的语言相关任务。 转移学习使您可以重用以前构建的模型中的知识，从而可以提高性能和泛化能力，同时需要更少的标签训练数据。"
}, {
  "tag": "P",
  "text": "The idea of pre-training models followed by task-specific fine-tuning is in itself not new — computer vision practitioners regularly use models pre-trained on large datasets like ImageNet, and in NLP we have been doing “shallow” transfer learning for years by reusing word embeddings.",
  "translation": "预训练模型然后进行针对特定任务的微调的想法本身并不是什么新鲜事物-计算机视觉从业人员经常使用在像ImageNet这样的大型数据集上进行预训练的模型，并且在NLP中，我们多年来一直在进行“浅”的转移学习 通过重复使用单词嵌入。"
}, {
  "tag": "P",
  "text": "But in 2019, with models like BERT, we saw a major shift towards deeper knowledge transfer by transferring entire models to new tasks — essentially using large pre-trained language models as reusable language comprehension feature extractors.",
  "translation": "但是在2019年，对于像BERT这样的模型，我们看到了向更深层知识转移的重大转变，即将整个模型转移到新任务上-本质上是使用大型的预训练语言模型作为可重用的语言理解特征提取器。"
}, {
  "tag": "P",
  "text": "This was talked about as “NLP’s ImageNet moment” last year, and in 2019, research continued to build on this trend. BERT was remarkable for making transfer learning in NLP easy, and in the process producing state-of-the-art results for 11 sentence-level and word-level NLP tasks with minimal adaptation. This is exciting from the practical point of view, but BERT and related models are perhaps even more interesting because they are advancing our fundamental understanding of how we should represent language to computers, and which representations best allow our models to solve challenging language problems.",
  "translation": "去年，这被称为“ NLP的ImageNet时刻”，而在2019年，研究继续以此趋势为基础。 BERT在简化NLP中的迁移学习以及在此过程中以最小的适应性生成11个句子级和单词级NLP任务的最新结果的过程中而著称。 从实用的角度来看，这是令人兴奋的，但是BERT和相关模型可能更有趣，因为它们正在促进我们对如何将语言表示为计算机，以及哪种表示最能使我们的模型解决具有挑战性的语言问题的基本理解。"
}, {
  "tag": "P",
  "text": "The emerging paradigm is: why constantly learn language syntax and semantics from scratch for every new NLP task when you can reuse BERT’s solid grasp of language?",
  "translation": "新兴的范式是：当您可以重用BERT对语言的扎实理解时，为什么要为每个新的NLP任务从头开始不断学习语言语法和语义？"
}, {
  "tag": "P",
  "text": "This core concept, together with an easy fine-tuning procedure and open source code, means that BERT has spread rapidly — initially released in late 2018, BERT has become enormously popular in 2019. I did not realise just how popular until I attempted to compile a list of BERT-related papers published this past year. I gathered 169 BERT-related papers, and manually annotated these into a few different categories of research (e.g. building domain-specific versions of BERT, understanding BERT’s inner mechanics, building multilingual BERTs, etc.).",
  "translation": "这个核心概念，再加上简单的微调过程和开源代码，意味着BERT迅速普及-BERT最初于2018年末发布，而BERT在2019年变得非常流行。直到我尝试编译时，我才意识到它有多么流行 去年发布的与BERT相关的论文清单。 我收集了169篇与BERT相关的论文，并将其手动注释为几种不同的研究类别（例如，构建BERT的特定领域版本，了解BERT的内部机制，构建多语言的BERT等）。"
}, {
  "tag": "P",
  "text": "Here is a plot of all of these papers at once:",
  "translation": "这是一次所有这些论文的图："
}, {
  "tag": "FIGURE",
  "image": "https://miro.medium.com/max/30/1*yDYl4eX-0MD7STzEgRnrDA.png?q=20",
  "caption": "A collection of BERT-related papers published between November 2018 and December 2019. The y axis is the log of the citation count (as measured by Google Scholar), but with a floor of 0. The majority of these papers were found by searching for BERT in the title of arXiv papers.",
  "type": "image",
  "file": "1*yDYl4eX-0MD7STzEgRnrDA.png"
}, {
  "tag": "P",
  "text": "This sort of information is often better interactive, so here is a GIF. You can also check out the Jupyter notebook to play with the plot yourself, and the raw data is here.",
  "translation": "这种信息通常具有更好的交互性，因此这里是GIF。 您也可以签出Jupyter笔记本以自己玩图，原始数据在这里。"
}, {
  "tag": "FIGURE",
  "image": "https://miro.medium.com/freeze/max/30/1*kw7Xr-XThSLnvlnw22AsmQ.gif?q=20",
  "caption": "Mousing over a mass of BERT papers.",
  "type": "image",
  "file": "1*kw7Xr-XThSLnvlnw22AsmQ.gif"
}, {
  "tag": "P",
  "text": "Now that’s a lot of BERT papers. Some notes on this plot:",
  "translation": "现在有很多BERT论文。 关于此情节的一些注意事项："
}, {
  "tag": "UL",
  "texts": ["It’s interesting to observe the (fairly short) lag between the publication of the original paper in November 2018, and the flood of papers starting around January 2019.", "The initial wave of BERT papers tended to focus on immediate extensions and applications of the core BERT model (red, purple and orange dots), like adapting BERT for recommendations systems, sentiment analysis, text summarisation, and document retrieval.", "Then, starting in April, a collection of papers probing the internal mechanisms of BERT were published (in green), like understanding how BERT models hierarchical linguistic phenomena and analysing the redundancy between attention heads. Of particular interest is the paper “BERT Rediscovers the Classical NLP Pipeline”, in which the authors find that BERT’s internal computations mirror the traditional NLP workflow (first do parts-of-speech tagging, then dependency parsing, then entity tagging, etc.).", "Around September, a collection of papers focused on compressing the model size of BERT were released (cyan), like the DistilBERT, ALBERT and TinyBERT papers. For instance, DistilBERT model from HuggingFace is a compressed version of BERT with half the number of parameters (from 110 million down to 66 million) but 95% of the performance on important NLP tasks (see the GLUE benchmarks). The original BERT models are not exactly lightweight, and this is a problem in places where computational resources are not plentiful (like mobile phones).", "This list of BERT papers is very likely to be incomplete. I wouldn’t be surprised if the true number of very BERT-relevant papers is double my figure. As a rough upper bound, the number of papers that cite the original BERT paper is currently over 3100.", "In case you’re curious about the names of some of these models — essentially NLP researchers are getting carried away with Sesame Street characters. We can blame the ELMo paper for getting this whole thing started, which made later models like BERT and ERNIE inevitable. I am eagerly awaiting a BIGBIRD model — and let’s call the compressed version SMALLBIRD?"],
  "translations": ["有趣的是，观察到2018年11月发表原始论文与2019年1月左右论文泛滥之间的时间间隔（相当短）。", "BERT论文的最初浪潮往往集中在核心BERT模型（红色，紫色和橙色点）的立即扩展和应用上，例如使BERT适用于推荐系统，情感分析，文本摘要和文档检索。", "然后，从4月开始，发布了一系列探讨BERT内部机制的论文（以绿色显示），例如了解BERT如何建模分层的语言现象并分析注意头之间的冗余。 特别令人感兴趣的是论文“ BERT重新发现经典的NLP管道”，作者发现BERT的内部计算与传统的NLP工作流程类似（首先进行词性标记，然后进行依存关系分析，然后进行实体标记等）。 。", "大约在9月左右，发布了一系列有关压缩BERT模型尺寸的论文（青色），例如DistilBERT，ALBERT和TinyBERT论文。 例如，来自HuggingFace的DistilBERT模型是BERT的压缩版本，其参数数量只有一半（从1.1亿降至6600万），但在重要的NLP任务上的性能却达到了95％（请参阅GLUE基准测试）。 原始的BERT模型并不十分轻巧，这在计算资源不足的地方（如移动电话）是一个问题。", "这份BERT论文清单很可能是不完整的。 如果与BERT相关的论文的真实数量是我的两倍，我不会感到惊讶。 作为粗略的上限，引用原始BERT论文的论文数量目前超过3100。", "如果您对其中一些型号的名称感到好奇，那么本质上讲，NLP研究人员对芝麻街的角色感到迷惑。 我们可以归咎于ELMo论文使这一切开始，这使得后来的诸如BERT和ERNIE的模型不可避免。 我热切地在等待一个BIGBIRD模型-我们将其称为压缩版本SMALLBIRD吗？"]
}, {
  "tag": "H1",
  "text": "A few lessons from the BERT literature",
  "translation": "BERT文献中的一些教训"
}, {
  "tag": "P",
  "text": "Going through this literature, a few general concepts emerged:",
  "translation": "通过这些文献，出现了一些一般概念："
}, {
  "tag": "UL",
  "texts": ["The value of open-sourcing machine learning models. The authors made the BERT model and relevant code freely available, and provided an easy, reusable fine-tuning procedure. This type of openness is vital for accelerating research, and I doubt the model would have been nearly as popular if the authors were less forthright.", "The importance of taking hyperparameter tuning seriously. The RoBERTa paper made a splash by presenting a more principled approach to training BERT, with optimised design choices (like changing a training task) and more extensive hyperparameter tuning. This updated training regime, together with just training the model for longer on more data, again pushed performance to record-breaking levels across various NLP benchmarks.", "Thoughts on model size. The original BERT authors were intrigued to observe that simply increasing the size of the model can dramatically improve performance, even on a very small dataset. Perhaps this means that in some sense you “need” hundreds of millions of parameters to represent human language. Several other papers in 2019 found that simply scaling up the size of NLP models leads to improvements (famously, OpenAI’s GPT-2 model). And there’s new tricks to train ridiculously huge NLP models, like NVIDIA’s 8 billion parameter behemoth. But there has also been evidence of diminishing returns as model size increases, similar to the wall computer vision researchers hit when adding more convolutional layers. The successes of papers on model compression and parameter efficiency suggest that significantly more performance can be squeezed from models of a given size."],
  "translations": ["开源机器学习模型的价值。 作者免费提供了BERT模型和相关代码，并提供了一个简单，可重用的微调过程。 这种开放性对于加快研究速度至关重要，而且我怀疑如果作者不那么直率的话，该模型是否会如此受欢迎。", "认真对待超参数调整的重要性。 RoBERTa的论文通过提出一种更原则性的BERT训练方法，优化的设计选择（如更改训练任务）和更广泛的超参数调整，引起了轰动。 这种更新的培训制度，再加上只是对更多数据进行更长时间培训的模型，再次将各种NLP基准的性能提升到了创纪录的水平。", "关于模型大小的想法。 最初的BERT作者很感兴趣，发现即使只是在非常小的数据集上，简单地增加模型的大小也可以极大地提高性能。 也许这意味着从某种意义上说，您“需要”数亿个参数来表示人类语言。 2019年的其他几篇论文发现，仅扩大NLP模型的规模即可带来改进（著名的是OpenAI的GPT-2模型）。 还有一些新的技巧可用来训练可笑的巨大NLP模型，例如NVIDIA的80亿参数巨兽。 但是，也有证据表明，随着模型尺寸的增加，收益会递减，这与研究人员在添加更多卷积层时遇到的墙式计算机视觉类似。 关于模型压缩和参数效率的论文的成功表明，可以从给定大小的模型中获得更多的性能。"]
}, {
  "tag": "FIGURE",
  "image": "https://miro.medium.com/max/30/1*to-B8ydshebn9QsNLkBgeQ.png?q=20",
  "caption": "Our NLP models are getting bigger and bigger. From DistilBERT paper.",
  "type": "image",
  "file": "1*to-B8ydshebn9QsNLkBgeQ.png"
}, {
  "tag": "H1",
  "text": "What is BERT, exactly?",
  "translation": "BERT是什么？"
}, {
  "tag": "P",
  "text": "Let’s take a few steps back and discuss what BERT actually is. BERT (Bidirectional Encoder Representations from Transformers) is a pre-trained language model (LM) built by Google researchers. Language models are trained on tasks that incentivise the model to learn a deep understanding of language; a common training task for LMs is next word prediction (“the cat sat on the ___”).",
  "translation": "让我们退后一步，讨论一下BERT到底是什么。 BERT（来自变压器的双向编码器表示形式）是Google研究人员建立的经过预先训练的语言模型（LM）。 对语言模型进行了培训，这些任务可以激励模型以学习对语言的深刻理解。 LM的常见训练任务是预测下一个单词（“猫坐在___上”）。"
}, {
  "tag": "P",
  "text": "BERT is based on a relatively new neural network architecture — Transformers, which use a mechanism called self-attention to capture the relationships between words. There are no convolutions (as in CNNs) or recurrence operations (as in RNNs) in Transformers (“Attention is All You Need”). There have been some excellent tutorials published explaining Transformers and self-attention, so I will not go into much detail here. But briefly:",
  "translation": "BERT基于一种相对较新的神经网络体系结构-变压器，该变压器使用一种称为自我注意的机制来捕获单词之间的关系。 变形金刚中没有卷积（如CNN中的卷积）或递归操作（如RNN中的卷积）（“注意就是您所需要的”）。 已经出版了一些出色的教程，介绍了《变形金刚》和《自我注意》，因此在这里我将不做详细介绍。 但简要地说："
}, {
  "tag": "UL",
  "texts": ["Self-attention is a sequence-to-sequence operation that updates input token embeddings by baking in each word’s context into its representation. This allows it to model the relationship between all input words simultaneously — contrast this to RNNs, in which input tokens are read in and processed sequentially. Self-attention computes the similarity between word vectors using dot products, and the resultant attention weights are often visualised as an attention weight matrix.", "Attention weights capture the strength of relationships between words, and we allow the model to learn different types of relationships by using multiple attention heads. Each attention head often captures a particular type of relationship between words (with some redundancy). Some of these relationships are intuitively interpretable (like subject-object relationships, or keeping track of neighbouring words), and some are rather inscrutable. You can think of attention heads as being analogous to filters in convolutional networks, where each filter extracts a specific type of feature from the data — whichever feature will best help the rest of the neural network make better predictions.", "This self-attention mechanism is the core operation in Transformers, but just to put it into context: Transformers were originally developed for machine translation, and they have an encoder-decoder structure. The building block of Transformer encoders and decoders is a Transformer block, which is itself generally composed of a self-attention layer, some amount of normalisation, and a standard feed-forward layer. Each block performs this sequence of operations on input vectors and passes the output to the next block. In Transformers, depth refers to the number of Transformer blocks."],
  "translations": ["自我注意是一种逐个序列的操作，它通过将每个单词的上下文烘焙成其表示形式来更新输入令牌嵌入。 这样，它就可以同时对所有输入词之间的关系进行建模-与RNN形成对比，在RNN中，输入令牌被顺序读取和处理。 自我注意会使用点积来计算单词向量之间的相似度，并且通常将结果注意权重可视化为注意权重矩阵。", "注意权重捕获单词之间关系的强度，我们允许模型通过使用多个注意头来学习不同类型的关系。 每个关注头通常会捕获单词之间的一种特定类型的关系（有些冗余）。 这些关系中的一些是直觉上可以解释的（例如主客关系或跟踪相邻单词），而有些则是难以理解的。 您可以认为注意力与卷积网络中的过滤器类似，其中每个过滤器从数据中提取特定类型的特征-哪种特征最能帮助神经网络的其余部分做出更好的预测。", "这种自我关注机制是Transformers中的核心操作，但仅是将其引入上下文：Transformers最初是为机器翻译而开发的，它们具有编码器-解码器结构。 Transformer编码器和解码器的构建块是一个Transformer块，它本身通常由一个自我注意层，一定数量的规范化层和一个标准前馈层组成。 每个块对输入矢量执行此操作序列，并将输出传递到下一个块。 在“变形金刚”中，深度是指变形块的数量。"]
}, {
  "tag": "P",
  "text": "Using this Transformer setup, the BERT model was trained on 2 unsupervised language tasks. The most important thing about BERT training is that it only requires unlabelled data — any text corpus can be used, you do not need any special labelled dataset. The BERT paper used Wikipedia and a book corpus for training the model. As with “normal” language models, data comes cheap, and this is a huge advantage.",
  "translation": "使用此Transformer设置，对BERT模型进行了2种无监督语言任务的训练。 BERT训练最重要的是，它只需要未标记的数据-可以使用任何文本语料库，而无需任何特殊的标记数据集。 BERT论文使用Wikipedia和书籍语料库来训练模型。 与“常规”语言模型一样，数据价格便宜，这是一个巨大的优势。"
}, {
  "tag": "H2",
  "text": "How is BERT trained?",
  "translation": "BERT如何训练？"
}, {
  "tag": "P",
  "text": "But what tasks is BERT trained on that encourage it to learn such a good, generally useful understanding of language? Future work tweaked the learning strategy, but the original paper used two tasks:",
  "translation": "但是BERT接受了哪些培训，以鼓励其学习这种对语言的良好，普遍有用的理解？ 未来的工作调整了学习策略，但是原始论文使用了两个任务："
}, {
  "tag": "OL",
  "texts": ["The Masked Language Model (MLM) task. This task encouraged the model to learn good representations at the word-level and at the sentence-level (since a sentence is the totality of word representations). Briefly, 15% of the words in a sentence are randomly chosen and hidden (or “masked”) with a <MASK> token. The model’s job is to predict the identity of these hidden words, making use of both the words before and after the <MASK> — hence, we are trying to reconstruct the text from a corrupted input, and both left and right contexts are used to make predictions. This allows us to build up representations of words that take all of the context into account. BERT learns its bidirectional representations simultaneously, in contrast to methods like ELMo (an RNN-based language model used for generating context-aware word embeddings), where left-to-right and right-to-left representations are independently learned by two language models and then concatenated. We could say that ELMo is a ‘shallow bidirectional’ model whereas BERT is a ‘deep bidirectional’ model.", "The Next Sentence Prediction (NSP) task. If our model is going to be used as the basis for language understanding, it would be good for it to have some knowledge of inter-sentence coherence. To encourage the model to learn about the relationship between sentences, we add the Next Sentence Prediction task, in which the model has to predict if a pair of sentences are related, namely if one is likely to come after another. Positive training pairs are real adjacent sentences in the corpus; negative training pairs are randomly sampled from the corpus. It’s not a perfect system, since randomly sampled pairs could actually be related, but it is good enough."],
  "translations": ["屏蔽语言模型（MLM）任务。此任务鼓励模型在单词级别和句子级别学习良好的表示形式（因为句子是单词表示形式的总和）。简短地说，一个句子中15％的单词是随机选择的，并用<MASK>标记隐藏（或“掩盖”）。该模型的工作是通过使用<MASK>之前和之后的单词来预测这些隐藏单词的身份-因此，我们正尝试从损坏的输入中重建文本，并且左右上下文都用于作出预测。这使我们能够建立将所有上下文都考虑在内的单词表示形式。与ELMo（基于RNN的语言模型用于生成上下文感知词嵌入）的方法相反，BERT同时学习其双向表示，其中两种语言模型分别学习从左到右和从右到左的表示形式然后串联起来。我们可以说ELMo是“浅双向”模型，而BERT是“深双向”模型。", "下一个句子预测（NSP）任务。 如果我们的模型将用作语言理解的基础，那么对一些句子间的连贯性知识会有所帮助。 为了鼓励模型学习句子之间的关系，我们添加了“下一句预测”任务，其中该模型必须预测一对句子是否相关，即一个句子是否可能接连出现。 正训练对是语料库中真正相邻的句子； 阴性训练对是从语料库中随机抽取的。 这不是一个完美的系统，因为实际上可以对随机采样的对进行关联，但这已经足够了。"]
}, {
  "tag": "P",
  "text": "The model must learn to do both tasks simultaneously, since the actual training loss is the combination of the losses from the two tasks (namely it’s the sum of the mean MLM and NSP likelihoods).",
  "translation": "该模型必须学会同时执行两项任务，因为实际的训练损失是两项任务损失的总和（即，MLM和NSP均值之和）。"
}, {
  "tag": "P",
  "text": "In case you spotted a bit of a problem with the masking approach: you’re right. Since a random 15% of words in a segment are masked, you’re likely to have multiple <MASK> occurrences present. This is fine, but BERT treats these masked tokens independently of one another, which is a bit limiting since they could easily be dependent. This is one of the points addressed by the XLNet paper, which some people consider the successor to BERT.",
  "translation": "以防万一您发现掩盖方法有问题：您是对的。 由于段中随机有15％的单词被屏蔽，因此您可能会出现多次<MASK>。 很好，但是BERT将这些屏蔽的令牌彼此独立对待，这是一个限制，因为它们很容易相互依赖。 这是XLNet文件解决的要点之一，有些人认为这是BERT的后继者。"
}, {
  "tag": "H2",
  "text": "Fine-tuning BERT",
  "translation": "微调BERT"
}, {
  "tag": "P",
  "text": "Once the base BERT model is trained, you would usually fine-tune in 2 steps: first by continuing the “unsupervised” training on your unlabelled data, and then by learning your actual task by adding an additional layer and training on your new objective (using not very many labelled examples). This approach has roots in this 2015 LSTM LM paper by Dai & Le from Google.",
  "translation": "训练完基本的BERT模型后，通常会分两步进行调整：首先，对未标记的数据继续进行“无监督”训练，然后通过添加额外的层并针对新目标进行训练来学习实际任务（ 使用的标签示例并不多）。 这种方法源于Google的Dai＆Le在2015年的LSTM LM论文中。"
}, {
  "tag": "P",
  "text": "BERT fine-tuning will actually update all of the parameters of your model, not just the ones in the new task-specific layer, so this approach differs from techniques which completely freeze transferred layer parameters.",
  "translation": "BERT微调实际上将更新模型的所有参数，而不仅是新的特定于任务的层中的参数，因此此方法与完全冻结传输的层参数的技术不同。"
}, {
  "tag": "P",
  "text": "In practice, with BERT transfer learning, often only the the trained Encoder stack is usually reused — you chop off the decoder half of the model and just use the Encoder Transformer blocks as a feature extractor. So, we don’t care about the predictions the Decoder part of the Transformer would have made on whatever language task it was originally trained on, we are just interested in the way that the model has learned to internally represent the textual input.",
  "translation": "在实践中，通过BERT传输学习，通常通常仅重用经过训练的编码器堆栈-您将模型的解码器切掉一半，而仅将编码器变压器块用作特征提取器。 因此，我们不在乎Transformer的Decoder部分对最初训练的任何语言任务所做的预测，我们只对模型学会了内部表示文本输入的方式感兴趣。"
}, {
  "tag": "P",
  "text": "BERT fine-tuning might take minutes or hours, depending on your task, data size and TPU/GPU resources. In case you’re interested in trying out BERT fine-tuning ASAP, you can use this ready-made code on Google Colab, which provides access to a free TPU.",
  "translation": "BERT微调可能需要几分钟或几小时，具体取决于您的任务，数据大小和TPU / GPU资源。 如果您有兴趣尝试尽快对BERT进行微调，则可以在Google Colab上使用此现成的代码，该代码提供对免费TPU的访问。"
}, {
  "tag": "H2",
  "text": "What was around before BERT?",
  "translation": "BERT之前发生了什么？"
}, {
  "tag": "P",
  "text": "The original BERT paper is well-written and I recommend checking it out; the following bullet points summarise the paper’s account of the previous major approaches in the language model pretraining and fine-tuning space:",
  "translation": "原始BERT纸写得很好，我建议您检查一下； 以下要点总结了本文对语言模型预训练和微调空间中以前主要方法的描述："
}, {
  "tag": "UL",
  "texts": ["Unsupervised feature-based approaches (like ELMo), which use pre-trained representations as input features but use task-specific architectures (i.e. they change the model’s structure for each new task). All of your favourite word embeddings (word2vec to GLoVe to FastText), sentence embeddings, and paragraph embeddings fall into this category. ELMo also provides word embeddings but in a context-sensitive manner — — the embedding/representation for a token is the concatenation of the left-to-right and a right-to-left language model hidden state vectors.", "Unsupervised fine-tuning approaches (like OpenAI’s GPT model), which fine-tune all pre-trained parameters for a supervised downstream task and only minimally change the model structure by introducing a few task-specific parameters. Pre-training is on unlabelled text and the learning tasks are usually either left-to-right language modelling, or text compression (as with autoencoders, which compress text into a vector representation, and reconstruct the text from the vector). However, the ability of these approaches to model context has been limiting because they have generally been unidirectional, left-to-right models — for a given word, there was no ability to incorporate all later words into its representation.", "Transfer learning from supervised data. There has also been some work on transferring the knowledge learned from supervised tasks that have lots of training data, e.g. using the weights from a machine translation model to initialise parameters for a different language problem."],
  "translations": ["无监督的基于特征的方法（例如ELMo），其使用预先训练的表示作为输入特征，但使用特定于任务的架构（即，它们针对每个新任务更改模型的结构）。 您所有喜欢的词嵌入（从word2vec到GLoVe到FastText），句子嵌入和段落嵌入均属于此类别。 ELMo还提供了词嵌入功能，但是以上下文相关的方式提供-令牌的嵌入/表示是从左到右和从右到左语言模型隐藏状态向量的串联。", "无监督的微调方法（例如OpenAI的GPT模型），可以微调所有用于下游任务的预训练参数，并通过引入一些特定于任务的参数来最小化模型结构。 预训练是对未标记的文本进行的，学习任务通常是从左到右的语言建模或文本压缩（与自动编码器一样，自动编码器将文本压缩为矢量表示形式，并从矢量中重构文本）。 但是，这些方法建模上下文的能力一直受到限制，因为它们通常是单向的，从左到右的模型-对于给定的单词，没有能力将所有以后的单词合并到其表示中。", "从监督数据转移学习。 还有一些工作要转移从有大量训练数据的监督任务中学到的知识，例如 使用机器翻译模型中的权重来初始化针对其他语言问题的参数。"]
}, {
  "tag": "H2",
  "text": "Problems, or Things to Think About",
  "translation": "问题或需要考虑的事情"
}, {
  "tag": "UL",
  "texts": ["There has been some work from computer vision to suggest that pre-training and fine-tuning mostly helps to speed up model converfence."],
  "translations": ["来自计算机视觉的一些工作表明，预训练和微调主要有助于加快模型收敛。"]
}, {
  "tag": "H2",
  "text": "Conclusion",
  "translation": "结论"
}, {
  "tag": "P",
  "text": "I hope this post has provided a reasonable recap of the BERT phenomenon, and shown just how popular and powerful this model has become in NLP. The field is progressing rapidly, and the results we are now seeing from state-of-the-art models just would not have been believable even just 5 years ago (e.g. superhuman performance in question answering). The two key trends in recent NLP progress are the rise of transfer learning and Transformers, and I’m keen to see how these will develop in 2020.",
  "translation": "我希望这篇文章对BERT现象进行了合理的回顾，并展示了该模型在NLP中的受欢迎程度和强大程度。 这个领域正在迅速发展，我们现在从最先进的模型中看到的结果甚至在5年前都令人难以置信（例如，超人类的表现正在回答）。 NLP最近的两个主要趋势是迁移学习和Transformers的兴起，我渴望看到它们在2020年将如何发展。"
}, {
  "tag": "FIGURE",
  "image": "https://miro.medium.com/max/26/1*Xy1mRDxF693gG3geLpJMKA.jpeg?q=20",
  "caption": "Happy holidays!",
  "type": "image",
  "file": "1*Xy1mRDxF693gG3geLpJMKA.jpeg"
}, {
  "tag": "P",
  "text": "—",
  "translation": "-"
}, {
  "tag": "P",
  "text": "Welocalize is an industry leader in NLP and translation technology. To chat with someone on our team about your NLP project, email Dave at david.clark@welocalize.com.",
  "translation": "Welocalize是NLP和翻译技术的行业领导者。 要与我们团队中的某人就您的NLP项目聊天，请发送电子邮件至david，网址为david.clark@welocalize.com。"
}, {
  "tag": "PRE",
  "text": "(本文翻译自Natasha Latysheva的文章《2019: The Year of BERT》，参考：https://towardsdatascience.com/2019-the-year-of-bert-354e8106f7ba)",
  "translation": "（本文翻译自Natasha Latysheva的文章《 2019：BERT年》，参考：https：//towardsdatascience.com/2019-the-year-of-bert-354e8106f7ba）"
}]