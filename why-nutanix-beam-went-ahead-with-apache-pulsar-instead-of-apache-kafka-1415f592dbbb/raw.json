[{
  "tag": "H1",
  "text": "Why Nutanix Beam went ahead with Apache Pulsar instead of Apache Kafka?",
  "translation": "为什么Nutanix Beam继续使用Apache Pulsar而不是Apache Kafka？"
}, {
  "tag": "FIGURE",
  "image": "https://miro.medium.com/max/60/1*mJiSX1ECCfmHD4_JisoqmA.jpeg?q=20",
  "type": "image",
  "file": "1*mJiSX1ECCfmHD4_JisoqmA.jpeg"
}, {
  "tag": "P",
  "text": "In Nutanix Beam(Saas Product) we crunch a lot of data to find insights about cloud spend as well as cloud security. Nutanix Beam is built on our microservices & service mesh architecture using Consul, Nomad, Vault, Envoy and Docker for synchronous RPC style requests. Although we are not going to discuss our microservice architecture here (that’s for a different day and a different post :) ), we will focus on the key technology that supports this architecture — asynchronous communication between teams and microservices.",
  "translation": "在Nutanix Beam（Saas产品）中，我们处理大量数据以查找有关云支出以及云安全性的见解。 Nutanix Beam建立在我们的微服务和服务网格架构上，使用Consul，Nomad，Vault，Envoy和Docker进行同步RPC样式请求。 尽管我们不会在这里讨论我们的微服务架构（这是在不同的日子和不同的帖子：），但是我们将重点关注支持该架构的关键技术-团队和微服务之间的异步通信。"
}, {
  "tag": "P",
  "text": "We use Disque & Conductor for batch processing. Both of these systems are queue-based systems. We wanted to add pub/sub capabilities into our platform, which led us to look for a streaming platform in which we can reliably store events and replay them when required. Because some of our team members have previous experience with Kafka and have operated Kafka at a moderate scale, it would have been easy for us to choose Kafka. But before deploying a technology we strongly believe it is important to look at the current landscape and validate our technology choices so that we don’t give in to bias toward familiar technology that may not be the best for a project. Also, it was important to make this decision carefully because the streaming platform would become a critical component of this SaaS product.",
  "translation": "我们使用Disque＆Conductor进行批处理。 这两个系统都是基于队列的系统。 我们希望将发布/订阅功能添加到我们的平台中，这导致我们寻找一种流媒体平台，在该平台中我们可以可靠地存储事件并在需要时重播它们。 由于我们的某些团队成员以前有过Kafka的经验，并且以中等规模经营Kafka，因此选择Kafka对我们来说很容易。 但是，在部署技术之前，我们强烈认为，重要的是要了解当前的形势并验证我们的技术选择，以免我们屈服于可能对项目不利的熟悉技术。 同样，务必谨慎地做出此决定，因为流媒体平台将成为此SaaS产品的关键组件。"
}, {
  "tag": "P",
  "text": "We first learned of Apache Pulsar when we saw it mentioned in a Github issue and decided to start evaluating it for our streaming platform use cases. Considering that Pulsar is under the Apache umbrella and has already graduated from Apache’s incubation process to become a top-level project, we could be confident that it was an established technology. Apache pulsar is running in production for many year in yahoo and adopted many companies mentioned here. We collected our list of use cases that require a streaming platform, and we started a deep dive analysis of the architecture of Apache Pulsar and Pulsar’s coordination, persistence, reliability, high availability, fault tolerance and client ecosystem.",
  "translation": "当我们在Github问题中提到Apache Pulsar时，我们首先了解到它，并决定开始针对我们的流媒体平台用例对其进行评估。 考虑到Pulsar处于Apache的保护伞下，并且已经从Apache的孵化过程中毕业，成为一个顶级项目，我们可以确信它是一项成熟的技术。 Apache pulsar在Yahoo中已投入生产多年，并采用了此处提到的许多公司。 我们收集了需要流平台的用例列表，并且开始深入分析Apache Pulsar的体系结构以及Pulsar的协调性，持久性，可靠性，高可用性，容错性和客户端生态系统。"
}, {
  "tag": "P",
  "text": "Introduction to Apache Pulsar",
  "translation": "Apache Pulsar简介"
}, {
  "tag": "P",
  "text": "Apache Pulsar is an open-source distributed pub-sub messaging system originally created at Yahoo and now part of the Apache Software Foundation.",
  "translation": "Apache Pulsar是最初在Yahoo创建的开源分布式pub-sub消息传递系统，现已成为Apache Software Foundation的一部分。"
}, {
  "tag": "P",
  "text": "Apache Pulsar uses an architecture that decouples message processing, serving, and storage. The storage layer embeds Apache BookKeeper for data persistence, replication and high availability. Message serving is handled by a set of stateless brokers that make up the serving layer, and message processing is handled by Pulsar Functions in their own layer of this architecture. Each layer of this architecture can be scaled independently — we can independently scale BookKeeper (storage layer) to increase storage capacity and throughput, Pulsar brokers (serving layer) to increase message serving throughput, and function workers (processing layer) to handle more data processing.",
  "translation": "Apache Pulsar使用一种将消息处理，服务和存储分离的架构。 存储层嵌入了Apache BookKeeper，以实现数据持久性，复制和高可用性。 消息服务由构成服务层的一组无状态代理处理，消息处理由该体系结构自己的层中的Pulsar Functions处理。 此体系结构的每一层都可以独立扩展-我们可以独立扩展BookKeeper（存储层）以提高存储容量和吞吐量，Pulsar代理（服务层）以提高消息服务吞吐量，而功能工作者（处理层）则可以处理更多数据处理 。"
}, {
  "tag": "P",
  "text": "To learn more about Apache Pulsar, see the Apache Pulsar website and documentation.",
  "translation": "要了解有关Apache Pulsar的更多信息，请参阅Apache Pulsar网站和文档。"
}, {
  "tag": "H1",
  "text": "Benefits for Development:",
  "translation": "发展的好处："
}, {
  "tag": "P",
  "text": "One interesting thing we found when we collected use cases is that we frequently want to consume messages using both message queues and pub-sub models. However, we can’t use Kafka as a queuing system because the maximum number of workers you can have is limited by the number of partitions, and because there is no way to acknowledge messages at the individual message level. You would need to manually commit offsets in Kafka by maintaining a record of individual message acknowledgments in your own data store, which adds a lot of extra overhead — too much overhead in my opinion.",
  "translation": "收集用例时发现的一件有趣的事情是，我们经常想同时使用消息队列和pub-sub模型来使用消息。 但是，我们不能将Kafka用作排队系统，因为您可以拥有的最大工作人员数量受到分区数量的限制，并且由于无法在单个消息级别上确认消息。 您将需要通过在自己的数据存储中维护单个消息确认的记录来手动在Kafka中提交偏移量，这会增加很多额外的开销—我认为这是太多的开销。"
}, {
  "tag": "P",
  "text": "With Pulsar you don’t need to worry about which model you are going to need for consuming data. In Pulsar you can consume and commit at the offset level as well as consume and acknowledge at the message level. For more details about how that works, the Streamlio team has written a nice blog here. Also worth noting is how Apache Pulsar handles message delivery in the case of a consumer crash: Pulsar will publish the message again to another consumer after a delay defined at the subscription level.",
  "translation": "使用Pulsar，您无需担心要使用哪种模型来使用数据。 在Pulsar中，您可以在偏移级别使用和提交，也可以在消息级别使用和确认。 有关其工作原理的更多详细信息，Streamlio团队在此处写了一个不错的博客。 同样值得注意的是，如果使用者崩溃，Apache Pulsar将如何处理消息传递：在订阅级别定义的延迟之后，Pulsar将再次将消息发布给另一个使用者。"
}, {
  "tag": "H1",
  "text": "Benefits for Operations:",
  "translation": "运营收益："
}, {
  "tag": "P",
  "text": "This is where things get interesting. Since we have operated Kafka at a moderate scale we already understood several of its limitations. We saw that Pulsar could address these limitations and better meet our needs.",
  "translation": "这就是事情变得有趣的地方。 由于我们以中等规模运营Kafka，我们已经了解了它的几个局限性。 我们看到Pulsar可以解决这些限制并更好地满足我们的需求。"
}, {
  "tag": "H1",
  "text": "Event Replay and Lagging Consumers:",
  "translation": "事件重播和落后的消费者："
}, {
  "tag": "P",
  "text": "In Apache Kafka, when consumers are lagging behind, producer throughput falls off a cliff because lagging consumers introduce random reads. Kafka’s architecture is designed in such a way that high throughput depends on accessing data sequentially in an append-only manner.",
  "translation": "在Apache Kafka中，当消费者落后时，由于落后的消费者引入了随机读取，因此生产者的吞吐量下降了。 Kafka的架构的设计方式是，高吞吐量取决于仅以追加方式顺序访问数据。"
}, {
  "tag": "P",
  "text": "Apache Pulsar solves this problem using the combination of a segment-based architecture and isolation between reads and writes. You can read more about it in this blog. In a nutshell, your hot reads are handled by the brokers’ in-memory cache and hot writes are handled by the journal. Event replay/lagging consumer reads access data from ledgers, which are on a separate disk from the journal.",
  "translation": "Apache Pulsar通过结合使用基于段的架构和读写之间的隔离来解决此问题。 您可以在此博客中了解更多信息。 简而言之，热读取由代理的内存中缓存处理，热写入由日志处理。 事件重播/滞后使用者从分类帐读取访问数据，分类帐与日志位于不同的磁盘上。"
}, {
  "tag": "P",
  "text": "For example, suppose there is a topic and we have 5 consumers, all of them reading happily without any lag. In this case, reads are served by broker cache. Periodically, a background process is moving the data from the BookKeeper journal to ledgers. Now imagine that suddenly our machine learning engineer decides to consume from the beginning of the topic to train/retrain his model because he tweaked his model’s sigma & mu :). Because the lagging consumption/event replay is provided from the ledger, which is sitting in a different disk, the event replay does not impact the performance of the consumers reading the most recent data in the topic.",
  "translation": "例如，假设有一个主题，我们有5个消费者，他们所有人阅读愉快，没有任何滞后。 在这种情况下，读取由代理缓存提供。 周期性地，后台进程将数据从BookKeeper日记帐移到分类帐。 现在想象一下，我们的机器学习工程师突然决定从本主题的开始就决定使用它来训练/重新训练他的模型，因为他调整了模型的sigma＆mu :)。 由于滞后的消费/事件重播是由位于不同磁盘中的分类帐提供的，因此事件重播不会影响消费者读取主题中最新数据的性能。"
}, {
  "tag": "H2",
  "text": "Data Retention without Exploding Our Cloud Bill",
  "translation": "保留数据而不会爆炸我们的云账单"
}, {
  "tag": "P",
  "text": "Apache Pulsar solves this with its storage tiering feature, which can offload older data into scalable storage systems like Amazon s3, Google Cloud Storage, Minio, etc. We can configure an offloading policy based on time and size. Once the data in a topic reaches the specified time or size threshold, older data segments are moved to object storage. The best part is that when a consumer requests data that has been offloaded to object storage, Pulsar transparently serves that data to the consumer from object storage. The consumer does not know whether the data came from disk or the object storage. In this way, your topic partition size is not limited to the size of storage on a single broker.",
  "translation": "Apache Pulsar通过其存储分层功能解决了此问题，该功能可以将较旧的数据卸载到可扩展的存储系统中，例如Amazon s3，Google Cloud Storage，Minio等。我们可以根据时间和大小配置卸载策略。 一旦主题中的数据达到指定的时间或大小阈值，较旧的数据段将移至对象存储。 最好的部分是，当使用者请求已被卸载到对象存储的数据时，Pulsar会从对象存储透明地将该数据提供给使用者。 使用者不知道数据是来自磁盘还是对象存储。 这样，您的主题分区大小不仅限于单个代理上的存储大小。"
}, {
  "tag": "H2",
  "text": "Online Scaling:",
  "translation": "在线扩展："
}, {
  "tag": "P",
  "text": "Adding nodes to an Apache Kafka cluster doesn’t necessarily solve performance problems related to overloaded nodes. Why not? New nodes added to a Kafka cluster will be used for new topics created after they are added, but won’t automatically take away some of the load from the overloaded nodes. To use them to spread the load, you need to do manual, costly rebalancing of data to migrate some of the topics from the old nodes to the new node.",
  "translation": "将节点添加到Apache Kafka群集不一定能解决与过载节点有关的性能问题。 为什么不？ 添加到Kafka群集中的新节点将用于添加后创建的新主题，但不会自动减轻过载节点的一些负担。 要使用它们来分散负载，您需要进行手动的，成本高昂的数据重新平衡，以将某些主题从旧节点迁移到新节点。"
}, {
  "tag": "P",
  "text": "In Pulsar, storage is handled by a storage layer provided by Apache BookKeeper. Pulsar uses a segment-based architecture where the messages in a topic partition are collected into segments, which are then persisted. As a result, there is no one-to-one mapping between topic partition and nodes as there is in Kafka. When a new storage node is added, some of the new segments are stored on the new node, reducing the load on the previously existing nodes immediately.",
  "translation": "在Pulsar中，存储由Apache BookKeeper提供的存储层处理。 Pulsar使用基于段的体系结构，其中主题分区中的消息被收集为段，然后将其持久化。 结果，在主题分区和节点之间不存在与Kafka中一样的一对一映射。 添加新的存储节点后，某些新段将存储在新节点上，从而立即减少了先前存在的节点上的负载。"
}, {
  "tag": "P",
  "text": "There is a nice diagram in this blog by Jack Vanlightly illustrating how this works.",
  "translation": "Jack Vanlightly在此博客中有一个很好的图表，说明了其工作原理。"
}, {
  "tag": "H2",
  "text": "Millions of Topics:",
  "translation": "数百万个主题："
}, {
  "tag": "P",
  "text": "Another important requirement for our use cases is the ability to support millions of topics. For example, suppose we want to replay data for one customer. As long as we create one topic per customer, it is easy to create a subscription on a specific customer’s topic and then consumes the data for that customer. Deleting customer data becomes very simple as deleting the topic.",
  "translation": "我们的用例的另一个重要要求是能够支持数百万个主题。 例如，假设我们要重播一位客户的数据。 只要我们为每个客户创建一个主题，就可以轻松地为特定客户的主题创建订阅，然后使用该客户的数据。 就像删除主题一样，删除客户数据变得非常简单。"
}, {
  "tag": "P",
  "text": "However, in Apache Kafka scaling a number of topics is a real architectural problem. Kafka creates multiple files (resources) per topic. Because of this, the number of topics we can create will be limited. However, if we were to put data from many or even all customers into a single topic, then when we want to replay data for a single customer we will be forced to replay the whole topic and discard all the messages except for those associated with the single customer we are interested in. Even if we were to use a partitioned topic with customer id as the partition key so that we can select just the partition where a single customer’s data is stored, there is a good amount of wasted resources needed on the broker and consumer side.",
  "translation": "但是，在Apache Kafka中，扩展多个主题是一个实际的体系结构问题。 Kafka为每个主题创建多个文件（资源）。 因此，我们可以创建的主题数量将受到限制。 但是，如果我们要将来自许多甚至所有客户的数据放在一个主题中，那么当我们要重放单个客户的数据时，我们将被迫重放整个主题并丢弃所有消息，但与消息关联的消息除外。 我们感兴趣的单个客户。即使我们要使用带有客户ID的分区主题作为分区键，以便我们仅选择存储单个客户数据的分区，也会浪费大量资源。 经纪人和消费者方面。"
}, {
  "tag": "P",
  "text": "Pulsar does not create files per topics. Since it uses Apache BookKeeper’s segment storage architecture, the number of segments created is not determined by the number of topics. We like this feature a lot because with it we can go ahead and create one topic per customer, a simpler and more efficient design.",
  "translation": "Pulsar不会按主题创建文件。 由于它使用Apache BookKeeper的细分存储架构，因此创建的细分数量不受主题数量决定。 我们非常喜欢此功能，因为有了它，我们可以继续为每个客户创建一个主题，这是一种更简单，更有效的设计。"
}, {
  "tag": "H2",
  "text": "Production Rollout",
  "translation": "生产部署"
}, {
  "tag": "P",
  "text": "Pulsar is running in production for the last 6 months and supports use cases such as Pub/Sub, Queuing and schema registry.",
  "translation": "Pulsar在生产中运行了最近6个月，并支持用例，例如发布/订阅，排队和架构注册表。"
}, {
  "tag": "H1",
  "text": "Conclusion",
  "translation": "结论"
}, {
  "tag": "P",
  "text": "Overall our take on Pulsar is that it greatly simplified the design of asynchronous communication in Nutanix Beam. It’s scalability and design allows us to make decisions about how to consume data on the consumer side based on the business use cases, rather than forcing us to make those decisions during the ingestion of data. That flexibility helps us to better support ever-changing business requirements.",
  "translation": "总的来说，我们对Pulsar的看法是，它极大地简化了Nutanix Beam中的异步通信设计。 凭借其可伸缩性和设计，我们可以根据业务用例来决定如何在消费者端使用数据，而不是强迫我们在数据提取期间做出这些决定。 这种灵活性可以帮助我们更好地支持不断变化的业务需求。"
}, {
  "tag": "FIGURE",
  "image": "https://miro.medium.com/max/60/1*gRA_hdrjFIrSDVMUPgDx8g.png?q=20",
  "caption": "Apache Pulsar is Next Generation Persisted Streaming Pub/Sub & Queuing Platform.",
  "type": "image",
  "file": "1*gRA_hdrjFIrSDVMUPgDx8g.png"
}, {
  "tag": "P",
  "text": "To understand more about how apache pulsar works you can read this awesome article written by Jack Vanightly. Also, streamlio team has nice blog posts that talk about each of the pulsar architectural details in depth. Apache Pulsar has an awesome community that is more than welcome in helping us to get things right and very prompt on merging our pull requests.",
  "translation": "要了解有关apache pulsar的工作方式的更多信息，请阅读Jack Vanightly撰写的这篇很棒的文章。 另外，streamlio团队有不错的博客文章，深入讨论了每个脉冲星建筑细节。 Apache Pulsar拥有一个很棒的社区，在帮助我们正确处理并合并合并请求请求时非常受欢迎。"
}, {
  "tag": "P",
  "text": "PS: This article is written keeping in mind that every technology has its place in this world. To highlight some of the architectural differences we have compared it with kafka. We have used kafka in the past and we are thankful to kafka & kafka community.",
  "translation": "PS：撰写本文时要记住，每种技术在这个世界上都有其地位。 为了突出某些架构差异，我们将其与kafka进行了比较。 我们过去曾经使用过kafka，也非常感谢kafka＆kafka社区。"
}, {
  "tag": "P",
  "text": "Let me know your thoughts in the comments section. You can follow me on twitter @SkyRocknRoll",
  "translation": "在评论部分让我知道您的想法。 您可以在Twitter上关注我@SkyRocknRoll"
}, {
  "tag": "PRE",
  "text": "(本文翻译自Yuvaraj Loganathan的文章《Why Nutanix Beam went ahead with Apache Pulsar instead of Apache Kafka?》，参考：https://medium.com/@yuvarajl/why-nutanix-beam-went-ahead-with-apache-pulsar-instead-of-apache-kafka-1415f592dbbb)",
  "translation": "（本文翻译自Yuvaraj Loganathan的文章，《 Nutanix Beam为什么选择Apache Pulsar而不是Apache Kafka？》，参考：https：//medium.com/@yuvarajl/why-nutanix-beam-went-ahead-with-apache 脉冲星代替Apache Kafka-1415f592dbbb）"
}]