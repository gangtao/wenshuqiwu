[{
  "tag": "H2",
  "text": "Learning Image to Image Translation with CycleGANs",
  "translation": "使用CycleGANs学习图像到图像的翻译"
}, {
  "tag": "H3",
  "text": "We recently worked with our partner Getty Images, a global stock photo agency, to explore image to image translation on…",
  "translation": "最近，我们与我们的合作伙伴Getty Images（一家全球性的图片服务公司）合作，在…上探索图像到图像的翻译。"
}, {
  "tag": "H2",
  "text": "Image-to-Image Translation in Tensorflow — Affine Layer",
  "translation": "Tensorflow中的图像到图像翻译-仿射层"
}, {
  "tag": "H3",
  "text": "make sure you have Tensorflow 0.12.1 installed first python -c “import tensorflow; print(tensorflow.__version__)” …",
  "translation": "确保首先安装了Tensorflow 0.12.1，然后执行python -c“ import tensorflow; print（tensorflow .__ version__）”…"
}, {
  "tag": "H1",
  "text": "CycleGANS and Pix2Pix",
  "translation": "CycleGANS和Pix2Pix"
}, {
  "tag": "P",
  "text": "Credits: Presenting abridged version of these blogs to explain the idea and concepts behind pix2pix and cycleGANs.",
  "translation": "鸣谢：介绍这些博客的简要版本，以解释pix2pix和cycleGAN背后的思想和概念。"
}, {
  "tag": "P",
  "text": "Christopher Hesse blog:",
  "translation": "Christopher Hesse博客："
}, {
  "tag": "H2",
  "text": "Image-to-Image Translation in Tensorflow — Affine Layer",
  "translation": "Tensorflow中的图像到图像翻译-仿射层"
}, {
  "tag": "H3",
  "text": "make sure you have Tensorflow 0.12.1 installed first python -c “import tensorflow; print(tensorflow.__version__)” …",
  "translation": "确保首先安装了Tensorflow 0.12.1，然后执行python -c“ import tensorflow; print（tensorflow .__ version__）”…"
}, {
  "tag": "P",
  "text": "Olga Liakhovich blog:",
  "translation": "Olga Liakhovich博客："
}, {
  "tag": "H2",
  "text": "Learning Image to Image Translation with CycleGANs",
  "translation": "使用CycleGANs学习图像到图像的翻译"
}, {
  "tag": "H3",
  "text": "We recently worked with our partner Getty Images, a global stock photo agency, to explore image to image translation on…",
  "translation": "最近，我们与我们的合作伙伴Getty Images（一家全球性的图片服务公司）合作，在…上探索图像到图像的翻译。"
}, {
  "tag": "H1",
  "text": "Pix2Pix:",
  "translation": "Pix2Pix："
}, {
  "tag": "P",
  "text": "paper: https://phillipi.github.io/pix2pix/",
  "translation": "论文：https://phillipi.github.io/pix2pix/"
}, {
  "tag": "P",
  "text": "pix2pix uses a conditional generative adversarial network (cGAN) to learn a mapping from an input image to an output image.",
  "translation": "pix2pix使用条件生成对抗网络（cGAN）来学习从输入图像到输出图像的映射。"
}, {
  "tag": "P",
  "text": "An example of a dataset would be that the input image is a black and white picture and the target image is the color version of the picture. The generator in this case is trying to learn how to colorize a black and white image. The discriminator is looking at the generator’s colorization attempts and trying to learn to tell the difference between the colorizations the generator provides and the true colorized target image provided in the dataset."
}, {
  "tag": "P",
  "text": "The structure of the generator is called an “encoder-decoder” and in pix2pix the encoder-decoder looks more or less like this:",
  "translation": "生成器的结构称为“编码器-解码器”，在pix2pix中，编码器/解码器看起来或多或少像这样："
}, {
  "tag": "FIGURE",
  "image": "https://miro.medium.com/max/30/1*grPpbT-8fwA4twYZkAHwmw.png?q=20",
  "type": "image",
  "file": "1*grPpbT-8fwA4twYZkAHwmw.png"
}, {
  "tag": "P",
  "text": "The volumes are there to give you a sense of the shape of the tensor dimensions next to them. The input in this example is a 256x256 image with 3 color channels (red, green, and blue, all equal for a black and white image), and the output is the same.",
  "translation": "那里的体积使您可以感觉到它们旁边的张量尺寸的形状。 在此示例中，输入是具有3个颜色通道（红色，绿色和蓝色，对于黑白图像均相等）的256x256图像，并且输出是相同的。"
}, {
  "tag": "P",
  "text": "The generator takes some input and tries to reduce it with a series of encoders (convolution + activation function) into a much smaller representation. The idea is that by compressing it this way we hopefully have a higher level representation of the data after the final encode layer. The decode layers do the opposite (deconvolution + activation function) and reverse the action of the encoder layers.",
  "translation": "生成器接受一些输入，并尝试通过一系列编码器（卷积+激活函数）将其减少为更小的表示形式。 我们的想法是，通过这种方式进行压缩，我们希望在最终编码层之后可以对数据进行更高级别的表示。 解码层执行相反的操作（解卷积+激活功能），并反转编码层的作用。"
}, {
  "tag": "P",
  "text": "In order to improve the performance of the image-to-image transform in the paper, the authors used a “U-Net” instead of an encoder-decoder. This is the same thing, but with “skip connections” directly connecting encoder layers to decoder layers:",
  "translation": "为了提高图像到图像变换的性能，作者使用“ U-Net”代替编码器-解码器。 这是同一回事，但是通过“跳过连接”将编码器层直接连接到解码器层："
}, {
  "tag": "FIGURE",
  "image": "https://miro.medium.com/max/30/1*kpWvVdQOmbMuX2ls-d78TA.png?q=20",
  "type": "image",
  "file": "1*kpWvVdQOmbMuX2ls-d78TA.png"
}, {
  "tag": "P",
  "text": "The skip connections give the network the option of bypassing the encoding/decoding part if it doesn’t have a use for it.",
  "translation": "跳过连接使网络可以选择不使用编码/解码部分。"
}, {
  "tag": "P",
  "text": "These diagrams are a slight simplification. For instance, the first and last layers of the network have no batch norm layer and a few layers in the middle have dropout units.",
  "translation": "这些图只是一个简单的简化。 例如，网络的第一层和最后一层没有批处理规范层，而中间的一些层则具有退出单元。"
}, {
  "tag": "P",
  "text": "The Discriminator",
  "translation": "鉴别者"
}, {
  "tag": "P",
  "text": "The Discriminator has the job of taking two images, an input image and an unknown image (which will be either a target or output image from the generator), and deciding if the second image was produced by the generator or not.",
  "translation": "鉴别器的工作是拍摄两个图像，一个输入图像和一个未知图像（这将是生成器的目标图像或输出图像），并确定第二个图像是否由生成器生成。"
}, {
  "tag": "FIGURE",
  "image": "https://miro.medium.com/max/30/1*-iPXj4C0sCK0UzW1aPzJZg.png?q=20",
  "type": "image",
  "file": "1*-iPXj4C0sCK0UzW1aPzJZg.png"
}, {
  "tag": "P",
  "text": "The structure looks a lot like the encoder section of the generator, but works a little differently. The output is a 30x30 image where each pixel value (0 to 1) represents how believable the corresponding section of the unknown image is. In the pix2pix implementation, each pixel from this 30x30 image corresponds to the believability of a 70x70 patch of the input image (the patches overlap a lot since the input images are 256x256). The architecture is called a “PatchGAN”.",
  "translation": "该结构看起来很像生成器的编码器部分，但工作方式略有不同。 输出为30x30图像，其中每个像素值（0到1）表示未知图像的相应部分的可信度。 在pix2pix实现中，来自此30x30图像的每个像素对应于输入图像的70x70色块的可信度（由于输入图像为256x256，所以色块重叠很多）。 该架构称为“ PatchGAN”。"
}, {
  "tag": "P",
  "text": "Training",
  "translation": "训练"
}, {
  "tag": "P",
  "text": "To train this network, there are two steps: training the discriminator and training the generator.",
  "translation": "要训练该网络，有两个步骤：训练鉴别器和训练生成器。"
}, {
  "tag": "P",
  "text": "To train the discriminator, first the generator generates an output image. The discriminator looks at the input/target pair and the input/output pair and produces its guess about how realistic they look. The weights of the discriminator are then adjusted based on the classification error of the input/output pair and the input/target pair.",
  "translation": "为了训练鉴别器，首先发生器产生输出图像。 鉴别器查看输入/目标对和输入/输出对，并就其看起来有多逼真做出猜测。 然后基于输入/输出对和输入/目标对的分类误差来调整鉴别器的权重。"
}, {
  "tag": "P",
  "text": "The generator’s weights are then adjusted based on the output of the discriminator as well as the difference between the output and target image.",
  "translation": "然后根据鉴别器的输出以及输出图像与目标图像之间的差异来调整生成器的权重。"
}, {
  "tag": "FIGURE",
  "image": "https://miro.medium.com/max/30/1*EUUrcoQ9nBGyNzIeqzCDJQ.png?q=20",
  "type": "image",
  "file": "1*EUUrcoQ9nBGyNzIeqzCDJQ.png"
}, {
  "tag": "FIGURE",
  "image": "https://miro.medium.com/max/30/1*vwCOla_RiuTD6pEEMitTgQ.png?q=20",
  "caption": "Discriminator and Generator training",
  "type": "image",
  "file": "1*vwCOla_RiuTD6pEEMitTgQ.png"
}, {
  "tag": "H1",
  "text": "CycleGANs",
  "translation": "循环GAN"
}, {
  "tag": "P",
  "text": "Original CycleGAN paper",
  "translation": "原始CycleGAN纸"
}, {
  "tag": "P",
  "text": "While PIX2PIX can produce truly magical results, the challenge is in training data. The two image spaces that you wanted to learn to translate between needed to be pre-formatted into a single X/Y image that held both tightly-correlated images. This could be time-consuming, infeasible, or even impossible based on what two image types you were trying to translate between (for instance, if you didn’t have one-to-one matches between the two image profiles). This is where the CycleGAN comes in.",
  "translation": "虽然PIX2PIX可以产生真正神奇的结果，但挑战在于训练数据。 您想要学习在其间转换的两个图像空间需要预先格式化为单个X / Y图像，以容纳两个紧密相关的图像。 根据您要转换的两种图像类型（例如，如果两个图像配置文件之间没有一对一的匹配），这可能是耗时的，不可行的，甚至是不可能的。 这就是CycleGAN的用武之地。"
}, {
  "tag": "P",
  "text": "The key idea behind CycleGANs is that they can build upon the power of the PIX2PIX architecture, but allow you to point the model at two discrete, unpaired collections of images. For example, one collection of images, Group X, would be full of sunny beach photos while Group Y would be a collection of overcast beach photos. The CycleGAN model can learn to translate the images between these two aesthetics without the need to merge tightly correlated matches together into a single X/Y training image.",
  "translation": "CycleGANs背后的关键思想是它们可以基于PIX2PIX体系结构的强大功能，但允许您将模型指向两个不成对的离散图像集合。 例如，一组图像X将充满阳光明媚的海滩照片，而组Y将是阴暗海滩照片的集合。 CycleGAN模型可以学习在这两种美学之间转换图像，而无需将紧密相关的匹配项合并到单个X / Y训练图像中。"
}, {
  "tag": "P",
  "text": "The way CycleGANs are able to learn such great translations without having explicit X/Y training images involves introducing the idea of a full translation cycle to determine how good the entire translation system is, thus improving both generators at the same time.",
  "translation": "CycleGAN无需显式的X / Y训练图像就可以学习如此出色的翻译的方法包括引入完整翻译周期的想法，以确定整个翻译系统的质量，从而同时改善两个生成器。"
}, {
  "tag": "FIGURE",
  "image": "https://miro.medium.com/max/30/0*D5yQU7v0NHXsb1Ep.jpg?q=20",
  "type": "image",
  "file": "0*D5yQU7v0NHXsb1Ep.jpg"
}, {
  "tag": "P",
  "text": "This approach is the clever power that CycleGANs brings to image-to-image translations and how it enables better translations among non-paired image styles.",
  "translation": "这种方法是CycleGAN带来的图像到图像转换的聪明力量，以及它如何在非配对图像样式之间实现更好的转换。"
}, {
  "tag": "P",
  "text": "The original CycleGANs paper, “Unpaired Image-to-Image Translation using Cycle-Consistent Adversarial Networks”, was published by Jun-Yan Zhu, et al.",
  "translation": "Zhujun-Yan Zhu等人发表了最初的CycleGANs论文“使用周期一致的对抗网络进行不成对的图像到图像的翻译”。"
}, {
  "tag": "H1",
  "text": "Loss functions",
  "translation": "损失函数"
}, {
  "tag": "P",
  "text": "The power of CycleGANs is in how they set up the loss function, and use the full cycle loss as an additional optimization target.",
  "translation": "CycleGAN的功能在于如何设置损耗函数，并将全周期损耗用作附加的优化目标。"
}, {
  "tag": "P",
  "text": "As a refresher: we’re dealing with 2 generators and 2 discriminators.",
  "translation": "复习一下：我们正在处理2个生成器和2个区分器。"
}, {
  "tag": "H2",
  "text": "Generator Loss",
  "translation": "发电机损耗"
}, {
  "tag": "P",
  "text": "Let’s start with the generator’s loss functions, which consist of 2 parts.",
  "translation": "让我们从生成器的损失函数开始，该函数由两部分组成。"
}, {
  "tag": "P",
  "text": "Part 1: The generator is successful if fake (generated) images are so good that discriminator can not distinguish those from real images. In other words, the discriminator’s output for fake images should be as close to 1 as possible. In TensorFlow terms, the generator would like to minimize:",
  "translation": "第1部分：如果伪造（生成的）图像非常好，以至于辨别器无法将其与真实图像区分开，则生成器成功。 换句话说，伪造图像的鉴别器输出应尽可能接近1。 用TensorFlow术语来说，生成器希望最小化："
}, {
  "tag": "PRE",
  "text": "g_loss_G_disc = tf.reduce_mean((discY_fake — tf.ones_like(discY_fake)) ** 2)g_loss_F_dicr = tf.reduce_mean((discX_fake — tf.ones_like(discX_fake)) ** 2)",
  "translation": "g_loss_G_disc = tf.reduce_mean（（discY_fake — tf.ones_like（discY_fake））** 2）g_loss_F_dicr = tf.reduce_mean（（discX_fake — tf.ones_like（discX_fake））** 2）"
}, {
  "tag": "P",
  "text": "Note: the “**” symbol above is the power operator in Python.",
  "translation": "注意：上面的“ **”符号是Python中的幂运算符。"
}, {
  "tag": "P",
  "text": "Part 2: We need to capture cyclic loss: as we go from one generator back to the original space of images using another generator, the difference between the original image (where we started the cycle) and the cyclic image should be minimized.",
  "translation": "第2部分：我们需要捕获循环损耗：当我们使用一个生成器从一个生成器返回图像的原始空间时，原始图像（开始循环的位置）与循环图像之间的差异应最小化。"
}, {
  "tag": "PRE",
  "text": "g_loss_G_cycle = tf.reduce_mean(tf.abs(real_X — genF_back)) + tf.reduce_mean(tf.abs(real_Y — genG_back))g_loss_F_cycle = tf.reduce_mean(tf.abs(real_X — genF_back)) + tf.reduce_mean(tf.abs(real_Y — genG_back))",
  "translation": "g_loss_G_cycle = tf.reduce_mean（tf.abs（real_X-genF_back））+ tf.reduce_mean（tf.abs（real_Y-genG_back））g_loss_F_cycle = tf.reduce_mean（tf.abs（real_X-meFantt.tf.back））+ .abs（real_Y — genG_back））"
}, {
  "tag": "P",
  "text": "Finally, the generator loss is the sum of these two terms:",
  "translation": "最后，发电机损耗是这两项的总和："
}, {
  "tag": "P",
  "text": "g_loss_G = g_loss_G_disc + g_loss_G_cycle",
  "translation": "g_loss_G = g_loss_G_disc + g_loss_G_cycle"
}, {
  "tag": "P",
  "text": "Because cyclic loss is so important we want to multiply its effect. We used an L1_lambda constant for this multiplier (in the paper the value 10 was used).",
  "translation": "由于周期性损失非常重要，因此我们想倍增其影响。 我们为此乘数使用了一个L1_lambda常数（在本文中使用了值10）。"
}, {
  "tag": "P",
  "text": "Now the grand finale of the generator loss looks like:",
  "translation": "现在，发电机损耗的总决赛看起来像："
}, {
  "tag": "P",
  "text": "g_loss_G = g_loss_G_disc + L1_lambda * g_loss_G_cycle",
  "translation": "g_loss_G = g_loss_G_disc + L1_lambda * g_loss_G_cycle"
}, {
  "tag": "P",
  "text": "g_loss_F = g_loss_F_disc + L1_lambda * g_loss_F_cycle",
  "translation": "g_loss_F = g_loss_F_disc + L1_lambda * g_loss_F_cycle"
}, {
  "tag": "P",
  "text": "Discriminator Loss",
  "translation": "鉴别损失"
}, {
  "tag": "P",
  "text": "The Discriminator has 2 decisions to make:",
  "translation": "鉴别器有2个决定要做出："
}, {
  "tag": "OL",
  "texts": ["Real images should be marked as real (recommendation should be as close to 1 as possible)", "The discriminator should be able to recognize generated images and thus predict 0 for fake images."],
  "translations": ["真实图片应标记为真实图片（建议尽可能接近1）", "鉴别器应该能够识别生成的图像，从而为假图像预测0。"]
}, {
  "tag": "PRE",
  "text": "DY_loss_real = tf.reduce_mean((DY — tf.ones_like(DY))** 2)DY_loss_fake = tf.reduce_mean((DY_fake_sample — tf.zeros_like(DY_fake_sample)) ** 2)DY_loss = (DY_loss_real + DY_loss_fake) / 2DX_loss_real = tf.reduce_mean((DX — tf.ones_like(DX)) ** 2)DX_loss_fake = tf.reduce_mean((DX_fake_sample — tf.zeros_like(DX_fake_sample)) ** 2)DX_loss = (DX_loss_real + DX_loss_fake) / 2",
  "translation": "DY_loss_real = tf.reduce_mean（（DY-tf.ones_like（DY））** 2）DY_loss_fake = tf.reduce_mean（（DY_fake_sample — tf.zeros_like（DY_fake_sample））** 2）DY_loss =（DY_loss_real + DY_loss_fake / tf.reduce_mean（（DX — tf.ones_like（DX））** 2）DX_loss_fake = tf.reduce_mean（（DX_fake_sample — tf.zeros_like（DX_fake_sample））** 2）DX_loss =（DX_loss_real + DX_loss_fake）/ 2"
}, {
  "tag": "PRE",
  "text": "(本文翻译自Manish Chablani的文章《CycleGANS and Pix2Pix》，参考：https://towardsdatascience.com/cyclegans-and-pix2pix-5e6a5f0159c4)",
  "translation": "（本文翻译自Manish Chablani的文章《 CycleGANS和Pix2Pix》，参考：https：//towardsdatascience.com/cyclegans-and-pix2pix-5e6a5f0159c4）"
}]