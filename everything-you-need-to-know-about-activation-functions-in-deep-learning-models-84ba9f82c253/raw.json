[{
  "tag": "H2",
  "text": "Handling Class imbalanced data using a loss specifically made for it",
  "translation": "å¤„ç†ç±»ä¸å¹³è¡¡æ•°æ®æ—¶ä½¿ç”¨ä¸“é—¨ä¸ºæ­¤é€ æˆçš„æŸå¤±"
}, {
  "tag": "H3",
  "text": "Get an accuracy boost of more than 4% on heavily class-imbalanced data by adding ~10 lines of code.",
  "translation": "é€šè¿‡æ·»åŠ çº¦10è¡Œä»£ç ï¼Œå¯ä»¥åœ¨ä¸¥é‡çš„ç±»ä¸å¹³è¡¡æ•°æ®ä¸Šå°†ç²¾åº¦æé«˜4ï¼…ä»¥ä¸Šã€‚"
}, {
  "tag": "H2",
  "text": "Self-Supervised GANs using auxiliary rotation loss",
  "translation": "ä½¿ç”¨è¾…åŠ©æ—‹è½¬æŸè€—çš„è‡ªæˆ‘ç›‘ç£GAN"
}, {
  "tag": "H3",
  "text": "Bridging the gap between supervised and un-supervised image generation",
  "translation": "ç¼©å°æœ‰ç›‘ç£å’Œæ— ç›‘ç£å›¾åƒç”Ÿæˆä¹‹é—´çš„å·®è·"
}, {
  "tag": "H2",
  "text": "Everything you need to know about MobileNetV3 and its comparison with previous versions",
  "translation": "æ‚¨éœ€è¦äº†è§£çš„æœ‰å…³MobileNetV3åŠå…¶ä¸ä»¥å‰ç‰ˆæœ¬çš„æ¯”è¾ƒçš„æ‰€æœ‰ä¿¡æ¯"
}, {
  "tag": "H3",
  "text": "Paper Review: Searching for MobilenetV3, ICCVâ€™19",
  "translation": "è®ºæ–‡è¯„è®ºï¼šæœç´¢MobilenetV3ï¼ŒICCVâ€™19"
}, {
  "tag": "H2",
  "text": "Everything you need to know about Auto-Deeplab: Googleâ€™s latest on Segmentation",
  "translation": "æ‚¨éœ€è¦äº†è§£çš„æœ‰å…³Auto-Deeplabçš„æ‰€æœ‰ä¿¡æ¯ï¼šGoogleå…³äºç»†åˆ†çš„æœ€æ–°ä¿¡æ¯"
}, {
  "tag": "H3",
  "text": "Searching a model for image segmentation",
  "translation": "æœç´¢æ¨¡å‹ä»¥è¿›è¡Œå›¾åƒåˆ†å‰²"
}, {
  "tag": "H1",
  "text": "Everything you need to know about â€œActivation Functionsâ€ in Deep learning models",
  "translation": "æ‚¨éœ€è¦äº†è§£çš„æœ‰å…³æ·±åº¦å­¦ä¹ æ¨¡å‹ä¸­â€œæ¿€æ´»åŠŸèƒ½â€çš„æ‰€æœ‰ä¿¡æ¯"
}, {
  "tag": "P",
  "text": "This article is your one-stop solution to every possible question related to activation functions that can come into your mind that are used in deep learning models. These are basically my notes on activation functions and all the knowledge that I have about this topic summed together in one place. So, without going into any unnecessary introduction, letâ€™s get straight down to business.",
  "translation": "æœ¬æ–‡æ˜¯æ‚¨ä¸æ·±åº¦å­¦ä¹ æ¨¡å‹ä¸­ä½¿ç”¨çš„ä¸æ¿€æ´»åŠŸèƒ½ç›¸å…³çš„æ¯ä¸ªå¯èƒ½é—®é¢˜çš„ä¸€ç«™å¼è§£å†³æ–¹æ¡ˆã€‚ è¿™äº›åŸºæœ¬ä¸Šæ˜¯æˆ‘å…³äºæ¿€æ´»åŠŸèƒ½çš„æ³¨é‡Šï¼Œä»¥åŠæˆ‘å¯¹è¿™ä¸ªä¸»é¢˜æ‹¥æœ‰çš„æ‰€æœ‰çŸ¥è¯†çš„æ€»æ‹¬åœ¨ä¸€ä¸ªåœ°æ–¹ã€‚ å› æ­¤ï¼Œåœ¨ä¸è¿›è¡Œä»»ä½•ä¸å¿…è¦çš„ä»‹ç»çš„æƒ…å†µä¸‹ï¼Œè®©æˆ‘ä»¬ç›´æ¥å¼€å§‹å·¥ä½œã€‚"
}, {
  "tag": "H1",
  "text": "Contents",
  "translation": "å†…å®¹"
}, {
  "tag": "OL",
  "texts": ["What is an activation function and what does it do in a network?", "Why is there a need for it and why not use a linear function instead?", "What are the desirable features in an activation function?", "Various non-linear activations in use", "Notable non-linear activations coming out of latest research", "How (and which) to use them in deep neural networks"],
  "translations": ["ä»€ä¹ˆæ˜¯æ¿€æ´»åŠŸèƒ½ï¼Ÿå®ƒåœ¨ç½‘ç»œä¸­èµ·ä»€ä¹ˆä½œç”¨ï¼Ÿ", "ä¸ºä»€ä¹ˆéœ€è¦å®ƒï¼Œä¸ºä»€ä¹ˆä¸ä½¿ç”¨çº¿æ€§å‡½æ•°å‘¢ï¼Ÿ", "æ¿€æ´»åŠŸèƒ½ä¸­éœ€è¦å…·å¤‡å“ªäº›åŠŸèƒ½ï¼Ÿ", "ä½¿ç”¨ä¸­çš„å„ç§éçº¿æ€§æ¿€æ´»", "æœ€æ–°ç ”ç©¶æ˜¾ç¤ºå‡ºæ˜¾ç€çš„éçº¿æ€§æ¿€æ´»", "å¦‚ä½•ï¼ˆä»¥åŠåœ¨æ·±åº¦ç¥ç»ç½‘ç»œä¸­ä½¿ç”¨å®ƒä»¬ï¼‰"]
}, {
  "tag": "H1",
  "text": "What is an activation function?",
  "translation": "ä»€ä¹ˆæ˜¯æ¿€æ´»åŠŸèƒ½ï¼Ÿ"
}, {
  "tag": "P",
  "text": "Simply put, an activation function is a function that is added into an artificial neural network in order to help the network learn complex patterns in the data. When comparing with a neuron-based model that is in our brains, the activation function is at the end deciding what is to be fired to the next neuron. That is exactly what an activation function does in an ANN as well. It takes in the output signal from the previous cell and converts it into some form that can be taken as input to the next cell. The comparison can be summarized in the figure below.",
  "translation": "ç®€è€Œè¨€ä¹‹ï¼Œæ¿€æ´»å‡½æ•°æ˜¯ä¸€ç§åŠŸèƒ½ï¼Œå·²æ·»åŠ åˆ°äººå·¥ç¥ç»ç½‘ç»œä¸­ï¼Œä»¥å¸®åŠ©ç½‘ç»œå­¦ä¹ æ•°æ®ä¸­çš„å¤æ‚æ¨¡å¼ã€‚ ä¸å¤§è„‘ä¸­åŸºäºç¥ç»å…ƒçš„æ¨¡å‹è¿›è¡Œæ¯”è¾ƒæ—¶ï¼Œæ¿€æ´»åŠŸèƒ½æœ€ç»ˆå†³å®šäº†è¦å‘å°„åˆ°ä¸‹ä¸€ä¸ªç¥ç»å…ƒçš„ä¸œè¥¿ã€‚ è¿™ä¹Ÿæ­£æ˜¯æ¿€æ´»å‡½æ•°åœ¨ANNä¸­æ‰€åšçš„ã€‚ å®ƒå¸æ”¶å‰ä¸€ä¸ªå•å…ƒçš„è¾“å‡ºä¿¡å·ï¼Œå¹¶å°†å…¶è½¬æ¢ä¸ºæŸç§å½¢å¼ï¼Œå¯ä»¥ä½œä¸ºä¸‹ä¸€ä¸ªå•å…ƒçš„è¾“å…¥ã€‚ æ¯”è¾ƒå¯ä»¥æ€»ç»“åœ¨ä¸‹å›¾ä¸­ã€‚"
}, {
  "tag": "FIGURE",
  "image": "https://miro.medium.com/max/30/1*BMSfafFNEpqGFCNU4smPkg.png?q=20",
  "caption": "Source: cs231n by Stanford",
  "type": "image",
  "file": "1*BMSfafFNEpqGFCNU4smPkg.png"
}, {
  "tag": "H1",
  "text": "Why is there a need for it?",
  "translation": "ä¸ºä»€ä¹ˆéœ€è¦å®ƒï¼Ÿ"
}, {
  "tag": "P",
  "text": "There are multiple reasons for having non-linear activation functions in a network.",
  "translation": "åœ¨ç½‘ç»œä¸­å…·æœ‰éçº¿æ€§æ¿€æ´»åŠŸèƒ½æœ‰å¤šç§åŸå› ã€‚"
}, {
  "tag": "OL",
  "texts": ["Apart from the biological similarity that was discussed earlier, they also help in keeping the value of the output from the neuron restricted to a certain limit as per our requirement. This is important because input into the activation function is W*x + b where W is the weights of the cell and the x is the inputs and then there is the bias b added to that. This value if not restricted to a certain limit can go very high in magnitude especially in case of very deep neural networks that have millions of parameters. This will lead to computational issues. For example, there are some activation functions (like softmax) that out specific values for different values of input (0 or 1).", "The most important feature in an activation function is its ability to add non-linearity into a neural network. To understand this, letâ€™s consider multidimensional data such as shown in the figure below:"],
  "translations": ["é™¤äº†å‰é¢è®¨è®ºçš„ç”Ÿç‰©å­¦ç›¸ä¼¼æ€§ä¹‹å¤–ï¼Œå®ƒä»¬è¿˜æœ‰åŠ©äºå°†ç¥ç»å…ƒè¾“å‡ºçš„å€¼é™åˆ¶ä¸ºæ ¹æ®æˆ‘ä»¬çš„è¦æ±‚é™åˆ¶åœ¨ä¸€å®šèŒƒå›´å†…ã€‚ è¿™å¾ˆé‡è¦ï¼Œå› ä¸ºæ¿€æ´»å‡½æ•°çš„è¾“å…¥æ˜¯W * x + bï¼Œå…¶ä¸­Wæ˜¯å•å…ƒçš„æƒé‡ï¼Œxæ˜¯è¾“å…¥ï¼Œç„¶ååœ¨å…¶ä¸ŠåŠ ä¸Šåå·®bã€‚ å¦‚æœä¸é™åˆ¶åœ¨æŸä¸ªå€¼ä¸Šï¼Œåˆ™è¯¥å€¼çš„å¹…åº¦å¯èƒ½ä¼šéå¸¸é«˜ï¼Œå°¤å…¶æ˜¯åœ¨å…·æœ‰æ•°ç™¾ä¸‡ä¸ªå‚æ•°çš„éå¸¸æ·±çš„ç¥ç»ç½‘ç»œçš„æƒ…å†µä¸‹ã€‚ è¿™å°†å¯¼è‡´è®¡ç®—é—®é¢˜ã€‚ ä¾‹å¦‚ï¼Œæœ‰ä¸€äº›æ¿€æ´»å‡½æ•°ï¼ˆå¦‚softmaxï¼‰é’ˆå¯¹ä¸åŒçš„è¾“å…¥å€¼ï¼ˆ0æˆ–1ï¼‰è¾“å‡ºç‰¹å®šçš„å€¼ã€‚", "æ¿€æ´»å‡½æ•°ä¸­æœ€é‡è¦çš„åŠŸèƒ½æ˜¯å…¶å°†éçº¿æ€§æ·»åŠ åˆ°ç¥ç»ç½‘ç»œçš„èƒ½åŠ›ã€‚ ä¸ºäº†ç†è§£è¿™ä¸€ç‚¹ï¼Œè®©æˆ‘ä»¬è€ƒè™‘å¤šç»´æ•°æ®ï¼Œå¦‚ä¸‹å›¾æ‰€ç¤ºï¼š"]
}, {
  "tag": "FIGURE",
  "image": "https://miro.medium.com/max/30/1*Ll3CR4irXqQioc4jTsQHzQ.png?q=20",
  "type": "image",
  "file": "1*Ll3CR4irXqQioc4jTsQHzQ.png"
}, {
  "tag": "P",
  "text": "A linear classifier using the three features(weight, Systolic Blood Pressure and Age in this figure) can give us a line through the 3-D space but it will never be able to exactly learn the pattern that makes a person a smoker or a non-smoker(the classification problem in hand) because the pattern that defines this classification is simply not linear. In come the artificial neural networks. What if we use an ANN with a single cell but without an activation function. So our output is basically W*x + b. But this is no good because W*x also has a degree of 1, hence linear and this is basically identical to a linear classifier.",
  "translation": "ä½¿ç”¨ä¸‰ä¸ªç‰¹å¾ï¼ˆæ­¤å›¾ä¸­çš„ä½“é‡ï¼Œæ”¶ç¼©å‹å’Œå¹´é¾„ï¼‰çš„çº¿æ€§åˆ†ç±»å™¨å¯ä»¥ä¸ºæˆ‘ä»¬æä¾›ä¸€æ¡ç©¿è¿‡3-Dç©ºé—´çš„çº¿ï¼Œä½†æ˜¯å®ƒå°†æ°¸è¿œæ— æ³•å‡†ç¡®åœ°äº†è§£ä½¿äººæˆä¸ºå¸çƒŸè€…æˆ–éå¸çƒŸè€…çš„æ¨¡å¼ -smokerï¼ˆæ‰‹å¤´çš„åˆ†ç±»é—®é¢˜ï¼‰ï¼Œå› ä¸ºå®šä¹‰æ­¤åˆ†ç±»çš„æ¨¡å¼æ ¹æœ¬ä¸æ˜¯çº¿æ€§çš„ã€‚ å‡ºç°äº†äººå·¥ç¥ç»ç½‘ç»œã€‚ å¦‚æœæˆ‘ä»¬ä½¿ç”¨å…·æœ‰å•ä¸ªå•å…ƒæ ¼ä½†æ²¡æœ‰æ¿€æ´»åŠŸèƒ½çš„ANNï¼Œè¯¥æ€ä¹ˆåŠã€‚ æ‰€ä»¥æˆ‘ä»¬çš„è¾“å‡ºåŸºæœ¬ä¸Šæ˜¯W * x + bã€‚ ä½†è¿™ä¸å¥½ï¼Œå› ä¸ºW * xä¹Ÿå…·æœ‰1çš„æ¬¡æ•°ï¼Œå› æ­¤æ˜¯çº¿æ€§çš„ï¼Œè¿™åŸºæœ¬ä¸Šä¸çº¿æ€§åˆ†ç±»å™¨ç›¸åŒã€‚"
}, {
  "tag": "P",
  "text": "What if we stack multiple layers. Letâ€™s represent náµ—Ê° layer as a function fâ‚™(x). So we have:",
  "translation": "å¦‚æœæˆ‘ä»¬å †å å¤šå±‚ï¼Œè¯¥æ€ä¹ˆåŠã€‚ è®©æˆ‘ä»¬å°†náµ—Ê°å±‚è¡¨ç¤ºä¸ºå‡½æ•°fâ‚™ï¼ˆxï¼‰ã€‚ å› æ­¤ï¼Œæˆ‘ä»¬æœ‰ï¼š"
}, {
  "tag": "P",
  "text": "o(x) = fâ‚™(fâ‚™â‚‹â‚(â€¦.fâ‚(x))",
  "translation": "oï¼ˆxï¼‰=fâ‚™ï¼ˆfâ‚™â‚‹â‚ï¼ˆ....fâ‚ï¼ˆxï¼‰ï¼‰"
}, {
  "tag": "P",
  "text": "However, this is also not complex enough especially for problems with very high patterns such as that faced in computer vision or natural language processing.",
  "translation": "ä½†æ˜¯ï¼Œè¿™è¿˜ä¸å¤Ÿå¤æ‚ï¼Œå°¤å…¶æ˜¯å¯¹äºè¯¸å¦‚è®¡ç®—æœºè§†è§‰æˆ–è‡ªç„¶è¯­è¨€å¤„ç†æ‰€é¢ä¸´çš„æ¨¡å¼éå¸¸é«˜çš„é—®é¢˜ã€‚"
}, {
  "tag": "P",
  "text": "In order to make the model get the power (aka the higher degree complexity) to learn the non-linear patterns, specific non-linear layers (activation functions) are added in between.",
  "translation": "ä¸ºäº†ä½¿æ¨¡å‹å…·æœ‰å­¦ä¹ éçº¿æ€§æ¨¡å¼çš„èƒ½åŠ›ï¼ˆåˆç§°è¾ƒé«˜ç¨‹åº¦çš„å¤æ‚æ€§ï¼‰ï¼Œåœ¨å®ƒä»¬ä¹‹é—´æ·»åŠ äº†ç‰¹å®šçš„éçº¿æ€§å±‚ï¼ˆæ¿€æ´»å‡½æ•°ï¼‰ã€‚"
}, {
  "tag": "H1",
  "text": "Desirable features of an activation function",
  "translation": "æ¿€æ´»åŠŸèƒ½çš„ç†æƒ³åŠŸèƒ½"
}, {
  "tag": "OL",
  "texts": ["Vanishing Gradient problem: Neural Networks are trained using the process gradient descent. The gradient descent consists of the backward propagation step which is basically chain rule to get the change in weights in order to reduce the loss after every epoch. Consider a two-layer network and the first layer is represented as fâ‚(x) and the second layer is represented as fâ‚‚(x). The overall network is o(x) = fâ‚‚(fâ‚(x)). If we calculate weights during the backward pass, we get o`(x) = fâ‚‚(x)*fâ‚`(x). Here fâ‚(x) is itself a compound function consisting of Act(Wâ‚*xâ‚ + bâ‚) where Act is the activation function after layer 1. Applying chain rule again, we clearly see that fâ‚`(x) = Act(Wâ‚*xâ‚ + bâ‚)*xâ‚ which means it also depends directly on the activation value. Now imagine such a chain rule going through multiple layers while backpropagation. If the value of Act() is between 0 and 1, then several such values will get multiplied to calculate the gradient of the initial layers. This reduces the value of the gradient for the initial layers and those layers are not able to learn properly. In other words, their gradients tend to vanish because of the depth of the network and the activation shifting the value to zero. This is called the vanishing gradient problem. So we want our activation function to not shift the gradient towards zero.", "Zero-Centered: Output of the activation function should be symmetrical at zero so that the gradients do not shift to a particular direction.", "Computational Expense: Activation functions are applied after every layer and need to be calculated millions of times in deep networks. Hence, they should be computationally inexpensive to calculate.", "Differentiable: As mentioned, neural networks are trained using the gradient descent process, hence the layers in the model need to differentiable or at least differentiable in parts. This is a necessary requirement for a function to work as activation function layer."],
  "translations": ["æ¶ˆå¤±æ¢¯åº¦é—®é¢˜ï¼šä½¿ç”¨è¿‡ç¨‹æ¢¯åº¦ä¸‹é™è®­ç»ƒç¥ç»ç½‘ç»œã€‚æ¢¯åº¦ä¸‹é™åŒ…æ‹¬åå‘ä¼ æ’­æ­¥éª¤ï¼Œè¯¥æ­¥éª¤åŸºæœ¬ä¸Šæ˜¯é“¾å¼è§„åˆ™ï¼Œå¯è·å–æƒé‡å˜åŒ–ï¼Œä»¥å‡å°‘æ¯ä¸ªæ—¶æœŸåçš„æŸå¤±ã€‚è€ƒè™‘ä¸¤å±‚ç½‘ç»œï¼Œç¬¬ä¸€å±‚è¡¨ç¤ºä¸ºf 1ï¼ˆxï¼‰ï¼Œç¬¬äºŒå±‚è¡¨ç¤ºä¸ºf 2ï¼ˆxï¼‰ã€‚æ•´ä¸ªç½‘ç»œä¸ºoï¼ˆxï¼‰= f 2ï¼ˆfâ‚ï¼ˆxï¼‰ï¼‰ã€‚å¦‚æœæˆ‘ä»¬åœ¨å‘åä¼ é€’è¿‡ç¨‹ä¸­è®¡ç®—æƒé‡ï¼Œåˆ™å¾—åˆ°o`ï¼ˆxï¼‰= f 2ï¼ˆxï¼‰*fâ‚`ï¼ˆxï¼‰ã€‚è¿™é‡Œfâ‚ï¼ˆxï¼‰æœ¬èº«å°±æ˜¯ä¸€ä¸ªç”±Actï¼ˆWâ‚*xâ‚+bâ‚ï¼‰ç»„æˆçš„å¤åˆå‡½æ•°ï¼Œå…¶ä¸­Actæ˜¯ç¬¬1å±‚ä¹‹åçš„æ¿€æ´»å‡½æ•°ã€‚å†æ¬¡åº”ç”¨é“¾å¼è§„åˆ™ï¼Œæˆ‘ä»¬æ¸…æ¥šåœ°çœ‹åˆ°fâ‚`ï¼ˆxï¼‰= Actï¼ˆWâ‚*xâ‚ï¼‰ +bâ‚ï¼‰*xâ‚ï¼Œè¿™ä¹Ÿç›´æ¥å–å†³äºæ¿€æ´»å€¼ã€‚ç°åœ¨æƒ³è±¡ä¸€ä¸‹ï¼Œè¿™æ ·çš„é“¾è§„åˆ™åœ¨åå‘ä¼ æ’­æ—¶ä¼šç»å†å¤šå±‚ã€‚å¦‚æœActï¼ˆï¼‰çš„å€¼åœ¨0åˆ°1ä¹‹é—´ï¼Œåˆ™å°†å¤šä¸ªè¿™æ ·çš„å€¼ç›¸ä¹˜ä»¥è®¡ç®—åˆå§‹å±‚çš„æ¢¯åº¦ã€‚è¿™ä¼šé™ä½åˆå§‹å±‚çš„æ¸å˜å€¼ï¼Œå¹¶ä¸”è¿™äº›å±‚å°†æ— æ³•æ­£ç¡®å­¦ä¹ ã€‚æ¢å¥è¯è¯´ï¼Œç”±äºç½‘ç»œæ·±åº¦å’Œæ¿€æ´»å°†å€¼ç§»è‡³é›¶ï¼Œå®ƒä»¬çš„æ¢¯åº¦è¶‹äºæ¶ˆå¤±ã€‚è¿™ç§°ä¸ºæ¶ˆå¤±æ¢¯åº¦é—®é¢˜ã€‚å› æ­¤ï¼Œæˆ‘ä»¬å¸Œæœ›æ¿€æ´»å‡½æ•°ä¸è¦å°†æ¢¯åº¦ç§»å‘é›¶ã€‚", "é›¶ä¸­å¿ƒï¼šæ¿€æ´»å‡½æ•°çš„è¾“å‡ºåº”å¯¹ç§°äºé›¶ï¼Œä»¥ä½¿æ¢¯åº¦ä¸ä¼šåç§»åˆ°ç‰¹å®šæ–¹å‘ã€‚", "è®¡ç®—è´¹ç”¨ï¼šæ¿€æ´»å‡½æ•°åº”ç”¨åœ¨æ¯ä¸€å±‚ä¹‹åï¼Œåœ¨æ·±åº¦ç½‘ç»œä¸­éœ€è¦è¿›è¡Œæ•°ç™¾ä¸‡æ¬¡è®¡ç®—ã€‚ å› æ­¤ï¼Œå®ƒä»¬åœ¨è®¡ç®—ä¸Šåº”è¯¥ä¾¿å®œã€‚", "å¯å¾®ï¼šå¦‚å‰æ‰€è¿°ï¼Œç¥ç»ç½‘ç»œæ˜¯ä½¿ç”¨æ¢¯åº¦ä¸‹é™è¿‡ç¨‹è®­ç»ƒçš„ï¼Œå› æ­¤æ¨¡å‹ä¸­çš„å„å±‚éœ€è¦éƒ¨åˆ†å¯å¾®æˆ–è‡³å°‘å¯å¾®ã€‚ è¿™æ˜¯åŠŸèƒ½ç”¨ä½œæ¿€æ´»åŠŸèƒ½å±‚çš„å¿…è¦æ¡ä»¶ã€‚"]
}, {
  "tag": "H1",
  "text": "Various non-linear activations in use",
  "translation": "ä½¿ç”¨ä¸­çš„å„ç§éçº¿æ€§æ¿€æ´»"
}, {
  "tag": "UL",
  "texts": ["Sigmoid: The sigmoid is defined as:"],
  "translations": ["ä¹™çŠ¶ç»“è‚ ï¼šä¹™çŠ¶ç»“è‚ å®šä¹‰ä¸ºï¼š"]
}, {
  "tag": "FIGURE",
  "image": "https://miro.medium.com/max/30/1*hYXbd20tIReMJ3T5D4OZLg.png?q=20",
  "type": "image",
  "file": "1*hYXbd20tIReMJ3T5D4OZLg.png"
}, {
  "tag": "P",
  "text": "This activation function is here only for historical reasons and never used in real models. It is computationally expensive, causes vanishing gradient problem and not zero-centred. This method is generally used for binary classification problems.",
  "translation": "æ­¤æ¿€æ´»åŠŸèƒ½ä»…å‡ºäºå†å²åŸå› ï¼Œä»æœªåœ¨å®é™…æ¨¡å‹ä¸­ä½¿ç”¨ã€‚ å®ƒåœ¨è®¡ç®—ä¸Šå¾ˆæ˜‚è´µï¼Œå¯¼è‡´æ¶ˆå¤±çš„æ¢¯åº¦é—®é¢˜ï¼Œè€Œä¸æ˜¯é›¶ä¸­å¿ƒã€‚ æ­¤æ–¹æ³•é€šå¸¸ç”¨äºäºŒè¿›åˆ¶åˆ†ç±»é—®é¢˜ã€‚"
}, {
  "tag": "UL",
  "texts": ["Softmax: The softmax is a more generalised form of the sigmoid. It is used in multi-class classification problems. Similar to sigmoid, it produces values in the range of 0â€“1 therefore it is used as the final layer in classification models.", "Tanh: The tanh is defined as:"],
  "translations": ["Softmaxï¼šsoftmaxæ˜¯ä¹™çŠ¶ç»“è‚ çš„æ›´å¹¿ä¹‰å½¢å¼ã€‚ å®ƒç”¨äºå¤šç±»åˆ†ç±»é—®é¢˜ã€‚ ä¸Så½¢ç›¸ä¼¼ï¼Œå®ƒäº§ç”Ÿçš„å€¼åœ¨0-1èŒƒå›´å†…ï¼Œå› æ­¤åœ¨åˆ†ç±»æ¨¡å‹ä¸­ç”¨ä½œæœ€åä¸€å±‚ã€‚", "tanhï¼štanhå®šä¹‰ä¸ºï¼š"]
}, {
  "tag": "FIGURE",
  "image": "https://miro.medium.com/max/30/1*IUc4KQ4L6ZWMfvzkNpNlag.png?q=20",
  "type": "image",
  "file": "1*IUc4KQ4L6ZWMfvzkNpNlag.png"
}, {
  "tag": "P",
  "text": "If you compare it to sigmoid, it solves just one problem of being zero-centred.",
  "translation": "å¦‚æœå°†å®ƒä¸Så‹ç›¸æ¯”è¾ƒï¼Œå®ƒåªèƒ½è§£å†³é›¶ä¸­å¿ƒé—®é¢˜ã€‚"
}, {
  "tag": "UL",
  "texts": ["ReLU: ReLU (Rectified Linear Unit) is defined as f(x) = max(0,x):"],
  "translations": ["ReLUï¼šReLUï¼ˆæ•´æµçº¿æ€§å•ä½ï¼‰å®šä¹‰ä¸ºfï¼ˆxï¼‰= maxï¼ˆ0ï¼Œxï¼‰ï¼š"]
}, {
  "tag": "FIGURE",
  "image": "https://miro.medium.com/max/30/1*nLGwhQGJRDOnQvluaX-WiQ.png?q=20",
  "type": "image",
  "file": "1*nLGwhQGJRDOnQvluaX-WiQ.png"
}, {
  "tag": "P",
  "text": "This is a widely used activation function, especially with Convolutional Neural networks. It is easy to compute and does not saturate and does not cause the Vanishing Gradient Problem. It has just one issue of not being zero centred. It suffers from â€œdying ReLUâ€ problem. Since the output is zero for all negative inputs. It causes some nodes to completely die and not learn anything.",
  "translation": "è¿™æ˜¯ä¸€ç§å¹¿æ³›ä½¿ç”¨çš„æ¿€æ´»å‡½æ•°ï¼Œå°¤å…¶æ˜¯åœ¨å·ç§¯ç¥ç»ç½‘ç»œä¸­ã€‚ å®ƒæ˜“äºè®¡ç®—ï¼Œä¸ä¼šé¥±å’Œï¼Œä¹Ÿä¸ä¼šå¼•èµ·æ¶ˆå¤±æ¢¯åº¦é—®é¢˜ã€‚ å®ƒåªæœ‰ä¸€ä¸ªä¸ä»¥é›¶ä¸ºä¸­å¿ƒçš„é—®é¢˜ã€‚ å®ƒé­å—â€œå‚æ­»çš„ReLUâ€é—®é¢˜çš„å›°æ‰°ã€‚ ç”±äºæ‰€æœ‰è´Ÿè¾“å…¥çš„è¾“å‡ºå‡ä¸ºé›¶ã€‚ å®ƒå¯¼è‡´æŸäº›èŠ‚ç‚¹å®Œå…¨æ­»æ‰ï¼Œå¹¶ä¸”ä¸å­¦ä¹ ä»»ä½•ä¸œè¥¿ã€‚"
}, {
  "tag": "P",
  "text": "Another problem with ReLU is of exploding the activations since it higher limit is, well, inf. This sometimes leads to unusable nodes.",
  "translation": "ReLUçš„å¦ä¸€ä¸ªé—®é¢˜æ˜¯çˆ†ç‚¸çš„æ¿€æ´»ï¼Œå› ä¸ºå®ƒçš„ä¸Šé™æ˜¯infã€‚ æœ‰æ—¶è¿™ä¼šå¯¼è‡´æ— æ³•ä½¿ç”¨çš„èŠ‚ç‚¹ã€‚"
}, {
  "tag": "UL",
  "texts": ["Leaky ReLU and Parametric ReLU: It is defined as f(x) = max(Î±x, x)"],
  "translations": ["æ³„æ¼ReLUå’Œå‚æ•°ReLUï¼šå®šä¹‰ä¸ºfï¼ˆxï¼‰= maxï¼ˆÎ±xï¼Œxï¼‰"]
}, {
  "tag": "FIGURE",
  "image": "https://miro.medium.com/max/30/1*scA-bQ597yTXJ-417RETAQ.png?q=20",
  "caption": "the figure is for Î± = 0.1",
  "type": "image",
  "file": "1*scA-bQ597yTXJ-417RETAQ.png"
}, {
  "tag": "P",
  "text": "Here Î± is a hyperparameter generally set to 0.01. Clearly, Leaky ReLU solves the â€œdying ReLUâ€ problem to some extent. Note that, if we set Î± as 1 then Leaky ReLU will become a linear function f(x) = x and will be of no use. Hence, the value of Î± is never set close to 1. If we set Î± as a hyperparameter for each neuron separately, we get parametric ReLU or PReLU.",
  "translation": "åœ¨è¿™é‡Œï¼ŒÎ±æ˜¯é€šå¸¸è®¾ç½®ä¸º0.01çš„è¶…å‚æ•°ã€‚ æ˜¾ç„¶ï¼ŒLeaky ReLUåœ¨æŸç§ç¨‹åº¦ä¸Šè§£å†³äº†â€œå‚æ­»çš„ReLUâ€é—®é¢˜ã€‚ è¯·æ³¨æ„ï¼Œå¦‚æœå°†Î±è®¾ç½®ä¸º1ï¼Œåˆ™Leaky ReLUå°†æˆä¸ºçº¿æ€§å‡½æ•°fï¼ˆxï¼‰= xï¼Œå°†æ— ç”¨ã€‚ å› æ­¤ï¼ŒÎ±çš„å€¼æ°¸è¿œä¸ä¼šè®¾ç½®ä¸ºæ¥è¿‘1ã€‚å¦‚æœæˆ‘ä»¬å°†Î±åˆ†åˆ«è®¾ç½®ä¸ºæ¯ä¸ªç¥ç»å…ƒçš„è¶…å‚æ•°ï¼Œåˆ™ä¼šå¾—åˆ°å‚æ•°ReLUæˆ–PReLUã€‚"
}, {
  "tag": "UL",
  "texts": ["ReLU6: It is basically ReLU restricted on the positive side and it is defined as f(x) = min(max(0,x),6)"],
  "translations": ["ReLU6ï¼šåŸºæœ¬ä¸Šæ˜¯ReLUé™åˆ¶åœ¨æ­£æï¼Œå®ƒå®šä¹‰ä¸ºfï¼ˆxï¼‰= minï¼ˆmaxï¼ˆ0ï¼Œxï¼‰ï¼Œ6ï¼‰"]
}, {
  "tag": "FIGURE",
  "image": "https://miro.medium.com/max/30/1*7BV9JmXkw9AG6S0dZfyrUw.png?q=20",
  "type": "image",
  "file": "1*7BV9JmXkw9AG6S0dZfyrUw.png"
}, {
  "tag": "P",
  "text": "This helps to stop blowing up the activation thereby stopping the gradients to explode(going to inf) as well another of the small issues that occur with normal ReLUs.",
  "translation": "è¿™æœ‰åŠ©äºåœæ­¢æ¿€æ´»æ¿€å¢ï¼Œä»è€Œåœæ­¢æ¢¯åº¦çˆ†ç‚¸ï¼ˆå˜ä¸ºinfï¼‰ï¼Œä»¥åŠæ­£å¸¸ReLUå‘ç”Ÿçš„å¦ä¸€ä¸ªå°é—®é¢˜ã€‚"
}, {
  "tag": "P",
  "text": "The idea that comes into oneâ€™s mind is why not combine ReLU6 and a LeakyReLU to solve all known issues that we have with previous activation functions. Popular DL frameworks do not provide an implementation of such an activation function but I think this would be a good idea.",
  "translation": "æƒ³åˆ°çš„ä¸€ä¸ªä¸»æ„æ˜¯ï¼Œä¸ºä»€ä¹ˆä¸å°†ReLU6å’ŒLeakyReLUç»“åˆèµ·æ¥è§£å†³ä»¥å‰çš„æ¿€æ´»åŠŸèƒ½æ‰€å­˜åœ¨çš„æ‰€æœ‰å·²çŸ¥é—®é¢˜ã€‚ æµè¡Œçš„DLæ¡†æ¶æ²¡æœ‰æä¾›è¿™ç§æ¿€æ´»åŠŸèƒ½çš„å®ç°ï¼Œä½†æ˜¯æˆ‘è®¤ä¸ºè¿™æ˜¯ä¸ªå¥½ä¸»æ„ã€‚"
}, {
  "tag": "H1",
  "text": "Notable non-linear activations coming out of latest research",
  "translation": "æœ€æ–°ç ”ç©¶æ˜¾ç¤ºå‡ºæ˜¾ç€çš„éçº¿æ€§æ¿€æ´»"
}, {
  "tag": "UL",
  "texts": ["Swish: This was proposed in 2017 by Ramachandran et.al. It is defined as f(x) = x*sigmoid(x)."],
  "translations": ["Swishï¼šè¿™æ˜¯Ramachandranç­‰äººäº2017å¹´æå‡ºçš„å»ºè®®ã€‚ å®šä¹‰ä¸ºfï¼ˆxï¼‰= x * Sigmoidï¼ˆxï¼‰ã€‚"]
}, {
  "tag": "FIGURE",
  "image": "https://miro.medium.com/max/30/1*tcMipRvB37hTLEM0baYsqg.png?q=20",
  "type": "image",
  "file": "1*tcMipRvB37hTLEM0baYsqg.png"
}, {
  "tag": "P",
  "text": "It is slightly better in performance as compared to ReLU since its graph is quite similar to ReLU. However, because it does not change abruptly at a point as ReLU does at x = 0, this makes it easier to converge while training.",
  "translation": "ä¸ReLUç›¸æ¯”ï¼Œå®ƒçš„æ€§èƒ½ç¨å¥½ä¸€äº›ï¼Œå› ä¸ºå®ƒçš„å›¾å½¢ä¸ReLUéå¸¸ç›¸ä¼¼ã€‚ ä½†æ˜¯ï¼Œç”±äºå®ƒåœ¨æŸä¸ªç‚¹ä¸ä¼šåƒReLUåœ¨x = 0æ—¶é‚£æ ·çªç„¶æ”¹å˜ï¼Œå› æ­¤åœ¨è®­ç»ƒæ—¶æ›´æ˜“äºæ”¶æ•›ã€‚"
}, {
  "tag": "P",
  "text": "But, the drawback of Swish is that it is computationally expensive. To solve that we come to the next version of Swish.",
  "translation": "ä½†æ˜¯ï¼ŒSwishçš„ç¼ºç‚¹æ˜¯è®¡ç®—é‡å¤§ã€‚ ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬æ¥è®¨è®ºä¸‹ä¸€ä¸ªSwishç‰ˆæœ¬ã€‚"
}, {
  "tag": "UL",
  "texts": ["Hard-Swish or H-Swish: This is defined as:"],
  "translations": ["ç¡¬æ‘†æˆ–é«˜æ‘†ï¼šå®šä¹‰ä¸ºï¼š"]
}, {
  "tag": "FIGURE",
  "image": "https://miro.medium.com/max/30/1*Mk7jTfv13SiXDyAl3ncYUQ.png?q=20",
  "type": "image",
  "file": "1*Mk7jTfv13SiXDyAl3ncYUQ.png"
}, {
  "tag": "FIGURE",
  "image": "https://miro.medium.com/max/30/1*S6ejo08UC5Ms3JK5qmascg.png?q=20",
  "type": "image",
  "file": "1*S6ejo08UC5Ms3JK5qmascg.png"
}, {
  "tag": "P",
  "text": "The best part is that it is almost similar to swish but it is less expensive computationally since it replaces sigmoid (exponential function) with a ReLU (linear type).",
  "translation": "æœ€å¥½çš„éƒ¨åˆ†æ˜¯ï¼Œå®ƒå‡ ä¹ä¸swishç›¸ä¼¼ï¼Œä½†è®¡ç®—æˆæœ¬è¾ƒä½ï¼Œå› ä¸ºå®ƒç”¨ReLUï¼ˆçº¿æ€§ç±»å‹ï¼‰ä»£æ›¿äº†Så‹ï¼ˆæŒ‡æ•°å‡½æ•°ï¼‰ã€‚"
}, {
  "tag": "H1",
  "text": "How to use them in deep neural networks?",
  "translation": "å¦‚ä½•åœ¨æ·±åº¦ç¥ç»ç½‘ç»œä¸­ä½¿ç”¨å®ƒä»¬ï¼Ÿ"
}, {
  "tag": "UL",
  "texts": ["Tanh and sigmoid cause huge vanishing gradient problems. Hence, they should not be used.", "Start with ReLU in your network. Activation layer is added after the weight layer (something like CNN, RNN, LSTM or linear dense layer) as discussed above in the article. If you think the model has stopped learning, then you can replace it with a LeakyReLU to avoid the Dying ReLU problem. However, the Leaky ReLU will increase the computation time a little bit.", "If you also have Batch-Norm layers in your network, that is added before the activation function making the order CNN-Batch Norm-Act. Although the order of Batch-Norm and Activation function is a topic of debate and some say that the order doesnâ€™t matter, I use the order mentioned above just to follow the original Batch-Norm paper.", "Activation functions work best in their default hyperparameters that are used in popular frameworks such as Tensorflow and Pytorch. However, one can fiddle with the negative slope in LeakyReLU and set it to 0.02 to expedite learning."],
  "translations": ["Tanhå’ŒSigmoidä¼šå¯¼è‡´å·¨å¤§çš„æ¶ˆå¤±æ¢¯åº¦é—®é¢˜ã€‚ å› æ­¤ï¼Œä¸åº”ä½¿ç”¨å®ƒä»¬ã€‚", "ä»ç½‘ç»œä¸­çš„ReLUå¼€å§‹ã€‚ å¦‚ä¸Šæ–‡æ‰€è¿°ï¼Œåœ¨æƒé‡å±‚ï¼ˆç±»ä¼¼äºCNNï¼ŒRNNï¼ŒLSTMæˆ–çº¿æ€§å¯†é›†å±‚ï¼‰ä¹‹åæ·»åŠ æ¿€æ´»å±‚ã€‚ å¦‚æœæ‚¨è®¤ä¸ºè¯¥æ¨¡å‹å·²åœæ­¢å­¦ä¹ ï¼Œåˆ™å¯ä»¥å°†å…¶æ›¿æ¢ä¸ºLeakyReLUï¼Œä»¥é¿å…Dying ReLUé—®é¢˜ã€‚ ä½†æ˜¯ï¼Œæ³„æ¼çš„ReLUå°†ç¨å¾®å¢åŠ è®¡ç®—æ—¶é—´ã€‚", "å¦‚æœæ‚¨çš„ç½‘ç»œä¸­ä¹Ÿæœ‰Batch-Normå±‚ï¼Œåˆ™åœ¨æ¿€æ´»åŠŸèƒ½å‘å‡ºCNN-Batch Norm-Actå‘½ä»¤ä¹‹å‰æ·»åŠ è¯¥å±‚ã€‚ å°½ç®¡â€œæ‰¹å¤„ç†è§„èŒƒâ€å’Œâ€œæ¿€æ´»â€åŠŸèƒ½çš„é¡ºåºæ˜¯ä¸€ä¸ªæœ‰äº‰è®®çš„è¯é¢˜ï¼Œå¹¶ä¸”æœ‰äº›äººè¯´è¯¥é¡ºåºæ— å…³ç´§è¦ï¼Œä½†æ˜¯æˆ‘ä½¿ç”¨ä¸Šè¿°é¡ºåºåªæ˜¯ä¸ºäº†éµå¾ªåŸå§‹çš„â€œæ‰¹å¤„ç†è§„èŒƒâ€è®ºæ–‡ã€‚", "æ¿€æ´»å‡½æ•°åœ¨å…¶æµè¡Œçš„æ¡†æ¶ï¼ˆå¦‚Tensorflowå’ŒPytorchï¼‰ä¸­ä½¿ç”¨çš„é»˜è®¤è¶…å‚æ•°ä¸­æ•ˆæœæœ€å¥½ã€‚ ä½†æ˜¯ï¼Œæ‚¨å¯ä»¥æ‘†å¼„LeakyReLUä¸­çš„è´Ÿæ–œç‡å¹¶å°†å…¶è®¾ç½®ä¸º0.02ä»¥åŠ å¿«å­¦ä¹ é€Ÿåº¦ã€‚"]
}, {
  "tag": "P",
  "text": "THATâ€™S ALL FOLKS ğŸ˜ƒ",
  "translation": "è¿™å°±æ˜¯æ‰€æœ‰çš„ğŸ˜ƒ"
}, {
  "tag": "P",
  "text": "I have tried to solve every possible question related to activation functions, however, if I have missed something please comment down below.",
  "translation": "æˆ‘å·²å°è¯•è§£å†³ä¸æ¿€æ´»åŠŸèƒ½ç›¸å…³çš„æ‰€æœ‰å¯èƒ½é—®é¢˜ï¼Œä½†æ˜¯ï¼Œå¦‚æœæˆ‘é”™è¿‡äº†ä»»ä½•äº‹æƒ…ï¼Œè¯·åœ¨ä¸‹é¢è¿›è¡Œè¯„è®ºã€‚"
}, {
  "tag": "P",
  "text": "You can look at more things related to deep learning on my Github and follow me on Linkedin.",
  "translation": "æ‚¨å¯ä»¥åœ¨æˆ‘çš„Githubä¸ŠæŸ¥çœ‹ä¸æ·±åº¦å­¦ä¹ æœ‰å…³çš„æ›´å¤šä¿¡æ¯ï¼Œå¹¶åœ¨Linkedinä¸Šå…³æ³¨æˆ‘ã€‚"
}, {
  "tag": "P",
  "text": "Some of my previous articles:",
  "translation": "æˆ‘ä»¥å‰çš„ä¸€äº›æ–‡ç« ï¼š"
}, {
  "tag": "H2",
  "text": "Everything you need to know about Auto-Deeplab: Googleâ€™s latest on Segmentation",
  "translation": "æ‚¨éœ€è¦äº†è§£çš„æœ‰å…³Auto-Deeplabçš„æ‰€æœ‰ä¿¡æ¯ï¼šGoogleå…³äºç»†åˆ†çš„æœ€æ–°ä¿¡æ¯"
}, {
  "tag": "H3",
  "text": "Searching a model for image segmentation",
  "translation": "æœç´¢æ¨¡å‹ä»¥è¿›è¡Œå›¾åƒåˆ†å‰²"
}, {
  "tag": "H2",
  "text": "Everything you need to know about MobileNetV3 and its comparison with previous versions",
  "translation": "æ‚¨éœ€è¦äº†è§£çš„æœ‰å…³MobileNetV3åŠå…¶ä¸ä»¥å‰ç‰ˆæœ¬çš„æ¯”è¾ƒçš„æ‰€æœ‰ä¿¡æ¯"
}, {
  "tag": "H3",
  "text": "Paper Review: Searching for MobilenetV3, ICCVâ€™19",
  "translation": "è®ºæ–‡è¯„è®ºï¼šæœç´¢MobilenetV3ï¼ŒICCVâ€™19"
}, {
  "tag": "H2",
  "text": "Self-Supervised GANs using auxiliary rotation loss",
  "translation": "ä½¿ç”¨è¾…åŠ©æ—‹è½¬æŸè€—çš„è‡ªæˆ‘ç›‘ç£GAN"
}, {
  "tag": "H3",
  "text": "Bridging the gap between supervised and un-supervised image generation",
  "translation": "ç¼©å°æœ‰ç›‘ç£å’Œæ— ç›‘ç£å›¾åƒç”Ÿæˆä¹‹é—´çš„å·®è·"
}, {
  "tag": "H2",
  "text": "Handling Class imbalanced data using a loss specifically made for it",
  "translation": "å¤„ç†ç±»ä¸å¹³è¡¡æ•°æ®æ—¶ä½¿ç”¨ä¸“é—¨ä¸ºæ­¤é€ æˆçš„æŸå¤±"
}, {
  "tag": "H3",
  "text": "Get an accuracy boost of more than 4% on heavily class-imbalanced data by adding ~10 lines of code.",
  "translation": "é€šè¿‡æ·»åŠ çº¦10è¡Œä»£ç ï¼Œå¯ä»¥åœ¨ä¸¥é‡çš„ç±»ä¸å¹³è¡¡æ•°æ®ä¸Šå°†ç²¾åº¦æé«˜4ï¼…ä»¥ä¸Šã€‚"
}, {
  "tag": "PRE",
  "text": "(æœ¬æ–‡ç¿»è¯‘è‡ªVandit Jainçš„æ–‡ç« ã€ŠEverything you need to know about â€œActivation Functionsâ€ in Deep learning modelsã€‹ï¼Œå‚è€ƒï¼šhttps://towardsdatascience.com/everything-you-need-to-know-about-activation-functions-in-deep-learning-models-84ba9f82c253)",
  "translation": "ï¼ˆæœ¬æ–‡ç¿»è¯‘è‡ªVandit Jainçš„æ–‡ç« ã€Šæ·±åº¦å­¦ä¹ æ¨¡å‹ä¸­çš„â€œæ¿€æ´»åŠŸèƒ½â€ï¼Œæ‚¨éœ€è¦äº†è§£çš„ä¸€åˆ‡â€ï¼Œå‚è€ƒï¼šhttpsï¼š//towardsdatascience.com/everything-you-need-to-know-about-activation-functions -æ·±å…¥å­¦ä¹ æ¨¡å‹-84ba9f82c253ï¼‰"
}]