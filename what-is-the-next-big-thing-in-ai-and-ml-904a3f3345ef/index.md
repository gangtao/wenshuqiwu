# 在2019年跟上AI
## AI和ML的下一件大事是什么？

过去的一年中，AI的事件，发现和发展非常丰富。 很难对噪声进行分类以查看信号是否存在以及信号的含义。 这篇文章试图为您提供确切的信息：在过去的一年中，我将尝试提取AI格局中的一些模式。 而且，如果幸运的话，我们将看到一些趋势如何延续到不久的将来。

引用孔子的话说：“最困难的是在黑暗的房间里找到一只黑猫，尤其是在没有猫的情况下。”明智的人。
![See that cat?](0*NVLoi3vu-9kry4t2)
> See that cat?


没错：这是一个观点。 我不是要建立年度的综合成绩记录。 我只是试图概述其中一些趋势。 另一个警告：此评论以美国为中心。 例如，中国正在发生很多有趣的事情，但是不幸的是，我对那个令人兴奋的生态系统并不熟悉。

这个帖子是给谁的？ 如果您仍在阅读，那么它可能适合您：想拓宽视野的工程师； 一个企业家，寻找下一个方向去引导他们的精力； 一个风险资本家，寻找他们的下一笔交易； 或只是技术啦啦队长，谁都迫不及待地想看看这种旋风将我们带往何处。
# 演算法

毫无疑问，算法论述是由深度神经网络主导的。 当然，您会听说有人在各处部署“经典”机器学习模型（例如Gradient Boosted树或Multi-armed土匪）。 并声称这是任何人唯一需要的东西。 有人宣称，深度学习正处于死亡的困境。 甚至顶级研究人员都对某些DNN架构的效率和健壮性提出质疑。 但是，不管喜欢与否，DNN无处不在：在无人驾驶汽车，自然语言系统，机器人中，您都可以说出来。 DNN的飞跃没有像自然语言处理，生成对抗网络和深度强化学习那样明显。
## 深度NLP：BERT等。

尽管在2018年之前使用DNN进行文本方面取得了一些突破（想到了word2vec，GLOVE，基于LSTM的模型），但缺少了一个关键的概念元素：转移学习。 也就是说，在大量公开可用数据上训练模型，然后在正在使用的特定数据集上对其进行“微调”。 在计算机视觉中，使用在著名的ImageNet数据集上发现的模式来解决特定问题通常是解决方案的一部分。

问题是，用于迁移学习的技术实际上不适用于NLP问题。 从某种意义上说，像word2vec这样的经过预训练的嵌入可以填补这一角色，但它们只能在单个单词级别上工作，而无法捕获语言的高级结构。

然而，在2018年，情况发生了变化。 ELMo，上下文化的嵌入成为改善NLP中的转移学习的重要的第一步。 ULMFiT甚至走得更远：作者对嵌入的语义捕获功能不满意，他们想出了一种对整个模型进行迁移学习的方法。
![The important guy.](0*S5L044q215w-mg6j)
> The important guy.


但是，最有趣的发展肯定是BERT的引入。 通过让语言模型从英语Wikipedia的全部文章中学习，该团队得以在11个NLP任务上取得了最新的成果-相当了不起！ 更好的是，代码和预训练模式都在线发布了-因此您可以将此突破应用于您自己的问题。
## GAN的许多面孔
![](0*4CIByVmyt17M-iQW)

CPU速度不再呈指数级增长，但是有关生成对抗网络的学术论文的数量似乎肯定还在继续增长。 GAN多年来一直是学术界的宠儿。 现实生活中的应用似乎很少，而且相差甚远，并且在2018年变化不大。GAN仍有巨大的潜力有待实现。

出现了一种新方法，即逐渐增加GAN的想法：让发生器在整个培训过程中逐渐提高其输出的分辨率。 使用此方法的最令人印象深刻的论文之一是采用样式转换技术来生成逼真的照片。 有多现实？ 你告诉我：
![Which one of this photos is of a real person? Trick question: none of them are.](0*Sn_Uz3dzQzkw0AJu)
> Which one of this photos is of a real person? Trick question: none of them are.


GAN如何以及为什么真正起作用？ 我们尚未对此进行深入了解，但是正在采取重要步骤：麻省理工学院的团队已对该问题进行了高质量的研究。

尽管不是技术上的GAN，但另一个有趣的发展是Adversarial Patch。 这个想法是同时使用黑盒（基本上不看神经网络的内部状态）和白盒方法来制作“补丁”，这会欺骗基于CNN的分类器。 这是一个重要的结果：它引导我们更直观地了解DNN的工作方式以及与人类概念感知的距离还有多远。
![Can you tell a the banana from a toaster? AI still can’t.](0*lJRt4MvyHBfVg2RR)
> Can you tell a the banana from a toaster? AI still can’t.

## 我们需要加强

自从AlphaGo在2016年击败Lee Sedol以来，强化学习一直是人们关注的焦点。尽管AI统治了最后一部“经典”游戏，但还有什么要征服的呢？ 好吧，世界其他地区！ 特别是计算机游戏和机器人技术。

在训练过程中，强化学习依赖于“奖励”信号，即在上一次尝试中的表现得分。 电脑游戏提供了一个自然的环境，在这种环境中，与现实生活相比，信号很容易获得。 因此，RL研究的所有注意力都集中在教授AI如何玩Atari游戏上。

谈到他们的新创造DeepMind，AlphaStar再次成为新闻。 这个新模型击败了星际争霸II顶级职业玩家之一。 与大多数棋盘游戏不同，《星际争霸》比国际象棋或围棋复杂得多，具有巨大的动作空间和对玩家隐藏的重要信息。 对于整个领域来说，这一胜利是非常重要的飞跃。

OpenAI是该领域或RL中的另一个重要参与者，也没有闲着。 他们声名is起的是OpenAI Five，该系统在八月份击败了Dota 2（一款极为复杂的电竞游戏）中的99.95％的玩家。

尽管OpenAI一直非常重视计算机游戏，但他们并没有忽略RL的真正潜在应用：机器人。 在现实世界中，可能会给机器人带来的反馈很少而且创建起来很昂贵：您基本上需要人工照看R2D2，同时它要迈出第一步。 您需要数百万个这些数据点。 为了弥合这种差距，最近的趋势是学习模拟环境并并行运行大量这些场景以教授机器人基本技能，然后再进入现实世界。 OpenAI和Google都在研究这种方法。
## 荣誉提名：Deepfakes

伪造品是（通常）显示公众人物在做或说自己从未做过或说过的话的图像或视频。 通过在“目标”人员的大量镜头上训练GAN，然后生成具有所需动作的新媒体来创建GAN。 2018年1月发布的名为FakeApp的桌面应用程序允许具有计算机知识和计算机科学零知识的任何人创建Deepfake。 尽管其制作的视频可以很容易地被发现是非正版的，但该技术已经取得了很大进步。 只需观看此视频即可了解多少。
# 基础设施
## TensorFlow与PyTorch

那里有许多深度学习框架。 这个领域很广阔，从表面上看这种变化是有意义的，但是实际上，最近大多数人都在使用Tensorflow或PyTorch。 如果您关心可靠性，易于部署，模型重新加载以及SRE通常关心的事情，则可能选择了Tensorflow。 如果您正在撰写研究论文，但不适用于Google，则可能使用了PyTorch。
## 机器学习即服务

今年，我们看到了更多的AI解决方案，这些API解决方案被打包为API，供软件工程师使用，而该软件工程师没有斯坦福大学的机器学习博士学位。 Google Cloud和Azure都改进了旧服务并添加了新服务。 AWS Machine Learning服务列表开始显得令人生畏。
![Man, AWS will soon need 2-level folder hierarchy for their services.](0*NeMASS_FiI3NruBW)
> Man, AWS will soon need 2-level folder hierarchy for their services.


尽管狂热有所缓解，但多家初创公司纷纷放弃了他们的手套。 每个人都对模型训练的速度，推断中的易用性和惊人的模型性能有希望。 只需输入您的信用卡，上传数据集，给模型一些时间进行训练或微调，调用REST（或者对于GraphQL对于更具前瞻性的初创公司来说）API并成为AI的主人，而无需弄清楚什么是辍学。

有了这么多的选择，为什么还要有人亲自构建模型和基础架构呢？ 在实践中，现成的MLaaS产品在80％的用例中都表现良好。 如果您希望剩余的20％也能正常工作-真不走运：不仅您不能真正选择模型，甚至无法控制超参数。 或者，如果您需要在云的舒适性之外的地方进行推理，通常就不需要。 绝对是一个权衡。
## 荣誉奖：AutoML和AI Hub

Google于今年推出了两项特别有趣的服务。

首先，Google Cloud AutoML是一组自定义的NLP和计算机视觉模型培训产品。 这意味着什么？ AutoML设计人员通过自动微调几个预训练的模型并选择性能最佳的模型来解决模型定制问题。 这意味着您很可能无需亲自定制模型。 当然，如果您想做一些新的或不同的事情，则此服务不适合您。 但是，作为附带的好处，Google在大量专有数据上对其模型进行了预训练。 想想所有那些猫的照片； 这些必须比Imagenet更好地推广！

其次是AI Hub和TensorFlow Hub。 在那两个之前，重用某人的模型是一件很麻烦的事。 GitHub上的随机代码很少起作用，通常记录很少，而且通常也不是一件令人愉快的事情。 以及用于迁移学习的预先训练的权重...假设您甚至不想尝试使它们发挥作用。 这正是TF Hub旨在解决的问题：这是一个可靠的，经过精心挑选的模型存储库，您可以对其进行微调或构建。 只需添加几行代码-TF Hub客户端就会从Google的服务器中获取代码和相应的权重-而且，这确实行得通！ AI Hub更进一步：它允许您共享整个ML管道，而不仅仅是模型！ 它仍然处于Alpha状态，但如果您了解我的意思，它已经比拥有最新文件“ 3年前修改”的随机存储库更好。
# 硬件
## 英伟达

如果您认真对待2018年的ML，尤其是DNN，则使用的是GPU（或多个）。 反过来，GPU领导者度过了非常繁忙的一年。 在加密货币狂潮消退和随后的股价暴跌之后，Nvidia发布了基于Turing架构的新一代新一代消费级卡。 仅在2017年发布的专业卡基于Volta芯片的情况下，新卡就包含了新的高速矩阵乘法硬件，称为Tensor Cores。 矩阵乘法是DNN操作方式的核心，因此加快这些操作的速度将大大提高在新GPU上进行神经网络训练的速度。

对于那些对“小型”和“慢速”游戏GPU不满意的人，Nvidia更新了他们的企业“超级计算机”。 DGX-2是具有16个Tesla Vs的巨型盒子，可进行480个TFLOP的FP16操作。 价格也进行了更新，达到令人印象深刻的40万美元。

自治硬件也进行了更新。 英伟达希望，Jetson AGX Xavier能够为下一代自动驾驶汽车提供动力。 八核CPU，视觉加速器，深度学习加速器-成长中的自动驾驶行业所需的一切。

在一项有趣的发展中，英伟达为其游戏卡推出了基于DNN的功能：深度学习超级采样。 这个想法是要替换抗锯齿，这通常是通过以比所需分辨率（例如4倍）更高的分辨率渲染图片，然后将其缩放到本机监视器分辨率来完成的。 现在，Nvidia允许开发人员在发布之前，以极高的质量在游戏上训练图像转换模型。 之后，将使用该预先训练的模型将游戏交付给最终用户。 在游戏过程中，无需花费旧式抗锯齿功能，而是通过该模型运行帧以提高画面质量。
## 英特尔

英特尔在2018年绝对不是AI硬件领域的开拓者。但似乎他们希望改变这一状况。

令人惊讶的是，大多数活动都发生在软件领域。 英特尔正在努力使现有和即将推出的硬件对开发人员更加友好。 考虑到这一点，他们发布了两个（令人惊讶的是，相互竞争的）工具包：OpenVINO和nGraph。

他们更新了神经计算棒：一种小型USB设备，可以加速在具有USB端口的任何设备上运行的DNN，甚至是Raspberry Pi。

传闻中的英特尔离散GPU越来越引起人们的兴趣。 八卦变得越来越持久，但是还有待观察新设备对DNN培训的适用性。 无疑将适用于深度学习的是一对传闻的专业深度学习卡，它们分别代号为Spring Hill和Spring Crest，后者基于英特尔几年前收购的初创公司Nervana的技术。
## 来自通常（和不寻常）嫌疑犯的定制硬件

Google推出了他们的第三代TPU：基于ASIC的DNN专用加速器，具有惊人的128Gb HMB内存。 256个这样的设备被组装到具有100 petaflops性能的容器中。 今年，Google不仅使这些设备的功能取笑了世界其他地区，还使TPU在Google Cloud上向公众开放。

以类似的方式（但主要针对推理应用程序），Amazon部署了AWS Inferentia：一种在生产环境中运行模型的更便宜，更有效的方法。
![](0*nTqHAwzY8MINf5j-)

谷歌还宣布了Edge TPU：上面讨论过的大型不良卡的弟弟。 芯片很小：其中10个可以装在1美分硬币的表面上。 同时，足以在实时视频上运行DNN，几乎不消耗任何能量。

Graphcore是一个有趣的潜在新进入者。 这家英国公司筹集了惊人的3.1亿美元，并于2018年交付了他们的第一款产品GC2芯片。 根据基准测试，GC2在进行推理的同时消除了顶级Nvidia服务器GPU卡，而功耗却大大降低。
## 荣誉奖：AWS Deep Racer

亚马逊出乎意料地采取了一项举措，但在某种程度上反映了他们之前与DeepLens达成的举措，推出了一款小型自动驾驶汽车DeepRacer和一个竞速联盟。 这款售价400美元的汽车配备了Atom处理器，4MP摄像头，wifi，几个USB端口以及足够的动力，可运行数小时。 可以在云端完全使用3d模拟环境来训练自动驾驶模型，然后将其直接部署到汽车上。 如果您一直梦想着建造自己的无人驾驶汽车，那么这是您无需创办由风投支持的公司的机会。
# 下一步是什么？
## 将重点转移到决策智能

现在，使AI有用的组件-算法，基础结构和硬件比以往任何时候都更好，企业意识到，开始应用AI的最大绊脚石是实践方面：您实际上如何从 在生产中运行有效，安全，可靠的系统的想法？ 应用AI或应用机器学习（ML），也称为决策智能，是为实际问题创建AI解决方案的科学。 尽管过去将大量精力放在算法背后的科学上，但未来很可能会更加关注该领域的端到端应用程序方面。
## 人工智能似乎创造了比破坏更多的工作

“人工智能将接手我们的所有工作”是媒体普遍反对的话题，也是蓝领和白领工人共同的恐惧。 而且，从表面上看，这似乎是一个合理的预测。 但是到目前为止，事实似乎恰恰相反。 例如，创建标签数据集的酬劳很多。

它超越了低收入国家通常的数据农场：LevelApp之类的一些应用程序使难民可以通过仅使用手机标记数据来赚钱。 哈蒙尼走得更远：他们甚至向难民营中的移民提供设备，使人们能够贡献自己的生活。

在数据标记之上，新的AI技术正在创建整个行业。 我们能够做一些甚至几年前都无法想象的事情，例如自动驾驶汽车或毒品发现。
## 更多与ML相关的计算将在边缘发生

面向数据的系统的工作方式通常是在系统的最边缘（摄取端）提供更多数据。 流水线的后期阶段通常会进行下采样或以其他方式降低信号的保真度。 另一方面，随着越来越复杂的AI模型，在处理更多数据时性能会更好。 将AI组件移到更靠近数据边缘的位置是否有意义？

一个简单的例子：想象一个高分辨率相机，它以30fps的速度产生高质量的视频。 处理该视频的计算机视觉模型在服务器上运行。 摄像机将视频流传输到服务器，但是上行链路带宽受到限制，因此视频被缩小并被高度压缩。 为什么不将视觉模型移至相机并消耗原始视频流？

总是存在多个障碍，主要是：边缘设备上可用的计算能力的数量和管理的复杂性（例如将更新的模型推送到边缘）。 专用硬件（例如Google的Edge TPU，苹果的神经引擎等），更高效的模型和优化的软件的出现消除了计算限制。 ML框架和工具的改进不断解决管理复杂性。
## 整合AI基础设施领域

前几年在AI基础设施中充满了活动：盛大的公告，大量的融资回合和崇高的承诺。 似乎在2018年，太空竞赛有所降温，尽管仍然有大量新的入口，但大部分贡献都是由大型现有玩家完成的。

一种可能的解释可能是，我们对AI系统理想基础架构的了解还不够成熟。 由于问题很复杂。 这将需要长期，持久，专注，资源丰富的工作才能产生可行的解决方案-初创公司和小型公司并不擅长。 如果一家初创公司突然“解决”了人工智能，那将真的令人惊讶。

另一方面，机器学习基础设施工程师很少见，而且相距甚远。 一个苦苦挣扎的初创公司，其中有几名员工，这对于大型公司来说显然是有价值的收购目标。 而且至少有一些人为赢得胜利而玩这款游戏，同时建立了内部和外部工具。 例如，对于AWS和Google Cloud而言，人工智能基础设施服务是主要卖点。

放在一起，对空间的重大合并将成为合理的预测。
## 更多定制硬件

至少对于CPU而言，摩尔定律已经失效，并且已经有很多年了。 GPU即将遇到类似的命运。 而且，尽管我们的模型变得越来越高效，但要解决一些更高级的问题，我们需要动手使用更多的计算能力。 这可以通过分布式培训来解决，但是它有其自身的局限性和权衡。

此外，如果您想在资源受限的设备上运行某些更大的模型，则分布式培训将无济于事。 输入自定义AI加速器。 根据您想要或可以采用的自定义方式，可以节省一个数量级的电源，成本或延迟。

从某种意义上说，甚至Nvidia的Tensor Core就是这种趋势的一个例子。 如果没有通用硬件，我们将看到更多。
## 减少对训练数据的依赖

标记的数据通常是昂贵的，不可访问的或两者兼有。 此规则几乎没有例外。 MNIST，ImageNet，COCO，Netflix奖和IMDB评论等开放式高质量数据集是令人难以置信的创新之源。 但是许多问题都没有相应的数据集。 虽然对于研究者而言，建立数据集并不是一项伟大的职业生涯，但是能够赞助或发布一个数据集的大公司却并不着急：他们正在建立庞大的数据集，但要让它们靠近自己的胸口。

那么，一个小的独立实体，如初创企业或大学研究小组，如何为棘手的问题提供有趣的解决方案？ 通过构建越来越少依赖监督信号，越来越多依赖无标签和非结构化数据的系统，这要归功于互联网和廉价传感器的普及。

这在某种程度上解释了对GAN，转移和强化学习的兴趣激增：所有这些技术都需要较少（或不需要）对训练数据集的投资。
# 所以，这只是一个泡沫，对吗？

那么那个黑暗的房间里有只猫吗？ 我认为肯定存在，而不仅仅是一个，而是多个。 尽管有些猫的腿，尾巴和胡须有四根（通常是这样），但有些是怪异的野兽，我们才刚刚开始看到它们的基本轮廓。

整个行业已经进入了炙手可热的AI“夏季”第七年。 在那段时间里，大量的研究工作，学术资助，风险投资，媒体关注和代码行注入了空间。 但有理由指出，人工智能的承诺仍未兑现。 他们的最后一次Uber旅程仍然有一名驾驶员。 他们仍然没有帮助早上做鸡蛋的机器人。 我什至不得不大声喊叫自己绑鞋带！

然而，无数研究生和软件工程师的努力并没有白费。 似乎每个大公司要么已经非常依赖AI，要么已经计划在将来。 AI艺术卖。 如果自动驾驶汽车还没有在这里，他们很快就会来。

现在，只要有人弄清楚那些讨厌的鞋带！ 等一下 他们做到了？

非常感谢Malika Cantor，Maya Grossman，Tom White，Cassie Kozyrkov和Peter Norvig阅读了这篇文章的草稿。

Max Grigorev已在Google，Airbnb和多家初创公司建立了机器学习系统。 他希望建立更多。 他还是Google Developers Launchpad的导师。
